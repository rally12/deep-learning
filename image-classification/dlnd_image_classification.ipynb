{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9eede1470>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = np.zeros((len(x),10))\n",
    "    data = zip(ret,x)\n",
    "    for rete,label in data:\n",
    "        rete[label]=1\n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "global std_dev\n",
    "std_dev = 0.05\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,\n",
    "        shape=(None, image_shape[0], image_shape[1], image_shape[2]), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,\n",
    "        shape=(None,n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32x32 -> 8.5x8.5 -> 4.25x4.25 - ret_shape =  [None, 4, 4, 10]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    tensor_depth = tensor_shape[3]\n",
    "    weights_shape = [\n",
    "        conv_ksize[0], conv_ksize[1], \n",
    "        tensor_depth, conv_num_outputs]\n",
    "    #print (\"weights_shape = \", weights_shape)\n",
    "    x=(tensor_shape[1]-conv_ksize[0])/(conv_strides[0])+1\n",
    "    y=(tensor_shape[2]-conv_ksize[1])/(conv_strides[1])+1\n",
    "    xm=(x-pool_ksize[0])/(pool_strides[0])+1\n",
    "    ym=(y-pool_ksize[1])/(pool_strides[1])+1\n",
    "    print (\" {0}x{1} -> {2}x{3} -> {4}x{5}\".format(\n",
    "        tensor_shape[1], tensor_shape[2],\n",
    "        x,y,\n",
    "        xm,ym),\n",
    "        end=\" - \")\n",
    "    \n",
    "    weights = tf.Variable(tf.random_normal( weights_shape, stddev=std_dev, seed=42))\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs], stddev=std_dev, seed=42))\n",
    "    ret = tf.nn.conv2d(x_tensor, weights, \n",
    "        strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "        padding='VALID')\n",
    "    ret = tf.nn.bias_add(ret,bias)\n",
    "    ret = tf.nn.relu(ret)\n",
    "    ret = tf.nn.max_pool(ret, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                        strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                        padding='VALID')\n",
    "    print (\"ret_shape = \", ret.get_shape().as_list())\n",
    "    return ret \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening shape [None, 10, 30, 6] -> 1800 inputs\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    count = np.product(shape[1:])\n",
    "    print (\"flattening shape {} -> {} inputs\".format(shape, count))\n",
    "    return tf.reshape(x_tensor, [-1, count])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    s = x_tensor.get_shape()[1].value    \n",
    "    w = tf.Variable(tf.random_normal([s,num_outputs], stddev=std_dev, seed=42))\n",
    "    b = tf.Variable(tf.random_normal([num_outputs], stddev=std_dev, seed=42))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, w),b))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    s = x_tensor.get_shape()[1].value    \n",
    "    w = tf.Variable(tf.random_normal([s,num_outputs], stddev=std_dev, seed=42))\n",
    "    b = tf.Variable(tf.random_normal([num_outputs], stddev=std_dev, seed=42))\n",
    "    out = tf.add(tf.matmul(x_tensor, w),b)\n",
    "    #out = tf.nn.softmax(out)   \n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits x rgb shape [None, 32, 32, 3]\n",
      "logits x bw  shape [None, 32, 32, 1]\n",
      " 32x32 -> 29.0x29.0 -> 14.0x14.0 - ret_shape =  [None, 14, 14, 24]\n",
      " 14x14 -> 12.0x12.0 -> 6.0x6.0 - ret_shape =  [None, 6, 6, 48]\n",
      " 6x6 -> 3.0x3.0 -> 3.0x3.0 - ret_shape =  [None, 3, 3, 72]\n",
      " 32x32 -> 29.0x29.0 -> 14.0x14.0 - ret_shape =  [None, 14, 14, 24]\n",
      " 14x14 -> 12.0x12.0 -> 6.0x6.0 - ret_shape =  [None, 6, 6, 48]\n",
      " 6x6 -> 3.0x3.0 -> 3.0x3.0 - ret_shape =  [None, 3, 3, 48]\n",
      "flattening shape [None, 3, 3, 120] -> 1080 inputs\n",
      "logits x rgb shape [None, 32, 32, 3]\n",
      "logits x bw  shape [None, 32, 32, 1]\n",
      " 32x32 -> 29.0x29.0 -> 14.0x14.0 - ret_shape =  [None, 14, 14, 24]\n",
      " 14x14 -> 12.0x12.0 -> 6.0x6.0 - ret_shape =  [None, 6, 6, 48]\n",
      " 6x6 -> 3.0x3.0 -> 3.0x3.0 - ret_shape =  [None, 3, 3, 72]\n",
      " 32x32 -> 29.0x29.0 -> 14.0x14.0 - ret_shape =  [None, 14, 14, 24]\n",
      " 14x14 -> 12.0x12.0 -> 6.0x6.0 - ret_shape =  [None, 6, 6, 48]\n",
      " 6x6 -> 3.0x3.0 -> 3.0x3.0 - ret_shape =  [None, 3, 3, 48]\n",
      "flattening shape [None, 3, 3, 120] -> 1080 inputs\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    filters = 24\n",
    "    x1 = tf.multiply(x,tf.constant([0.3,0.6,0.1]))\n",
    "    x1 =  tf.reduce_sum(x1,3, keep_dims = True )\n",
    "    print (\"logits x rgb shape\", x.get_shape().as_list())\n",
    "    print (\"logits x bw  shape\", x1.get_shape().as_list())\n",
    "    \n",
    "    \n",
    "    logits0 = conv2d_maxpool(x,         filters, (4,4), (1,1), (3,3), (2,2))\n",
    "    logits0 = conv2d_maxpool(logits0,  2*filters, (3,3), (1,1), (2,2), (2,2))\n",
    "    logits0 = conv2d_maxpool(logits0, 3*filters, (4,4), (1,1), (1,1), (1,1))\n",
    "    \n",
    "    logits1 = conv2d_maxpool(x1,        filters, (4,4), (1,1), (3,3), (2,2))\n",
    "    logits1 = conv2d_maxpool(logits1, 2*filters, (3,3), (1,1), (2,2), (2,2))\n",
    "    logits1 = conv2d_maxpool(logits1, 2*filters, (4,4), (1,1), (1,1), (1,1))\n",
    "        \n",
    "    logits = tf.concat([logits0, logits1], 3)\n",
    "    \n",
    "    logits_shape = logits.get_shape().as_list()    \n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    #logits = tf.nn.dropout(logits, keep_prob, noise_shape = [ 1, 1, 1, logits_shape[3]], seed = 42.0)\n",
    "    logits = tf.nn.dropout(logits, keep_prob, noise_shape = [ 1, logits_shape[1], logits_shape[2], 1], seed = 42.0)\n",
    "    logits = flatten(logits)\n",
    "    #logits = tf.nn.relu(logits)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    #logits = fully_conn(logits, int(0.35 * filters))\n",
    "    logits = fully_conn(logits, int(0.4* logits.get_shape()[1].value))\n",
    "    logits = tf.nn.dropout(logits, keep_prob, seed = 42.0)\n",
    "    logits = fully_conn(logits, int(0.4* logits.get_shape()[1].value))\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits = output(logits, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict = {\n",
    "        keep_prob:keep_probability,\n",
    "        x:feature_batch,\n",
    "        y:label_batch\n",
    "    })\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global last_time\n",
    "    loss = sess.run(cost, feed_dict = {\n",
    "        keep_prob: 1.0,\n",
    "        x:feature_batch,\n",
    "        y:label_batch\n",
    "    })\n",
    "    accuracy = sess.run(accuracy, feed_dict = {\n",
    "        keep_prob: 1.0,\n",
    "        x:valid_features,\n",
    "        y:valid_labels\n",
    "    })\n",
    "    \n",
    "    print (\"stats: loss {: >5.8f}, accuracy {:1.8f}, elapsed {:>3.2f}     \\r\".format(loss, accuracy, time.time() - last_time))\n",
    "    last_time=time.time()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 250\n",
    "batch_size = 512\n",
    "keep_probability = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  stats: loss 2.29539633, accuracy 0.12300000, elapsed 5.20     \n",
      "Epoch  2, CIFAR-10 Batch 1:  stats: loss 2.21034884, accuracy 0.21499999, elapsed 2.27     \n",
      "Epoch  3, CIFAR-10 Batch 1:  stats: loss 2.07708120, accuracy 0.22460000, elapsed 2.24     \n",
      "Epoch  4, CIFAR-10 Batch 1:  stats: loss 2.05489540, accuracy 0.23899999, elapsed 2.27     \n",
      "Epoch  5, CIFAR-10 Batch 1:  stats: loss 2.06799865, accuracy 0.28979999, elapsed 2.26     \n",
      "Epoch  6, CIFAR-10 Batch 1:  stats: loss 1.95489395, accuracy 0.28459996, elapsed 2.25     \n",
      "Epoch  7, CIFAR-10 Batch 1:  stats: loss 2.13462472, accuracy 0.21479997, elapsed 2.27     \n",
      "Epoch  8, CIFAR-10 Batch 1:  stats: loss 2.02736616, accuracy 0.26639998, elapsed 2.26     \n",
      "Epoch  9, CIFAR-10 Batch 1:  stats: loss 1.93603325, accuracy 0.32639998, elapsed 2.24     \n",
      "Epoch 10, CIFAR-10 Batch 1:  stats: loss 1.85359919, accuracy 0.33919996, elapsed 2.25     \n",
      "Epoch 11, CIFAR-10 Batch 1:  stats: loss 1.82012367, accuracy 0.36879998, elapsed 2.23     \n",
      "Epoch 12, CIFAR-10 Batch 1:  stats: loss 1.73566961, accuracy 0.38440001, elapsed 2.24     \n",
      "Epoch 13, CIFAR-10 Batch 1:  stats: loss 1.74579859, accuracy 0.38900000, elapsed 2.26     \n",
      "Epoch 14, CIFAR-10 Batch 1:  stats: loss 1.66440392, accuracy 0.40679997, elapsed 2.26     \n",
      "Epoch 15, CIFAR-10 Batch 1:  stats: loss 1.64108980, accuracy 0.42619997, elapsed 2.25     \n",
      "Epoch 16, CIFAR-10 Batch 1:  stats: loss 1.75975752, accuracy 0.35400000, elapsed 2.26     \n",
      "Epoch 17, CIFAR-10 Batch 1:  stats: loss 1.60784554, accuracy 0.43320000, elapsed 2.26     \n",
      "Epoch 18, CIFAR-10 Batch 1:  stats: loss 1.59951735, accuracy 0.43300003, elapsed 2.25     \n",
      "Epoch 19, CIFAR-10 Batch 1:  stats: loss 1.50852716, accuracy 0.44919994, elapsed 2.27     \n",
      "Epoch 20, CIFAR-10 Batch 1:  stats: loss 1.57480371, accuracy 0.42879999, elapsed 2.25     \n",
      "Epoch 21, CIFAR-10 Batch 1:  stats: loss 1.60206151, accuracy 0.44739997, elapsed 2.27     \n",
      "Epoch 22, CIFAR-10 Batch 1:  stats: loss 1.48110390, accuracy 0.46100000, elapsed 2.26     \n",
      "Epoch 23, CIFAR-10 Batch 1:  stats: loss 1.54777515, accuracy 0.42739996, elapsed 2.24     \n",
      "Epoch 24, CIFAR-10 Batch 1:  stats: loss 1.43557620, accuracy 0.47939995, elapsed 2.25     \n",
      "Epoch 25, CIFAR-10 Batch 1:  stats: loss 1.43440974, accuracy 0.49879995, elapsed 2.27     \n",
      "Epoch 26, CIFAR-10 Batch 1:  stats: loss 1.34361672, accuracy 0.49859995, elapsed 2.26     \n",
      "Epoch 27, CIFAR-10 Batch 1:  stats: loss 1.40750837, accuracy 0.50039995, elapsed 2.25     \n",
      "Epoch 28, CIFAR-10 Batch 1:  stats: loss 1.32746208, accuracy 0.51959991, elapsed 2.25     \n",
      "Epoch 29, CIFAR-10 Batch 1:  stats: loss 1.29781544, accuracy 0.51239991, elapsed 2.24     \n",
      "Epoch 30, CIFAR-10 Batch 1:  stats: loss 1.32458925, accuracy 0.50899994, elapsed 2.25     \n",
      "Epoch 31, CIFAR-10 Batch 1:  stats: loss 1.23598230, accuracy 0.52839994, elapsed 2.25     \n",
      "Epoch 32, CIFAR-10 Batch 1:  stats: loss 1.30077374, accuracy 0.50059998, elapsed 2.26     \n",
      "Epoch 33, CIFAR-10 Batch 1:  stats: loss 1.24370944, accuracy 0.52599996, elapsed 2.26     \n",
      "Epoch 34, CIFAR-10 Batch 1:  stats: loss 1.25200748, accuracy 0.52079993, elapsed 2.25     \n",
      "Epoch 35, CIFAR-10 Batch 1:  stats: loss 1.21547532, accuracy 0.54539996, elapsed 2.27     \n",
      "Epoch 36, CIFAR-10 Batch 1:  stats: loss 1.19593620, accuracy 0.52079999, elapsed 2.27     \n",
      "Epoch 37, CIFAR-10 Batch 1:  stats: loss 1.30534959, accuracy 0.53959996, elapsed 2.25     \n",
      "Epoch 38, CIFAR-10 Batch 1:  stats: loss 1.15774322, accuracy 0.55899996, elapsed 2.26     \n",
      "Epoch 39, CIFAR-10 Batch 1:  stats: loss 1.09526932, accuracy 0.55939996, elapsed 2.24     \n",
      "Epoch 40, CIFAR-10 Batch 1:  stats: loss 1.08984303, accuracy 0.56459993, elapsed 2.26     \n",
      "Epoch 41, CIFAR-10 Batch 1:  stats: loss 1.10846853, accuracy 0.56099999, elapsed 2.26     \n",
      "Epoch 42, CIFAR-10 Batch 1:  stats: loss 1.06539357, accuracy 0.56179988, elapsed 2.24     \n",
      "Epoch 43, CIFAR-10 Batch 1:  stats: loss 1.10263038, accuracy 0.54959995, elapsed 2.27     \n",
      "Epoch 44, CIFAR-10 Batch 1:  stats: loss 1.09669304, accuracy 0.56259996, elapsed 2.24     \n",
      "Epoch 45, CIFAR-10 Batch 1:  stats: loss 1.04356194, accuracy 0.57459992, elapsed 2.28     \n",
      "Epoch 46, CIFAR-10 Batch 1:  stats: loss 1.00175524, accuracy 0.57879996, elapsed 2.26     \n",
      "Epoch 47, CIFAR-10 Batch 1:  stats: loss 0.95736337, accuracy 0.58439994, elapsed 2.27     \n",
      "Epoch 48, CIFAR-10 Batch 1:  stats: loss 0.96288383, accuracy 0.57159996, elapsed 2.26     \n",
      "Epoch 49, CIFAR-10 Batch 1:  stats: loss 0.98667312, accuracy 0.58439988, elapsed 2.29     \n",
      "Epoch 50, CIFAR-10 Batch 1:  stats: loss 0.94868720, accuracy 0.58379990, elapsed 2.27     \n",
      "Epoch 51, CIFAR-10 Batch 1:  stats: loss 0.94235909, accuracy 0.59559995, elapsed 2.27     \n",
      "Epoch 52, CIFAR-10 Batch 1:  stats: loss 0.88403165, accuracy 0.59699988, elapsed 2.24     \n",
      "Epoch 53, CIFAR-10 Batch 1:  stats: loss 0.87037241, accuracy 0.60359991, elapsed 2.27     \n",
      "Epoch 54, CIFAR-10 Batch 1:  stats: loss 0.85795557, accuracy 0.58939993, elapsed 2.25     \n",
      "Epoch 55, CIFAR-10 Batch 1:  stats: loss 0.85297078, accuracy 0.58819997, elapsed 2.25     \n",
      "Epoch 56, CIFAR-10 Batch 1:  stats: loss 0.82530582, accuracy 0.59799987, elapsed 2.26     \n",
      "Epoch 57, CIFAR-10 Batch 1:  stats: loss 0.79421222, accuracy 0.60719991, elapsed 2.25     \n",
      "Epoch 58, CIFAR-10 Batch 1:  stats: loss 0.76110744, accuracy 0.61759990, elapsed 2.28     \n",
      "Epoch 59, CIFAR-10 Batch 1:  stats: loss 0.74688900, accuracy 0.61999989, elapsed 2.25     \n",
      "Epoch 60, CIFAR-10 Batch 1:  stats: loss 0.84339952, accuracy 0.60959995, elapsed 2.23     \n",
      "Epoch 61, CIFAR-10 Batch 1:  stats: loss 0.73957676, accuracy 0.62319988, elapsed 2.25     \n",
      "Epoch 62, CIFAR-10 Batch 1:  stats: loss 0.76896471, accuracy 0.59839994, elapsed 2.24     \n",
      "Epoch 63, CIFAR-10 Batch 1:  stats: loss 0.71819270, accuracy 0.62499994, elapsed 2.26     \n",
      "Epoch 64, CIFAR-10 Batch 1:  stats: loss 0.70372230, accuracy 0.62659991, elapsed 2.24     \n",
      "Epoch 65, CIFAR-10 Batch 1:  stats: loss 0.91749829, accuracy 0.57039994, elapsed 2.25     \n",
      "Epoch 66, CIFAR-10 Batch 1:  stats: loss 0.73989612, accuracy 0.61859995, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 1:  stats: loss 0.68687487, accuracy 0.62959993, elapsed 2.26     \n",
      "Epoch 68, CIFAR-10 Batch 1:  stats: loss 0.71491408, accuracy 0.60679990, elapsed 2.26     \n",
      "Epoch 69, CIFAR-10 Batch 1:  stats: loss 0.66141075, accuracy 0.61879987, elapsed 2.26     \n",
      "Epoch 70, CIFAR-10 Batch 1:  stats: loss 0.66781759, accuracy 0.61979997, elapsed 2.25     \n",
      "Epoch 71, CIFAR-10 Batch 1:  stats: loss 0.63415414, accuracy 0.62739986, elapsed 2.25     \n",
      "Epoch 72, CIFAR-10 Batch 1:  stats: loss 0.65538394, accuracy 0.59939992, elapsed 2.26     \n",
      "Epoch 73, CIFAR-10 Batch 1:  stats: loss 0.62869972, accuracy 0.62079990, elapsed 2.28     \n",
      "Epoch 74, CIFAR-10 Batch 1:  stats: loss 0.66311854, accuracy 0.61619991, elapsed 2.27     \n",
      "Epoch 75, CIFAR-10 Batch 1:  stats: loss 0.56404543, accuracy 0.63859993, elapsed 2.27     \n",
      "Epoch 76, CIFAR-10 Batch 1:  stats: loss 0.58023113, accuracy 0.62819988, elapsed 2.26     \n",
      "Epoch 77, CIFAR-10 Batch 1:  stats: loss 0.57130456, accuracy 0.62599993, elapsed 2.26     \n",
      "Epoch 78, CIFAR-10 Batch 1:  stats: loss 0.54391187, accuracy 0.63259989, elapsed 2.27     \n",
      "Epoch 79, CIFAR-10 Batch 1:  stats: loss 0.52539843, accuracy 0.63999993, elapsed 2.25     \n",
      "Epoch 80, CIFAR-10 Batch 1:  stats: loss 0.53040463, accuracy 0.63279986, elapsed 2.26     \n",
      "Epoch 81, CIFAR-10 Batch 1:  stats: loss 0.53889352, accuracy 0.64039993, elapsed 2.26     \n",
      "Epoch 82, CIFAR-10 Batch 1:  stats: loss 0.55825746, accuracy 0.63239992, elapsed 2.28     \n",
      "Epoch 83, CIFAR-10 Batch 1:  stats: loss 0.52273333, accuracy 0.64019990, elapsed 2.26     \n",
      "Epoch 84, CIFAR-10 Batch 1:  stats: loss 0.51028889, accuracy 0.64279985, elapsed 2.28     \n",
      "Epoch 85, CIFAR-10 Batch 1:  stats: loss 0.49307939, accuracy 0.64639992, elapsed 2.26     \n",
      "Epoch 86, CIFAR-10 Batch 1:  stats: loss 0.48399666, accuracy 0.63479990, elapsed 2.23     \n",
      "Epoch 87, CIFAR-10 Batch 1:  stats: loss 0.49278924, accuracy 0.64039993, elapsed 2.26     \n",
      "Epoch 88, CIFAR-10 Batch 1:  stats: loss 0.60508579, accuracy 0.61979997, elapsed 2.26     \n",
      "Epoch 89, CIFAR-10 Batch 1:  stats: loss 0.50973094, accuracy 0.63739991, elapsed 2.28     \n",
      "Epoch 90, CIFAR-10 Batch 1:  stats: loss 0.48755735, accuracy 0.64079982, elapsed 2.26     \n",
      "Epoch 91, CIFAR-10 Batch 1:  stats: loss 0.44578311, accuracy 0.64179987, elapsed 2.25     \n",
      "Epoch 92, CIFAR-10 Batch 1:  stats: loss 0.43737262, accuracy 0.63699996, elapsed 2.26     \n",
      "Epoch 93, CIFAR-10 Batch 1:  stats: loss 0.46464297, accuracy 0.63499987, elapsed 2.26     \n",
      "Epoch 94, CIFAR-10 Batch 1:  stats: loss 0.45372206, accuracy 0.63339996, elapsed 2.25     \n",
      "Epoch 95, CIFAR-10 Batch 1:  stats: loss 0.42198968, accuracy 0.64719987, elapsed 2.26     \n",
      "Epoch 96, CIFAR-10 Batch 1:  stats: loss 0.41557798, accuracy 0.64559990, elapsed 2.27     \n",
      "Epoch 97, CIFAR-10 Batch 1:  stats: loss 0.44037074, accuracy 0.64059991, elapsed 2.26     \n",
      "Epoch 98, CIFAR-10 Batch 1:  stats: loss 0.43357518, accuracy 0.63419998, elapsed 2.25     \n",
      "Epoch 99, CIFAR-10 Batch 1:  stats: loss 0.40597898, accuracy 0.64219987, elapsed 2.26     \n",
      "Epoch 100, CIFAR-10 Batch 1:  stats: loss 0.38653609, accuracy 0.64859986, elapsed 2.24     \n",
      "Epoch 101, CIFAR-10 Batch 1:  stats: loss 0.37747300, accuracy 0.65479994, elapsed 2.27     \n",
      "Epoch 102, CIFAR-10 Batch 1:  stats: loss 0.40248096, accuracy 0.64839989, elapsed 2.26     \n",
      "Epoch 103, CIFAR-10 Batch 1:  stats: loss 0.42854252, accuracy 0.62459993, elapsed 2.26     \n",
      "Epoch 104, CIFAR-10 Batch 1:  stats: loss 0.40396473, accuracy 0.63959992, elapsed 2.27     \n",
      "Epoch 105, CIFAR-10 Batch 1:  stats: loss 0.43140408, accuracy 0.62419993, elapsed 2.27     \n",
      "Epoch 106, CIFAR-10 Batch 1:  stats: loss 0.37741095, accuracy 0.64299989, elapsed 2.26     \n",
      "Epoch 107, CIFAR-10 Batch 1:  stats: loss 0.38179803, accuracy 0.63179994, elapsed 2.25     \n",
      "Epoch 108, CIFAR-10 Batch 1:  stats: loss 0.38374317, accuracy 0.64279997, elapsed 2.26     \n",
      "Epoch 109, CIFAR-10 Batch 1:  stats: loss 0.38050243, accuracy 0.64239985, elapsed 2.26     \n",
      "Epoch 110, CIFAR-10 Batch 1:  stats: loss 0.36389583, accuracy 0.64479989, elapsed 2.27     \n",
      "Epoch 111, CIFAR-10 Batch 1:  stats: loss 0.34257376, accuracy 0.65199995, elapsed 2.28     \n",
      "Epoch 112, CIFAR-10 Batch 1:  stats: loss 0.32270032, accuracy 0.65679991, elapsed 2.26     \n",
      "Epoch 113, CIFAR-10 Batch 1:  stats: loss 0.35351175, accuracy 0.64819992, elapsed 2.24     \n",
      "Epoch 114, CIFAR-10 Batch 1:  stats: loss 0.31201851, accuracy 0.66219991, elapsed 2.28     \n",
      "Epoch 115, CIFAR-10 Batch 1:  stats: loss 0.29654369, accuracy 0.65199989, elapsed 2.25     \n",
      "Epoch 116, CIFAR-10 Batch 1:  stats: loss 0.34714019, accuracy 0.62619990, elapsed 2.27     \n",
      "Epoch 117, CIFAR-10 Batch 1:  stats: loss 0.32826337, accuracy 0.64939994, elapsed 2.27     \n",
      "Epoch 118, CIFAR-10 Batch 1:  stats: loss 0.33471602, accuracy 0.63179982, elapsed 2.26     \n",
      "Epoch 119, CIFAR-10 Batch 1:  stats: loss 0.32356903, accuracy 0.64839983, elapsed 2.26     \n",
      "Epoch 120, CIFAR-10 Batch 1:  stats: loss 0.30376720, accuracy 0.65799981, elapsed 2.26     \n",
      "Epoch 121, CIFAR-10 Batch 1:  stats: loss 0.29430652, accuracy 0.65059990, elapsed 2.28     \n",
      "Epoch 122, CIFAR-10 Batch 1:  stats: loss 0.28506878, accuracy 0.65559989, elapsed 2.28     \n",
      "Epoch 123, CIFAR-10 Batch 1:  stats: loss 0.26055348, accuracy 0.65479994, elapsed 2.26     \n",
      "Epoch 124, CIFAR-10 Batch 1:  stats: loss 0.26842988, accuracy 0.65199989, elapsed 2.26     \n",
      "Epoch 125, CIFAR-10 Batch 1:  stats: loss 0.26123929, accuracy 0.65639991, elapsed 2.26     \n",
      "Epoch 126, CIFAR-10 Batch 1:  stats: loss 0.25807187, accuracy 0.64719987, elapsed 2.26     \n",
      "Epoch 127, CIFAR-10 Batch 1:  stats: loss 0.26143205, accuracy 0.65559989, elapsed 2.25     \n",
      "Epoch 128, CIFAR-10 Batch 1:  stats: loss 0.24603423, accuracy 0.65859991, elapsed 2.28     \n",
      "Epoch 129, CIFAR-10 Batch 1:  stats: loss 0.25155929, accuracy 0.64419991, elapsed 2.26     \n",
      "Epoch 130, CIFAR-10 Batch 1:  stats: loss 0.24240868, accuracy 0.64499992, elapsed 2.27     \n",
      "Epoch 131, CIFAR-10 Batch 1:  stats: loss 0.26902342, accuracy 0.63679993, elapsed 2.27     \n",
      "Epoch 132, CIFAR-10 Batch 1:  stats: loss 0.28692329, accuracy 0.62499988, elapsed 2.26     \n",
      "Epoch 133, CIFAR-10 Batch 1:  stats: loss 0.29695392, accuracy 0.62239993, elapsed 2.27     \n",
      "Epoch 134, CIFAR-10 Batch 1:  stats: loss 0.26473418, accuracy 0.65579987, elapsed 2.27     \n",
      "Epoch 135, CIFAR-10 Batch 1:  stats: loss 0.24106182, accuracy 0.66459990, elapsed 2.26     \n",
      "Epoch 136, CIFAR-10 Batch 1:  stats: loss 0.23994999, accuracy 0.65759993, elapsed 2.26     \n",
      "Epoch 137, CIFAR-10 Batch 1:  stats: loss 0.21398620, accuracy 0.66399986, elapsed 2.26     \n",
      "Epoch 138, CIFAR-10 Batch 1:  stats: loss 0.21802747, accuracy 0.65519989, elapsed 2.25     \n",
      "Epoch 139, CIFAR-10 Batch 1:  stats: loss 0.23761281, accuracy 0.65419990, elapsed 2.26     \n",
      "Epoch 140, CIFAR-10 Batch 1:  stats: loss 0.23777652, accuracy 0.66519988, elapsed 2.26     \n",
      "Epoch 141, CIFAR-10 Batch 1:  stats: loss 0.19400014, accuracy 0.66739988, elapsed 2.26     \n",
      "Epoch 142, CIFAR-10 Batch 1:  stats: loss 0.20955873, accuracy 0.65679991, elapsed 2.26     \n",
      "Epoch 143, CIFAR-10 Batch 1:  stats: loss 0.21174760, accuracy 0.66159987, elapsed 2.24     \n",
      "Epoch 144, CIFAR-10 Batch 1:  stats: loss 0.21750930, accuracy 0.64919990, elapsed 2.26     \n",
      "Epoch 145, CIFAR-10 Batch 1:  stats: loss 0.19801770, accuracy 0.66219985, elapsed 2.27     \n",
      "Epoch 146, CIFAR-10 Batch 1:  stats: loss 0.22062154, accuracy 0.64719993, elapsed 2.27     \n",
      "Epoch 147, CIFAR-10 Batch 1:  stats: loss 0.22123076, accuracy 0.65219992, elapsed 2.28     \n",
      "Epoch 148, CIFAR-10 Batch 1:  stats: loss 0.18611698, accuracy 0.66319984, elapsed 2.25     \n",
      "Epoch 149, CIFAR-10 Batch 1:  stats: loss 0.19252284, accuracy 0.66559982, elapsed 2.27     \n",
      "Epoch 150, CIFAR-10 Batch 1:  stats: loss 0.17666319, accuracy 0.66059995, elapsed 2.26     \n",
      "Epoch 151, CIFAR-10 Batch 1:  stats: loss 0.16182837, accuracy 0.66839981, elapsed 2.24     \n",
      "Epoch 152, CIFAR-10 Batch 1:  stats: loss 0.16050741, accuracy 0.66679990, elapsed 2.26     \n",
      "Epoch 153, CIFAR-10 Batch 1:  stats: loss 0.15273185, accuracy 0.66739988, elapsed 2.27     \n",
      "Epoch 154, CIFAR-10 Batch 1:  stats: loss 0.17483865, accuracy 0.65999985, elapsed 2.26     \n",
      "Epoch 155, CIFAR-10 Batch 1:  stats: loss 0.19096419, accuracy 0.66279989, elapsed 2.27     \n",
      "Epoch 156, CIFAR-10 Batch 1:  stats: loss 0.15188380, accuracy 0.66179991, elapsed 2.28     \n",
      "Epoch 157, CIFAR-10 Batch 1:  stats: loss 0.15789668, accuracy 0.66459984, elapsed 2.28     \n",
      "Epoch 158, CIFAR-10 Batch 1:  stats: loss 0.15727659, accuracy 0.64759988, elapsed 2.24     \n",
      "Epoch 159, CIFAR-10 Batch 1:  stats: loss 0.14578927, accuracy 0.66159993, elapsed 2.27     \n",
      "Epoch 160, CIFAR-10 Batch 1:  stats: loss 0.16850558, accuracy 0.66499984, elapsed 2.27     \n",
      "Epoch 161, CIFAR-10 Batch 1:  stats: loss 0.15693831, accuracy 0.65299994, elapsed 2.25     \n",
      "Epoch 162, CIFAR-10 Batch 1:  stats: loss 0.18142286, accuracy 0.65799987, elapsed 2.27     \n",
      "Epoch 163, CIFAR-10 Batch 1:  stats: loss 0.18913211, accuracy 0.64239991, elapsed 2.27     \n",
      "Epoch 164, CIFAR-10 Batch 1:  stats: loss 0.21878310, accuracy 0.63319987, elapsed 2.26     \n",
      "Epoch 165, CIFAR-10 Batch 1:  stats: loss 0.17997412, accuracy 0.66319990, elapsed 2.27     \n",
      "Epoch 166, CIFAR-10 Batch 1:  stats: loss 0.15147360, accuracy 0.65499991, elapsed 2.27     \n",
      "Epoch 167, CIFAR-10 Batch 1:  stats: loss 0.14031696, accuracy 0.65999991, elapsed 2.25     \n",
      "Epoch 168, CIFAR-10 Batch 1:  stats: loss 0.15734516, accuracy 0.65379989, elapsed 2.26     \n",
      "Epoch 169, CIFAR-10 Batch 1:  stats: loss 0.12432997, accuracy 0.66679990, elapsed 2.28     \n",
      "Epoch 170, CIFAR-10 Batch 1:  stats: loss 0.12188533, accuracy 0.66199988, elapsed 2.25     \n",
      "Epoch 171, CIFAR-10 Batch 1:  stats: loss 0.12888712, accuracy 0.66559988, elapsed 2.26     \n",
      "Epoch 172, CIFAR-10 Batch 1:  stats: loss 0.11545820, accuracy 0.66759992, elapsed 2.25     \n",
      "Epoch 173, CIFAR-10 Batch 1:  stats: loss 0.11781574, accuracy 0.67299986, elapsed 2.27     \n",
      "Epoch 174, CIFAR-10 Batch 1:  stats: loss 0.11106460, accuracy 0.66359985, elapsed 2.25     \n",
      "Epoch 175, CIFAR-10 Batch 1:  stats: loss 0.12418191, accuracy 0.66919982, elapsed 2.28     \n",
      "Epoch 176, CIFAR-10 Batch 1:  stats: loss 0.11360920, accuracy 0.66759992, elapsed 2.25     \n",
      "Epoch 177, CIFAR-10 Batch 1:  stats: loss 0.12302482, accuracy 0.66099983, elapsed 2.25     \n",
      "Epoch 178, CIFAR-10 Batch 1:  stats: loss 0.12548123, accuracy 0.66999990, elapsed 2.26     \n",
      "Epoch 179, CIFAR-10 Batch 1:  stats: loss 0.11032775, accuracy 0.67019987, elapsed 2.28     \n",
      "Epoch 180, CIFAR-10 Batch 1:  stats: loss 0.12977603, accuracy 0.66439986, elapsed 2.25     \n",
      "Epoch 181, CIFAR-10 Batch 1:  stats: loss 0.12496954, accuracy 0.66239989, elapsed 2.26     \n",
      "Epoch 182, CIFAR-10 Batch 1:  stats: loss 0.13163090, accuracy 0.66759992, elapsed 2.27     \n",
      "Epoch 183, CIFAR-10 Batch 1:  stats: loss 0.10942512, accuracy 0.66439986, elapsed 2.26     \n",
      "Epoch 184, CIFAR-10 Batch 1:  stats: loss 0.10333546, accuracy 0.66259992, elapsed 2.29     \n",
      "Epoch 185, CIFAR-10 Batch 1:  stats: loss 0.10037780, accuracy 0.66219985, elapsed 2.28     \n",
      "Epoch 186, CIFAR-10 Batch 1:  stats: loss 0.09610142, accuracy 0.66559988, elapsed 2.28     \n",
      "Epoch 187, CIFAR-10 Batch 1:  stats: loss 0.10061325, accuracy 0.66759986, elapsed 2.27     \n",
      "Epoch 188, CIFAR-10 Batch 1:  stats: loss 0.11230968, accuracy 0.65979993, elapsed 2.26     \n",
      "Epoch 189, CIFAR-10 Batch 1:  stats: loss 0.11020721, accuracy 0.66719991, elapsed 2.25     \n",
      "Epoch 190, CIFAR-10 Batch 1:  stats: loss 0.10914215, accuracy 0.65899986, elapsed 2.26     \n",
      "Epoch 191, CIFAR-10 Batch 1:  stats: loss 0.09745805, accuracy 0.66579992, elapsed 2.26     \n",
      "Epoch 192, CIFAR-10 Batch 1:  stats: loss 0.10960615, accuracy 0.66359985, elapsed 2.27     \n",
      "Epoch 193, CIFAR-10 Batch 1:  stats: loss 0.10375589, accuracy 0.66039991, elapsed 2.29     \n",
      "Epoch 194, CIFAR-10 Batch 1:  stats: loss 0.10599811, accuracy 0.66519988, elapsed 2.28     \n",
      "Epoch 195, CIFAR-10 Batch 1:  stats: loss 0.12152597, accuracy 0.64659983, elapsed 2.26     \n",
      "Epoch 196, CIFAR-10 Batch 1:  stats: loss 0.11196966, accuracy 0.66099995, elapsed 2.24     \n",
      "Epoch 197, CIFAR-10 Batch 1:  stats: loss 0.09388922, accuracy 0.65919989, elapsed 2.27     \n",
      "Epoch 198, CIFAR-10 Batch 1:  stats: loss 0.09322600, accuracy 0.67339993, elapsed 2.25     \n",
      "Epoch 199, CIFAR-10 Batch 1:  stats: loss 0.09836473, accuracy 0.67599988, elapsed 2.25     \n",
      "Epoch 200, CIFAR-10 Batch 1:  stats: loss 0.09951311, accuracy 0.66279984, elapsed 2.28     \n",
      "Epoch 201, CIFAR-10 Batch 1:  stats: loss 0.08222282, accuracy 0.66859990, elapsed 2.26     \n",
      "Epoch 202, CIFAR-10 Batch 1:  stats: loss 0.08193164, accuracy 0.67279989, elapsed 2.26     \n",
      "Epoch 203, CIFAR-10 Batch 1:  stats: loss 0.07839686, accuracy 0.66519988, elapsed 2.29     \n",
      "Epoch 204, CIFAR-10 Batch 1:  stats: loss 0.08359673, accuracy 0.66899991, elapsed 2.26     \n",
      "Epoch 205, CIFAR-10 Batch 1:  stats: loss 0.08715777, accuracy 0.66459990, elapsed 2.25     \n",
      "Epoch 206, CIFAR-10 Batch 1:  stats: loss 0.09262583, accuracy 0.66559988, elapsed 2.26     \n",
      "Epoch 207, CIFAR-10 Batch 1:  stats: loss 0.10135743, accuracy 0.65259987, elapsed 2.28     \n",
      "Epoch 208, CIFAR-10 Batch 1:  stats: loss 0.07754707, accuracy 0.66239989, elapsed 2.26     \n",
      "Epoch 209, CIFAR-10 Batch 1:  stats: loss 0.07578786, accuracy 0.66379994, elapsed 2.26     \n",
      "Epoch 210, CIFAR-10 Batch 1:  stats: loss 0.08120369, accuracy 0.67639989, elapsed 2.28     \n",
      "Epoch 211, CIFAR-10 Batch 1:  stats: loss 0.06878276, accuracy 0.67619991, elapsed 2.28     \n",
      "Epoch 212, CIFAR-10 Batch 1:  stats: loss 0.08067973, accuracy 0.66299987, elapsed 2.27     \n",
      "Epoch 213, CIFAR-10 Batch 1:  stats: loss 0.07540304, accuracy 0.67119980, elapsed 2.27     \n",
      "Epoch 214, CIFAR-10 Batch 1:  stats: loss 0.08209323, accuracy 0.66319984, elapsed 2.27     \n",
      "Epoch 215, CIFAR-10 Batch 1:  stats: loss 0.09163781, accuracy 0.66239989, elapsed 2.29     \n",
      "Epoch 216, CIFAR-10 Batch 1:  stats: loss 0.07579859, accuracy 0.65899992, elapsed 2.27     \n",
      "Epoch 217, CIFAR-10 Batch 1:  stats: loss 0.07705995, accuracy 0.66139984, elapsed 2.27     \n",
      "Epoch 218, CIFAR-10 Batch 1:  stats: loss 0.07591100, accuracy 0.66539985, elapsed 2.26     \n",
      "Epoch 219, CIFAR-10 Batch 1:  stats: loss 0.07467924, accuracy 0.66099989, elapsed 2.27     \n",
      "Epoch 220, CIFAR-10 Batch 1:  stats: loss 0.07516971, accuracy 0.66679990, elapsed 2.29     \n",
      "Epoch 221, CIFAR-10 Batch 1:  stats: loss 0.07870048, accuracy 0.66059989, elapsed 2.28     \n",
      "Epoch 222, CIFAR-10 Batch 1:  stats: loss 0.06410399, accuracy 0.66519988, elapsed 2.26     \n",
      "Epoch 223, CIFAR-10 Batch 1:  stats: loss 0.06080794, accuracy 0.67239988, elapsed 2.26     \n",
      "Epoch 224, CIFAR-10 Batch 1:  stats: loss 0.05557718, accuracy 0.66499984, elapsed 2.26     \n",
      "Epoch 225, CIFAR-10 Batch 1:  stats: loss 0.06018488, accuracy 0.66679984, elapsed 2.26     \n",
      "Epoch 226, CIFAR-10 Batch 1:  stats: loss 0.06080483, accuracy 0.67139983, elapsed 2.26     \n",
      "Epoch 227, CIFAR-10 Batch 1:  stats: loss 0.07596177, accuracy 0.66259986, elapsed 2.25     \n",
      "Epoch 228, CIFAR-10 Batch 1:  stats: loss 0.07319820, accuracy 0.65999991, elapsed 2.26     \n",
      "Epoch 229, CIFAR-10 Batch 1:  stats: loss 0.08620296, accuracy 0.66539991, elapsed 2.27     \n",
      "Epoch 230, CIFAR-10 Batch 1:  stats: loss 0.09723163, accuracy 0.65959990, elapsed 2.26     \n",
      "Epoch 231, CIFAR-10 Batch 1:  stats: loss 0.07585029, accuracy 0.66439986, elapsed 2.27     \n",
      "Epoch 232, CIFAR-10 Batch 1:  stats: loss 0.05895314, accuracy 0.66519988, elapsed 2.25     \n",
      "Epoch 233, CIFAR-10 Batch 1:  stats: loss 0.05474675, accuracy 0.67179984, elapsed 2.27     \n",
      "Epoch 234, CIFAR-10 Batch 1:  stats: loss 0.05711586, accuracy 0.67419988, elapsed 2.27     \n",
      "Epoch 235, CIFAR-10 Batch 1:  stats: loss 0.05350424, accuracy 0.66919988, elapsed 2.25     \n",
      "Epoch 236, CIFAR-10 Batch 1:  stats: loss 0.07516479, accuracy 0.65579987, elapsed 2.26     \n",
      "Epoch 237, CIFAR-10 Batch 1:  stats: loss 0.05960147, accuracy 0.65959990, elapsed 2.27     \n",
      "Epoch 238, CIFAR-10 Batch 1:  stats: loss 0.06414998, accuracy 0.65539992, elapsed 2.28     \n",
      "Epoch 239, CIFAR-10 Batch 1:  stats: loss 0.06986606, accuracy 0.66499990, elapsed 2.25     \n",
      "Epoch 240, CIFAR-10 Batch 1:  stats: loss 0.05501316, accuracy 0.66839993, elapsed 2.28     \n",
      "Epoch 241, CIFAR-10 Batch 1:  stats: loss 0.05174894, accuracy 0.66739988, elapsed 2.25     \n",
      "Epoch 242, CIFAR-10 Batch 1:  stats: loss 0.04725712, accuracy 0.67639989, elapsed 2.27     \n",
      "Epoch 243, CIFAR-10 Batch 1:  stats: loss 0.05353910, accuracy 0.66019988, elapsed 2.27     \n",
      "Epoch 244, CIFAR-10 Batch 1:  stats: loss 0.04162624, accuracy 0.66599989, elapsed 2.28     \n",
      "Epoch 245, CIFAR-10 Batch 1:  stats: loss 0.04978454, accuracy 0.66239989, elapsed 2.26     \n",
      "Epoch 246, CIFAR-10 Batch 1:  stats: loss 0.04169615, accuracy 0.66739988, elapsed 2.26     \n",
      "Epoch 247, CIFAR-10 Batch 1:  stats: loss 0.05309703, accuracy 0.66799992, elapsed 2.25     \n",
      "Epoch 248, CIFAR-10 Batch 1:  stats: loss 0.05973884, accuracy 0.65839994, elapsed 2.29     \n",
      "Epoch 249, CIFAR-10 Batch 1:  stats: loss 0.04439605, accuracy 0.66839987, elapsed 2.28     \n",
      "Epoch 250, CIFAR-10 Batch 1:  stats: loss 0.05731408, accuracy 0.65699989, elapsed 2.26     \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "global last_time\n",
    "last_time = time.time()\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  stats: loss 2.29631424, accuracy 0.10859999, elapsed 2.51     \n",
      "Epoch  1, CIFAR-10 Batch 2:  stats: loss 2.24698782, accuracy 0.15220000, elapsed 2.26     \n",
      "Epoch  1, CIFAR-10 Batch 3:  stats: loss 2.02986956, accuracy 0.24339999, elapsed 2.26     \n",
      "Epoch  1, CIFAR-10 Batch 4:  stats: loss 2.05491042, accuracy 0.20140000, elapsed 2.26     \n",
      "Epoch  1, CIFAR-10 Batch 5:  stats: loss 1.94295287, accuracy 0.23859999, elapsed 2.25     \n",
      "Epoch  2, CIFAR-10 Batch 1:  stats: loss 1.97443390, accuracy 0.29120001, elapsed 2.27     \n",
      "Epoch  2, CIFAR-10 Batch 2:  stats: loss 1.95645905, accuracy 0.28199998, elapsed 2.27     \n",
      "Epoch  2, CIFAR-10 Batch 3:  stats: loss 1.81621790, accuracy 0.33299997, elapsed 2.26     \n",
      "Epoch  2, CIFAR-10 Batch 4:  stats: loss 1.76466060, accuracy 0.31359997, elapsed 2.26     \n",
      "Epoch  2, CIFAR-10 Batch 5:  stats: loss 1.74630427, accuracy 0.32979998, elapsed 2.31     \n",
      "Epoch  3, CIFAR-10 Batch 1:  stats: loss 1.90875101, accuracy 0.33120000, elapsed 2.27     \n",
      "Epoch  3, CIFAR-10 Batch 2:  stats: loss 1.73342848, accuracy 0.36100000, elapsed 2.26     \n",
      "Epoch  3, CIFAR-10 Batch 3:  stats: loss 1.67935228, accuracy 0.34759998, elapsed 2.26     \n",
      "Epoch  3, CIFAR-10 Batch 4:  stats: loss 1.66273403, accuracy 0.39280000, elapsed 2.27     \n",
      "Epoch  3, CIFAR-10 Batch 5:  stats: loss 1.61296868, accuracy 0.41299999, elapsed 2.27     \n",
      "Epoch  4, CIFAR-10 Batch 1:  stats: loss 1.67903423, accuracy 0.42659992, elapsed 2.25     \n",
      "Epoch  4, CIFAR-10 Batch 2:  stats: loss 1.57985687, accuracy 0.42580003, elapsed 2.26     \n",
      "Epoch  4, CIFAR-10 Batch 3:  stats: loss 1.43032122, accuracy 0.43659997, elapsed 2.27     \n",
      "Epoch  4, CIFAR-10 Batch 4:  stats: loss 1.49082184, accuracy 0.43579999, elapsed 2.25     \n",
      "Epoch  4, CIFAR-10 Batch 5:  stats: loss 1.50849366, accuracy 0.45820001, elapsed 2.27     \n",
      "Epoch  5, CIFAR-10 Batch 1:  stats: loss 1.57016671, accuracy 0.47719994, elapsed 2.26     \n",
      "Epoch  5, CIFAR-10 Batch 2:  stats: loss 1.46760368, accuracy 0.47479996, elapsed 2.24     \n",
      "Epoch  5, CIFAR-10 Batch 3:  stats: loss 1.32492912, accuracy 0.47499996, elapsed 2.25     \n",
      "Epoch  5, CIFAR-10 Batch 4:  stats: loss 1.46732306, accuracy 0.44499996, elapsed 2.25     \n",
      "Epoch  5, CIFAR-10 Batch 5:  stats: loss 1.46250546, accuracy 0.46339998, elapsed 2.25     \n",
      "Epoch  6, CIFAR-10 Batch 1:  stats: loss 1.48269260, accuracy 0.49519995, elapsed 2.25     \n",
      "Epoch  6, CIFAR-10 Batch 2:  stats: loss 1.43033028, accuracy 0.47699997, elapsed 2.24     \n",
      "Epoch  6, CIFAR-10 Batch 3:  stats: loss 1.21664679, accuracy 0.50279993, elapsed 2.26     \n",
      "Epoch  6, CIFAR-10 Batch 4:  stats: loss 1.34193707, accuracy 0.51019990, elapsed 2.25     \n",
      "Epoch  6, CIFAR-10 Batch 5:  stats: loss 1.34859514, accuracy 0.49379998, elapsed 2.25     \n",
      "Epoch  7, CIFAR-10 Batch 1:  stats: loss 1.40829897, accuracy 0.52519995, elapsed 2.27     \n",
      "Epoch  7, CIFAR-10 Batch 2:  stats: loss 1.40528512, accuracy 0.46439993, elapsed 2.25     \n",
      "Epoch  7, CIFAR-10 Batch 3:  stats: loss 1.30271018, accuracy 0.50439996, elapsed 2.27     \n",
      "Epoch  7, CIFAR-10 Batch 4:  stats: loss 1.25104475, accuracy 0.52800000, elapsed 2.25     \n",
      "Epoch  7, CIFAR-10 Batch 5:  stats: loss 1.31685174, accuracy 0.52339995, elapsed 2.24     \n",
      "Epoch  8, CIFAR-10 Batch 1:  stats: loss 1.33124816, accuracy 0.52559990, elapsed 2.27     \n",
      "Epoch  8, CIFAR-10 Batch 2:  stats: loss 1.34651709, accuracy 0.53299993, elapsed 2.26     \n",
      "Epoch  8, CIFAR-10 Batch 3:  stats: loss 1.20363772, accuracy 0.53960001, elapsed 2.24     \n",
      "Epoch  8, CIFAR-10 Batch 4:  stats: loss 1.22379458, accuracy 0.53919989, elapsed 2.25     \n",
      "Epoch  8, CIFAR-10 Batch 5:  stats: loss 1.27816772, accuracy 0.52859992, elapsed 2.24     \n",
      "Epoch  9, CIFAR-10 Batch 1:  stats: loss 1.33604753, accuracy 0.56299990, elapsed 2.27     \n",
      "Epoch  9, CIFAR-10 Batch 2:  stats: loss 1.25495446, accuracy 0.55679989, elapsed 2.26     \n",
      "Epoch  9, CIFAR-10 Batch 3:  stats: loss 1.15942395, accuracy 0.55139995, elapsed 2.25     \n",
      "Epoch  9, CIFAR-10 Batch 4:  stats: loss 1.22919381, accuracy 0.53819990, elapsed 2.26     \n",
      "Epoch  9, CIFAR-10 Batch 5:  stats: loss 1.20458376, accuracy 0.54219991, elapsed 2.27     \n",
      "Epoch 10, CIFAR-10 Batch 1:  stats: loss 1.25689363, accuracy 0.57359993, elapsed 2.25     \n",
      "Epoch 10, CIFAR-10 Batch 2:  stats: loss 1.18844271, accuracy 0.57559991, elapsed 2.27     \n",
      "Epoch 10, CIFAR-10 Batch 3:  stats: loss 1.11070907, accuracy 0.57439995, elapsed 2.25     \n",
      "Epoch 10, CIFAR-10 Batch 4:  stats: loss 1.16249132, accuracy 0.58039993, elapsed 2.26     \n",
      "Epoch 10, CIFAR-10 Batch 5:  stats: loss 1.13443017, accuracy 0.57959992, elapsed 2.24     \n",
      "Epoch 11, CIFAR-10 Batch 1:  stats: loss 1.15765750, accuracy 0.59419996, elapsed 2.24     \n",
      "Epoch 11, CIFAR-10 Batch 2:  stats: loss 1.13465679, accuracy 0.57599992, elapsed 2.26     \n",
      "Epoch 11, CIFAR-10 Batch 3:  stats: loss 1.05189109, accuracy 0.59699988, elapsed 2.25     \n",
      "Epoch 11, CIFAR-10 Batch 4:  stats: loss 1.08792150, accuracy 0.58879995, elapsed 2.27     \n",
      "Epoch 11, CIFAR-10 Batch 5:  stats: loss 1.10422266, accuracy 0.58719993, elapsed 2.28     \n",
      "Epoch 12, CIFAR-10 Batch 1:  stats: loss 1.13467669, accuracy 0.60199994, elapsed 2.28     \n",
      "Epoch 12, CIFAR-10 Batch 2:  stats: loss 1.11176026, accuracy 0.60179996, elapsed 2.26     \n",
      "Epoch 12, CIFAR-10 Batch 3:  stats: loss 1.03132391, accuracy 0.60159993, elapsed 2.26     \n",
      "Epoch 12, CIFAR-10 Batch 4:  stats: loss 0.98820448, accuracy 0.60859990, elapsed 2.26     \n",
      "Epoch 12, CIFAR-10 Batch 5:  stats: loss 1.12431037, accuracy 0.57159990, elapsed 2.28     \n",
      "Epoch 13, CIFAR-10 Batch 1:  stats: loss 1.15055728, accuracy 0.60299993, elapsed 2.26     \n",
      "Epoch 13, CIFAR-10 Batch 2:  stats: loss 1.16152692, accuracy 0.57899994, elapsed 2.27     \n",
      "Epoch 13, CIFAR-10 Batch 3:  stats: loss 1.04801953, accuracy 0.59959990, elapsed 2.27     \n",
      "Epoch 13, CIFAR-10 Batch 4:  stats: loss 1.00338352, accuracy 0.61359990, elapsed 2.27     \n",
      "Epoch 13, CIFAR-10 Batch 5:  stats: loss 1.10887933, accuracy 0.57639992, elapsed 2.27     \n",
      "Epoch 14, CIFAR-10 Batch 1:  stats: loss 1.08723164, accuracy 0.61739987, elapsed 2.26     \n",
      "Epoch 14, CIFAR-10 Batch 2:  stats: loss 1.04563403, accuracy 0.62299991, elapsed 2.26     \n",
      "Epoch 14, CIFAR-10 Batch 3:  stats: loss 0.98578507, accuracy 0.61359990, elapsed 2.26     \n",
      "Epoch 14, CIFAR-10 Batch 4:  stats: loss 0.93154591, accuracy 0.63479996, elapsed 2.27     \n",
      "Epoch 14, CIFAR-10 Batch 5:  stats: loss 0.99880427, accuracy 0.61999989, elapsed 2.28     \n",
      "Epoch 15, CIFAR-10 Batch 1:  stats: loss 1.10769761, accuracy 0.59479988, elapsed 2.27     \n",
      "Epoch 15, CIFAR-10 Batch 2:  stats: loss 1.05690229, accuracy 0.60459989, elapsed 2.27     \n",
      "Epoch 15, CIFAR-10 Batch 3:  stats: loss 0.95861852, accuracy 0.62999988, elapsed 2.24     \n",
      "Epoch 15, CIFAR-10 Batch 4:  stats: loss 0.92205799, accuracy 0.63779992, elapsed 2.26     \n",
      "Epoch 15, CIFAR-10 Batch 5:  stats: loss 0.98677367, accuracy 0.62399989, elapsed 2.26     \n",
      "Epoch 16, CIFAR-10 Batch 1:  stats: loss 1.06183469, accuracy 0.62079990, elapsed 2.27     \n",
      "Epoch 16, CIFAR-10 Batch 2:  stats: loss 1.04185724, accuracy 0.62059999, elapsed 2.28     \n",
      "Epoch 16, CIFAR-10 Batch 3:  stats: loss 0.90097690, accuracy 0.64819992, elapsed 2.27     \n",
      "Epoch 16, CIFAR-10 Batch 4:  stats: loss 0.94739676, accuracy 0.62439990, elapsed 2.27     \n",
      "Epoch 16, CIFAR-10 Batch 5:  stats: loss 0.92470127, accuracy 0.65159994, elapsed 2.24     \n",
      "Epoch 17, CIFAR-10 Batch 1:  stats: loss 1.00819290, accuracy 0.64799982, elapsed 2.27     \n",
      "Epoch 17, CIFAR-10 Batch 2:  stats: loss 0.99622411, accuracy 0.63299996, elapsed 2.27     \n",
      "Epoch 17, CIFAR-10 Batch 3:  stats: loss 0.86326838, accuracy 0.65339988, elapsed 2.27     \n",
      "Epoch 17, CIFAR-10 Batch 4:  stats: loss 0.87801510, accuracy 0.64799982, elapsed 2.26     \n",
      "Epoch 17, CIFAR-10 Batch 5:  stats: loss 0.90680957, accuracy 0.65039986, elapsed 2.27     \n",
      "Epoch 18, CIFAR-10 Batch 1:  stats: loss 0.98188895, accuracy 0.64019990, elapsed 2.27     \n",
      "Epoch 18, CIFAR-10 Batch 2:  stats: loss 1.04999185, accuracy 0.62279993, elapsed 2.24     \n",
      "Epoch 18, CIFAR-10 Batch 3:  stats: loss 0.88966584, accuracy 0.64859986, elapsed 2.27     \n",
      "Epoch 18, CIFAR-10 Batch 4:  stats: loss 0.92969835, accuracy 0.64099985, elapsed 2.27     \n",
      "Epoch 18, CIFAR-10 Batch 5:  stats: loss 0.90879887, accuracy 0.65659988, elapsed 2.25     \n",
      "Epoch 19, CIFAR-10 Batch 1:  stats: loss 0.94029856, accuracy 0.66119993, elapsed 2.27     \n",
      "Epoch 19, CIFAR-10 Batch 2:  stats: loss 0.94517720, accuracy 0.64499992, elapsed 2.26     \n",
      "Epoch 19, CIFAR-10 Batch 3:  stats: loss 0.81720543, accuracy 0.66099989, elapsed 2.25     \n",
      "Epoch 19, CIFAR-10 Batch 4:  stats: loss 0.82383132, accuracy 0.66359985, elapsed 2.26     \n",
      "Epoch 19, CIFAR-10 Batch 5:  stats: loss 0.88580823, accuracy 0.65739983, elapsed 2.27     \n",
      "Epoch 20, CIFAR-10 Batch 1:  stats: loss 0.95207262, accuracy 0.64639986, elapsed 2.28     \n",
      "Epoch 20, CIFAR-10 Batch 2:  stats: loss 0.99771267, accuracy 0.62079984, elapsed 2.27     \n",
      "Epoch 20, CIFAR-10 Batch 3:  stats: loss 0.83912396, accuracy 0.66039991, elapsed 2.26     \n",
      "Epoch 20, CIFAR-10 Batch 4:  stats: loss 0.87960923, accuracy 0.64199984, elapsed 2.27     \n",
      "Epoch 20, CIFAR-10 Batch 5:  stats: loss 0.85389918, accuracy 0.67479986, elapsed 2.26     \n",
      "Epoch 21, CIFAR-10 Batch 1:  stats: loss 0.92414230, accuracy 0.67159992, elapsed 2.26     \n",
      "Epoch 21, CIFAR-10 Batch 2:  stats: loss 0.89195573, accuracy 0.64919984, elapsed 2.25     \n",
      "Epoch 21, CIFAR-10 Batch 3:  stats: loss 0.80496120, accuracy 0.66639990, elapsed 2.27     \n",
      "Epoch 21, CIFAR-10 Batch 4:  stats: loss 0.79249281, accuracy 0.67099983, elapsed 2.27     \n",
      "Epoch 21, CIFAR-10 Batch 5:  stats: loss 0.82773846, accuracy 0.67399991, elapsed 2.28     \n",
      "Epoch 22, CIFAR-10 Batch 1:  stats: loss 0.87429935, accuracy 0.67579991, elapsed 2.26     \n",
      "Epoch 22, CIFAR-10 Batch 2:  stats: loss 0.86719495, accuracy 0.66379988, elapsed 2.26     \n",
      "Epoch 22, CIFAR-10 Batch 3:  stats: loss 0.79013836, accuracy 0.65359986, elapsed 2.26     \n",
      "Epoch 22, CIFAR-10 Batch 4:  stats: loss 0.80593252, accuracy 0.66459990, elapsed 2.26     \n",
      "Epoch 22, CIFAR-10 Batch 5:  stats: loss 0.81772226, accuracy 0.67379987, elapsed 2.26     \n",
      "Epoch 23, CIFAR-10 Batch 1:  stats: loss 0.85467768, accuracy 0.67599994, elapsed 2.25     \n",
      "Epoch 23, CIFAR-10 Batch 2:  stats: loss 0.85121059, accuracy 0.67599988, elapsed 2.27     \n",
      "Epoch 23, CIFAR-10 Batch 3:  stats: loss 0.76038283, accuracy 0.66899991, elapsed 2.26     \n",
      "Epoch 23, CIFAR-10 Batch 4:  stats: loss 0.72309893, accuracy 0.68939984, elapsed 2.27     \n",
      "Epoch 23, CIFAR-10 Batch 5:  stats: loss 0.77786398, accuracy 0.68939984, elapsed 2.27     \n",
      "Epoch 24, CIFAR-10 Batch 1:  stats: loss 0.94617170, accuracy 0.63679993, elapsed 2.26     \n",
      "Epoch 24, CIFAR-10 Batch 2:  stats: loss 0.90029514, accuracy 0.66039991, elapsed 2.26     \n",
      "Epoch 24, CIFAR-10 Batch 3:  stats: loss 0.76119709, accuracy 0.67319989, elapsed 2.25     \n",
      "Epoch 24, CIFAR-10 Batch 4:  stats: loss 0.78048629, accuracy 0.67359984, elapsed 2.26     \n",
      "Epoch 24, CIFAR-10 Batch 5:  stats: loss 0.84643608, accuracy 0.65579993, elapsed 2.27     \n",
      "Epoch 25, CIFAR-10 Batch 1:  stats: loss 0.87016916, accuracy 0.67979985, elapsed 2.27     \n",
      "Epoch 25, CIFAR-10 Batch 2:  stats: loss 0.82008100, accuracy 0.67839992, elapsed 2.27     \n",
      "Epoch 25, CIFAR-10 Batch 3:  stats: loss 0.74010289, accuracy 0.68499988, elapsed 2.27     \n",
      "Epoch 25, CIFAR-10 Batch 4:  stats: loss 0.71883070, accuracy 0.68679988, elapsed 2.28     \n",
      "Epoch 25, CIFAR-10 Batch 5:  stats: loss 0.76038027, accuracy 0.68159986, elapsed 2.26     \n",
      "Epoch 26, CIFAR-10 Batch 1:  stats: loss 0.80571145, accuracy 0.68979985, elapsed 2.27     \n",
      "Epoch 26, CIFAR-10 Batch 2:  stats: loss 0.81102061, accuracy 0.68959987, elapsed 2.27     \n",
      "Epoch 26, CIFAR-10 Batch 3:  stats: loss 0.70651132, accuracy 0.68899989, elapsed 2.27     \n",
      "Epoch 26, CIFAR-10 Batch 4:  stats: loss 0.70535314, accuracy 0.68379992, elapsed 2.26     \n",
      "Epoch 26, CIFAR-10 Batch 5:  stats: loss 0.74305886, accuracy 0.67779988, elapsed 2.25     \n",
      "Epoch 27, CIFAR-10 Batch 1:  stats: loss 0.81080091, accuracy 0.68459988, elapsed 2.29     \n",
      "Epoch 27, CIFAR-10 Batch 2:  stats: loss 0.79263163, accuracy 0.68439990, elapsed 2.26     \n",
      "Epoch 27, CIFAR-10 Batch 3:  stats: loss 0.70736849, accuracy 0.68679988, elapsed 2.25     \n",
      "Epoch 27, CIFAR-10 Batch 4:  stats: loss 0.67690092, accuracy 0.69959986, elapsed 2.27     \n",
      "Epoch 27, CIFAR-10 Batch 5:  stats: loss 0.77139568, accuracy 0.68479985, elapsed 2.26     \n",
      "Epoch 28, CIFAR-10 Batch 1:  stats: loss 0.76150769, accuracy 0.69299990, elapsed 2.26     \n",
      "Epoch 28, CIFAR-10 Batch 2:  stats: loss 0.78479952, accuracy 0.69119990, elapsed 2.26     \n",
      "Epoch 28, CIFAR-10 Batch 3:  stats: loss 0.68783307, accuracy 0.68559986, elapsed 2.24     \n",
      "Epoch 28, CIFAR-10 Batch 4:  stats: loss 0.78195673, accuracy 0.67459995, elapsed 2.25     \n",
      "Epoch 28, CIFAR-10 Batch 5:  stats: loss 0.75427365, accuracy 0.68039989, elapsed 2.26     \n",
      "Epoch 29, CIFAR-10 Batch 1:  stats: loss 0.79022574, accuracy 0.69319993, elapsed 2.26     \n",
      "Epoch 29, CIFAR-10 Batch 2:  stats: loss 0.75101566, accuracy 0.69799989, elapsed 2.26     \n",
      "Epoch 29, CIFAR-10 Batch 3:  stats: loss 0.70074433, accuracy 0.69759989, elapsed 2.28     \n",
      "Epoch 29, CIFAR-10 Batch 4:  stats: loss 0.68509430, accuracy 0.69199985, elapsed 2.24     \n",
      "Epoch 29, CIFAR-10 Batch 5:  stats: loss 0.68420285, accuracy 0.70199984, elapsed 2.25     \n",
      "Epoch 30, CIFAR-10 Batch 1:  stats: loss 0.75758445, accuracy 0.69319987, elapsed 2.26     \n",
      "Epoch 30, CIFAR-10 Batch 2:  stats: loss 0.75176060, accuracy 0.69519985, elapsed 2.25     \n",
      "Epoch 30, CIFAR-10 Batch 3:  stats: loss 0.69716030, accuracy 0.67779988, elapsed 2.27     \n",
      "Epoch 30, CIFAR-10 Batch 4:  stats: loss 0.65197563, accuracy 0.69479990, elapsed 2.27     \n",
      "Epoch 30, CIFAR-10 Batch 5:  stats: loss 0.68572986, accuracy 0.69419992, elapsed 2.26     \n",
      "Epoch 31, CIFAR-10 Batch 1:  stats: loss 0.75534868, accuracy 0.69839990, elapsed 2.29     \n",
      "Epoch 31, CIFAR-10 Batch 2:  stats: loss 0.71812046, accuracy 0.70459992, elapsed 2.26     \n",
      "Epoch 31, CIFAR-10 Batch 3:  stats: loss 0.60036916, accuracy 0.71279985, elapsed 2.26     \n",
      "Epoch 31, CIFAR-10 Batch 4:  stats: loss 0.63979173, accuracy 0.69899982, elapsed 2.24     \n",
      "Epoch 31, CIFAR-10 Batch 5:  stats: loss 0.68891478, accuracy 0.69359988, elapsed 2.26     \n",
      "Epoch 32, CIFAR-10 Batch 1:  stats: loss 0.77924490, accuracy 0.70199984, elapsed 2.27     \n",
      "Epoch 32, CIFAR-10 Batch 2:  stats: loss 0.74934965, accuracy 0.69739985, elapsed 2.26     \n",
      "Epoch 32, CIFAR-10 Batch 3:  stats: loss 0.62709308, accuracy 0.70719987, elapsed 2.26     \n",
      "Epoch 32, CIFAR-10 Batch 4:  stats: loss 0.63220316, accuracy 0.70579994, elapsed 2.26     \n",
      "Epoch 32, CIFAR-10 Batch 5:  stats: loss 0.66616189, accuracy 0.69479990, elapsed 2.25     \n",
      "Epoch 33, CIFAR-10 Batch 1:  stats: loss 0.74342173, accuracy 0.71039987, elapsed 2.29     \n",
      "Epoch 33, CIFAR-10 Batch 2:  stats: loss 0.72142684, accuracy 0.68859988, elapsed 2.27     \n",
      "Epoch 33, CIFAR-10 Batch 3:  stats: loss 0.65008557, accuracy 0.69599980, elapsed 2.25     \n",
      "Epoch 33, CIFAR-10 Batch 4:  stats: loss 0.63472360, accuracy 0.71059984, elapsed 2.25     \n",
      "Epoch 33, CIFAR-10 Batch 5:  stats: loss 0.66163629, accuracy 0.70359993, elapsed 2.27     \n",
      "Epoch 34, CIFAR-10 Batch 1:  stats: loss 0.69631439, accuracy 0.71499985, elapsed 2.26     \n",
      "Epoch 34, CIFAR-10 Batch 2:  stats: loss 0.67293096, accuracy 0.70719987, elapsed 2.26     \n",
      "Epoch 34, CIFAR-10 Batch 3:  stats: loss 0.59113950, accuracy 0.71659982, elapsed 2.28     \n",
      "Epoch 34, CIFAR-10 Batch 4:  stats: loss 0.58453804, accuracy 0.71679986, elapsed 2.28     \n",
      "Epoch 34, CIFAR-10 Batch 5:  stats: loss 0.67658788, accuracy 0.70799983, elapsed 2.28     \n",
      "Epoch 35, CIFAR-10 Batch 1:  stats: loss 0.74739265, accuracy 0.69439983, elapsed 2.27     \n",
      "Epoch 35, CIFAR-10 Batch 2:  stats: loss 0.71233106, accuracy 0.70499980, elapsed 2.28     \n",
      "Epoch 35, CIFAR-10 Batch 3:  stats: loss 0.58493352, accuracy 0.71959984, elapsed 2.26     \n",
      "Epoch 35, CIFAR-10 Batch 4:  stats: loss 0.58541250, accuracy 0.70579982, elapsed 2.27     \n",
      "Epoch 35, CIFAR-10 Batch 5:  stats: loss 0.65323448, accuracy 0.70939988, elapsed 2.25     \n",
      "Epoch 36, CIFAR-10 Batch 1:  stats: loss 0.69951761, accuracy 0.71939993, elapsed 2.29     \n",
      "Epoch 36, CIFAR-10 Batch 2:  stats: loss 0.67819089, accuracy 0.70979989, elapsed 2.28     \n",
      "Epoch 36, CIFAR-10 Batch 3:  stats: loss 0.56675982, accuracy 0.72139984, elapsed 2.28     \n",
      "Epoch 36, CIFAR-10 Batch 4:  stats: loss 0.57382363, accuracy 0.71879983, elapsed 2.27     \n",
      "Epoch 36, CIFAR-10 Batch 5:  stats: loss 0.65008169, accuracy 0.71099985, elapsed 2.24     \n",
      "Epoch 37, CIFAR-10 Batch 1:  stats: loss 0.72957718, accuracy 0.70579994, elapsed 2.27     \n",
      "Epoch 37, CIFAR-10 Batch 2:  stats: loss 0.69077849, accuracy 0.70199990, elapsed 2.26     \n",
      "Epoch 37, CIFAR-10 Batch 3:  stats: loss 0.59872091, accuracy 0.70339990, elapsed 2.26     \n",
      "Epoch 37, CIFAR-10 Batch 4:  stats: loss 0.61734164, accuracy 0.72099990, elapsed 2.26     \n",
      "Epoch 37, CIFAR-10 Batch 5:  stats: loss 0.64283991, accuracy 0.71739984, elapsed 2.27     \n",
      "Epoch 38, CIFAR-10 Batch 1:  stats: loss 0.68597293, accuracy 0.71419984, elapsed 2.24     \n",
      "Epoch 38, CIFAR-10 Batch 2:  stats: loss 0.68538797, accuracy 0.70199990, elapsed 2.26     \n",
      "Epoch 38, CIFAR-10 Batch 3:  stats: loss 0.58323491, accuracy 0.70319986, elapsed 2.25     \n",
      "Epoch 38, CIFAR-10 Batch 4:  stats: loss 0.56982976, accuracy 0.72379982, elapsed 2.27     \n",
      "Epoch 38, CIFAR-10 Batch 5:  stats: loss 0.67281646, accuracy 0.70379984, elapsed 2.27     \n",
      "Epoch 39, CIFAR-10 Batch 1:  stats: loss 0.67427206, accuracy 0.72219986, elapsed 2.27     \n",
      "Epoch 39, CIFAR-10 Batch 2:  stats: loss 0.64489770, accuracy 0.72079986, elapsed 2.25     \n",
      "Epoch 39, CIFAR-10 Batch 3:  stats: loss 0.54707652, accuracy 0.72819984, elapsed 2.28     \n",
      "Epoch 39, CIFAR-10 Batch 4:  stats: loss 0.54262006, accuracy 0.72379977, elapsed 2.28     \n",
      "Epoch 39, CIFAR-10 Batch 5:  stats: loss 0.61650407, accuracy 0.71339989, elapsed 2.27     \n",
      "Epoch 40, CIFAR-10 Batch 1:  stats: loss 0.64886397, accuracy 0.73119980, elapsed 2.25     \n",
      "Epoch 40, CIFAR-10 Batch 2:  stats: loss 0.65243673, accuracy 0.72619987, elapsed 2.28     \n",
      "Epoch 40, CIFAR-10 Batch 3:  stats: loss 0.51362628, accuracy 0.73179990, elapsed 2.27     \n",
      "Epoch 40, CIFAR-10 Batch 4:  stats: loss 0.55866510, accuracy 0.72059989, elapsed 2.25     \n",
      "Epoch 40, CIFAR-10 Batch 5:  stats: loss 0.60903770, accuracy 0.71779984, elapsed 2.27     \n",
      "Epoch 41, CIFAR-10 Batch 1:  stats: loss 0.64560163, accuracy 0.72539985, elapsed 2.27     \n",
      "Epoch 41, CIFAR-10 Batch 2:  stats: loss 0.62753642, accuracy 0.72219980, elapsed 2.26     \n",
      "Epoch 41, CIFAR-10 Batch 3:  stats: loss 0.56908399, accuracy 0.72919983, elapsed 2.25     \n",
      "Epoch 41, CIFAR-10 Batch 4:  stats: loss 0.59421301, accuracy 0.71359986, elapsed 2.25     \n",
      "Epoch 41, CIFAR-10 Batch 5:  stats: loss 0.60072273, accuracy 0.73039991, elapsed 2.24     \n",
      "Epoch 42, CIFAR-10 Batch 1:  stats: loss 0.66318327, accuracy 0.72059989, elapsed 2.28     \n",
      "Epoch 42, CIFAR-10 Batch 2:  stats: loss 0.64731616, accuracy 0.72879988, elapsed 2.26     \n",
      "Epoch 42, CIFAR-10 Batch 3:  stats: loss 0.53424370, accuracy 0.73859984, elapsed 2.27     \n",
      "Epoch 42, CIFAR-10 Batch 4:  stats: loss 0.53976208, accuracy 0.72539985, elapsed 2.27     \n",
      "Epoch 42, CIFAR-10 Batch 5:  stats: loss 0.60094047, accuracy 0.72279990, elapsed 2.28     \n",
      "Epoch 43, CIFAR-10 Batch 1:  stats: loss 0.62818611, accuracy 0.73479986, elapsed 2.27     \n",
      "Epoch 43, CIFAR-10 Batch 2:  stats: loss 0.59954786, accuracy 0.72439992, elapsed 2.26     \n",
      "Epoch 43, CIFAR-10 Batch 3:  stats: loss 0.54550380, accuracy 0.73419988, elapsed 2.23     \n",
      "Epoch 43, CIFAR-10 Batch 4:  stats: loss 0.52188319, accuracy 0.73699981, elapsed 2.28     \n",
      "Epoch 43, CIFAR-10 Batch 5:  stats: loss 0.58135915, accuracy 0.72199976, elapsed 2.27     \n",
      "Epoch 44, CIFAR-10 Batch 1:  stats: loss 0.62375289, accuracy 0.73079979, elapsed 2.25     \n",
      "Epoch 44, CIFAR-10 Batch 2:  stats: loss 0.62421709, accuracy 0.71479982, elapsed 2.26     \n",
      "Epoch 44, CIFAR-10 Batch 3:  stats: loss 0.53616589, accuracy 0.73359984, elapsed 2.26     \n",
      "Epoch 44, CIFAR-10 Batch 4:  stats: loss 0.54583740, accuracy 0.72839981, elapsed 2.27     \n",
      "Epoch 44, CIFAR-10 Batch 5:  stats: loss 0.56788570, accuracy 0.73499990, elapsed 2.28     \n",
      "Epoch 45, CIFAR-10 Batch 1:  stats: loss 0.64090848, accuracy 0.72479987, elapsed 2.27     \n",
      "Epoch 45, CIFAR-10 Batch 2:  stats: loss 0.58362967, accuracy 0.73059988, elapsed 2.27     \n",
      "Epoch 45, CIFAR-10 Batch 3:  stats: loss 0.56316447, accuracy 0.72399986, elapsed 2.26     \n",
      "Epoch 45, CIFAR-10 Batch 4:  stats: loss 0.50639939, accuracy 0.73299992, elapsed 2.27     \n",
      "Epoch 45, CIFAR-10 Batch 5:  stats: loss 0.58440316, accuracy 0.72539979, elapsed 2.27     \n",
      "Epoch 46, CIFAR-10 Batch 1:  stats: loss 0.61861426, accuracy 0.73559988, elapsed 2.28     \n",
      "Epoch 46, CIFAR-10 Batch 2:  stats: loss 0.59059215, accuracy 0.73159987, elapsed 2.27     \n",
      "Epoch 46, CIFAR-10 Batch 3:  stats: loss 0.60624254, accuracy 0.70959985, elapsed 2.25     \n",
      "Epoch 46, CIFAR-10 Batch 4:  stats: loss 0.57487732, accuracy 0.72559983, elapsed 2.27     \n",
      "Epoch 46, CIFAR-10 Batch 5:  stats: loss 0.62469441, accuracy 0.70479989, elapsed 2.26     \n",
      "Epoch 47, CIFAR-10 Batch 1:  stats: loss 0.62929356, accuracy 0.71779990, elapsed 2.27     \n",
      "Epoch 47, CIFAR-10 Batch 2:  stats: loss 0.59290850, accuracy 0.72099984, elapsed 2.26     \n",
      "Epoch 47, CIFAR-10 Batch 3:  stats: loss 0.61533272, accuracy 0.70359987, elapsed 2.27     \n",
      "Epoch 47, CIFAR-10 Batch 4:  stats: loss 0.56992370, accuracy 0.72299987, elapsed 2.24     \n",
      "Epoch 47, CIFAR-10 Batch 5:  stats: loss 0.60472488, accuracy 0.72399992, elapsed 2.28     \n",
      "Epoch 48, CIFAR-10 Batch 1:  stats: loss 0.62431520, accuracy 0.73319989, elapsed 2.28     \n",
      "Epoch 48, CIFAR-10 Batch 2:  stats: loss 0.56706011, accuracy 0.73479986, elapsed 2.27     \n",
      "Epoch 48, CIFAR-10 Batch 3:  stats: loss 0.53133845, accuracy 0.72159982, elapsed 2.26     \n",
      "Epoch 48, CIFAR-10 Batch 4:  stats: loss 0.58120203, accuracy 0.71099985, elapsed 2.26     \n",
      "Epoch 48, CIFAR-10 Batch 5:  stats: loss 0.57353294, accuracy 0.72839981, elapsed 2.27     \n",
      "Epoch 49, CIFAR-10 Batch 1:  stats: loss 0.63473606, accuracy 0.72879982, elapsed 2.27     \n",
      "Epoch 49, CIFAR-10 Batch 2:  stats: loss 0.58625221, accuracy 0.73179984, elapsed 2.26     \n",
      "Epoch 49, CIFAR-10 Batch 3:  stats: loss 0.48608625, accuracy 0.73639983, elapsed 2.28     \n",
      "Epoch 49, CIFAR-10 Batch 4:  stats: loss 0.52612364, accuracy 0.73439980, elapsed 2.28     \n",
      "Epoch 49, CIFAR-10 Batch 5:  stats: loss 0.57224202, accuracy 0.72679985, elapsed 2.28     \n",
      "Epoch 50, CIFAR-10 Batch 1:  stats: loss 0.59843677, accuracy 0.72759980, elapsed 2.27     \n",
      "Epoch 50, CIFAR-10 Batch 2:  stats: loss 0.56960720, accuracy 0.73499984, elapsed 2.27     \n",
      "Epoch 50, CIFAR-10 Batch 3:  stats: loss 0.49301609, accuracy 0.73559976, elapsed 2.27     \n",
      "Epoch 50, CIFAR-10 Batch 4:  stats: loss 0.52094686, accuracy 0.72979987, elapsed 2.27     \n",
      "Epoch 50, CIFAR-10 Batch 5:  stats: loss 0.59655756, accuracy 0.71559989, elapsed 2.26     \n",
      "Epoch 51, CIFAR-10 Batch 1:  stats: loss 0.62201554, accuracy 0.74039984, elapsed 2.27     \n",
      "Epoch 51, CIFAR-10 Batch 2:  stats: loss 0.57539952, accuracy 0.72719991, elapsed 2.26     \n",
      "Epoch 51, CIFAR-10 Batch 3:  stats: loss 0.50586963, accuracy 0.73539990, elapsed 2.27     \n",
      "Epoch 51, CIFAR-10 Batch 4:  stats: loss 0.56558365, accuracy 0.71899986, elapsed 2.27     \n",
      "Epoch 51, CIFAR-10 Batch 5:  stats: loss 0.55374438, accuracy 0.72799981, elapsed 2.27     \n",
      "Epoch 52, CIFAR-10 Batch 1:  stats: loss 0.61268032, accuracy 0.73599982, elapsed 2.29     \n",
      "Epoch 52, CIFAR-10 Batch 2:  stats: loss 0.52957678, accuracy 0.74559987, elapsed 2.27     \n",
      "Epoch 52, CIFAR-10 Batch 3:  stats: loss 0.47622859, accuracy 0.73399985, elapsed 2.28     \n",
      "Epoch 52, CIFAR-10 Batch 4:  stats: loss 0.53912532, accuracy 0.72599983, elapsed 2.25     \n",
      "Epoch 52, CIFAR-10 Batch 5:  stats: loss 0.54915309, accuracy 0.73239988, elapsed 2.27     \n",
      "Epoch 53, CIFAR-10 Batch 1:  stats: loss 0.58593178, accuracy 0.73779982, elapsed 2.28     \n",
      "Epoch 53, CIFAR-10 Batch 2:  stats: loss 0.54598975, accuracy 0.74259990, elapsed 2.27     \n",
      "Epoch 53, CIFAR-10 Batch 3:  stats: loss 0.49700585, accuracy 0.72879988, elapsed 2.26     \n",
      "Epoch 53, CIFAR-10 Batch 4:  stats: loss 0.48275149, accuracy 0.74279988, elapsed 2.28     \n",
      "Epoch 53, CIFAR-10 Batch 5:  stats: loss 0.50422931, accuracy 0.73299986, elapsed 2.27     \n",
      "Epoch 54, CIFAR-10 Batch 1:  stats: loss 0.56734860, accuracy 0.74959987, elapsed 2.27     \n",
      "Epoch 54, CIFAR-10 Batch 2:  stats: loss 0.60042095, accuracy 0.71759987, elapsed 2.24     \n",
      "Epoch 54, CIFAR-10 Batch 3:  stats: loss 0.42865449, accuracy 0.74439979, elapsed 2.27     \n",
      "Epoch 54, CIFAR-10 Batch 4:  stats: loss 0.53006703, accuracy 0.72799993, elapsed 2.26     \n",
      "Epoch 54, CIFAR-10 Batch 5:  stats: loss 0.51508188, accuracy 0.73499984, elapsed 2.27     \n",
      "Epoch 55, CIFAR-10 Batch 1:  stats: loss 0.56425560, accuracy 0.74419987, elapsed 2.24     \n",
      "Epoch 55, CIFAR-10 Batch 2:  stats: loss 0.54147136, accuracy 0.73739982, elapsed 2.27     \n",
      "Epoch 55, CIFAR-10 Batch 3:  stats: loss 0.47510013, accuracy 0.72699982, elapsed 2.27     \n",
      "Epoch 55, CIFAR-10 Batch 4:  stats: loss 0.45924979, accuracy 0.74719989, elapsed 2.26     \n",
      "Epoch 55, CIFAR-10 Batch 5:  stats: loss 0.53401506, accuracy 0.73379982, elapsed 2.28     \n",
      "Epoch 56, CIFAR-10 Batch 1:  stats: loss 0.55157346, accuracy 0.74639988, elapsed 2.28     \n",
      "Epoch 56, CIFAR-10 Batch 2:  stats: loss 0.52577531, accuracy 0.73779982, elapsed 2.27     \n",
      "Epoch 56, CIFAR-10 Batch 3:  stats: loss 0.45496863, accuracy 0.73839986, elapsed 2.24     \n",
      "Epoch 56, CIFAR-10 Batch 4:  stats: loss 0.50887603, accuracy 0.71999991, elapsed 2.27     \n",
      "Epoch 56, CIFAR-10 Batch 5:  stats: loss 0.52558368, accuracy 0.74399990, elapsed 2.27     \n",
      "Epoch 57, CIFAR-10 Batch 1:  stats: loss 0.53600323, accuracy 0.74619991, elapsed 2.27     \n",
      "Epoch 57, CIFAR-10 Batch 2:  stats: loss 0.51127040, accuracy 0.74779981, elapsed 2.27     \n",
      "Epoch 57, CIFAR-10 Batch 3:  stats: loss 0.43315145, accuracy 0.74579984, elapsed 2.27     \n",
      "Epoch 57, CIFAR-10 Batch 4:  stats: loss 0.47814518, accuracy 0.74219990, elapsed 2.25     \n",
      "Epoch 57, CIFAR-10 Batch 5:  stats: loss 0.55694491, accuracy 0.71699989, elapsed 2.27     \n",
      "Epoch 58, CIFAR-10 Batch 1:  stats: loss 0.55676156, accuracy 0.74859977, elapsed 2.27     \n",
      "Epoch 58, CIFAR-10 Batch 2:  stats: loss 0.53151697, accuracy 0.74799979, elapsed 2.25     \n",
      "Epoch 58, CIFAR-10 Batch 3:  stats: loss 0.48006472, accuracy 0.74179983, elapsed 2.25     \n",
      "Epoch 58, CIFAR-10 Batch 4:  stats: loss 0.45348868, accuracy 0.74299979, elapsed 2.27     \n",
      "Epoch 58, CIFAR-10 Batch 5:  stats: loss 0.51801026, accuracy 0.73999977, elapsed 2.27     \n",
      "Epoch 59, CIFAR-10 Batch 1:  stats: loss 0.53782153, accuracy 0.74739993, elapsed 2.27     \n",
      "Epoch 59, CIFAR-10 Batch 2:  stats: loss 0.50373560, accuracy 0.73599982, elapsed 2.27     \n",
      "Epoch 59, CIFAR-10 Batch 3:  stats: loss 0.44069612, accuracy 0.73679990, elapsed 2.26     \n",
      "Epoch 59, CIFAR-10 Batch 4:  stats: loss 0.45721930, accuracy 0.74519986, elapsed 2.24     \n",
      "Epoch 59, CIFAR-10 Batch 5:  stats: loss 0.53052568, accuracy 0.73999989, elapsed 2.27     \n",
      "Epoch 60, CIFAR-10 Batch 1:  stats: loss 0.56619704, accuracy 0.74079978, elapsed 2.26     \n",
      "Epoch 60, CIFAR-10 Batch 2:  stats: loss 0.51744819, accuracy 0.74439985, elapsed 2.26     \n",
      "Epoch 60, CIFAR-10 Batch 3:  stats: loss 0.44555745, accuracy 0.74599987, elapsed 2.26     \n",
      "Epoch 60, CIFAR-10 Batch 4:  stats: loss 0.44906425, accuracy 0.73999983, elapsed 2.26     \n",
      "Epoch 60, CIFAR-10 Batch 5:  stats: loss 0.52576083, accuracy 0.73579985, elapsed 2.26     \n",
      "Epoch 61, CIFAR-10 Batch 1:  stats: loss 0.52941155, accuracy 0.74579984, elapsed 2.28     \n",
      "Epoch 61, CIFAR-10 Batch 2:  stats: loss 0.48347867, accuracy 0.74499989, elapsed 2.27     \n",
      "Epoch 61, CIFAR-10 Batch 3:  stats: loss 0.43895543, accuracy 0.73419976, elapsed 2.26     \n",
      "Epoch 61, CIFAR-10 Batch 4:  stats: loss 0.49272755, accuracy 0.72779989, elapsed 2.27     \n",
      "Epoch 61, CIFAR-10 Batch 5:  stats: loss 0.51882190, accuracy 0.74039984, elapsed 2.26     \n",
      "Epoch 62, CIFAR-10 Batch 1:  stats: loss 0.53433883, accuracy 0.74379981, elapsed 2.27     \n",
      "Epoch 62, CIFAR-10 Batch 2:  stats: loss 0.49452946, accuracy 0.74839979, elapsed 2.24     \n",
      "Epoch 62, CIFAR-10 Batch 3:  stats: loss 0.42988876, accuracy 0.74459982, elapsed 2.26     \n",
      "Epoch 62, CIFAR-10 Batch 4:  stats: loss 0.47902650, accuracy 0.72439992, elapsed 2.25     \n",
      "Epoch 62, CIFAR-10 Batch 5:  stats: loss 0.48226118, accuracy 0.74999988, elapsed 2.26     \n",
      "Epoch 63, CIFAR-10 Batch 1:  stats: loss 0.50537705, accuracy 0.75599986, elapsed 2.27     \n",
      "Epoch 63, CIFAR-10 Batch 2:  stats: loss 0.47313881, accuracy 0.74899983, elapsed 2.24     \n",
      "Epoch 63, CIFAR-10 Batch 3:  stats: loss 0.43534163, accuracy 0.73879981, elapsed 2.26     \n",
      "Epoch 63, CIFAR-10 Batch 4:  stats: loss 0.46134204, accuracy 0.73679984, elapsed 2.26     \n",
      "Epoch 63, CIFAR-10 Batch 5:  stats: loss 0.48275957, accuracy 0.74719983, elapsed 2.25     \n",
      "Epoch 64, CIFAR-10 Batch 1:  stats: loss 0.52979445, accuracy 0.74399990, elapsed 2.27     \n",
      "Epoch 64, CIFAR-10 Batch 2:  stats: loss 0.47302201, accuracy 0.74359983, elapsed 2.25     \n",
      "Epoch 64, CIFAR-10 Batch 3:  stats: loss 0.42788258, accuracy 0.74599987, elapsed 2.25     \n",
      "Epoch 64, CIFAR-10 Batch 4:  stats: loss 0.45769078, accuracy 0.74419987, elapsed 2.26     \n",
      "Epoch 64, CIFAR-10 Batch 5:  stats: loss 0.48099005, accuracy 0.73179984, elapsed 2.27     \n",
      "Epoch 65, CIFAR-10 Batch 1:  stats: loss 0.50867581, accuracy 0.75239980, elapsed 2.26     \n",
      "Epoch 65, CIFAR-10 Batch 2:  stats: loss 0.48128170, accuracy 0.73779988, elapsed 2.28     \n",
      "Epoch 65, CIFAR-10 Batch 3:  stats: loss 0.41183841, accuracy 0.74879986, elapsed 2.25     \n",
      "Epoch 65, CIFAR-10 Batch 4:  stats: loss 0.42263150, accuracy 0.75199986, elapsed 2.24     \n",
      "Epoch 65, CIFAR-10 Batch 5:  stats: loss 0.45079201, accuracy 0.75239980, elapsed 2.25     \n",
      "Epoch 66, CIFAR-10 Batch 1:  stats: loss 0.49528322, accuracy 0.75759989, elapsed 2.26     \n",
      "Epoch 66, CIFAR-10 Batch 2:  stats: loss 0.48222601, accuracy 0.74879980, elapsed 2.26     \n",
      "Epoch 66, CIFAR-10 Batch 3:  stats: loss 0.42577741, accuracy 0.73859984, elapsed 2.30     \n",
      "Epoch 66, CIFAR-10 Batch 4:  stats: loss 0.42075971, accuracy 0.75299978, elapsed 2.28     \n",
      "Epoch 66, CIFAR-10 Batch 5:  stats: loss 0.50941557, accuracy 0.73059988, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 1:  stats: loss 0.53103065, accuracy 0.75019985, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 2:  stats: loss 0.44555175, accuracy 0.75319982, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 3:  stats: loss 0.41317701, accuracy 0.75379986, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 4:  stats: loss 0.43410927, accuracy 0.74079984, elapsed 2.26     \n",
      "Epoch 67, CIFAR-10 Batch 5:  stats: loss 0.48449829, accuracy 0.74619991, elapsed 2.26     \n",
      "Epoch 68, CIFAR-10 Batch 1:  stats: loss 0.52582473, accuracy 0.76099992, elapsed 2.27     \n",
      "Epoch 68, CIFAR-10 Batch 2:  stats: loss 0.49485222, accuracy 0.74839985, elapsed 2.23     \n",
      "Epoch 68, CIFAR-10 Batch 3:  stats: loss 0.45452809, accuracy 0.74079978, elapsed 2.25     \n",
      "Epoch 68, CIFAR-10 Batch 4:  stats: loss 0.40948653, accuracy 0.75019985, elapsed 2.25     \n",
      "Epoch 68, CIFAR-10 Batch 5:  stats: loss 0.47099948, accuracy 0.74759984, elapsed 2.26     \n",
      "Epoch 69, CIFAR-10 Batch 1:  stats: loss 0.54204798, accuracy 0.74139982, elapsed 2.27     \n",
      "Epoch 69, CIFAR-10 Batch 2:  stats: loss 0.48319447, accuracy 0.74719977, elapsed 2.27     \n",
      "Epoch 69, CIFAR-10 Batch 3:  stats: loss 0.45181382, accuracy 0.73879993, elapsed 2.28     \n",
      "Epoch 69, CIFAR-10 Batch 4:  stats: loss 0.46306121, accuracy 0.72399986, elapsed 2.26     \n",
      "Epoch 69, CIFAR-10 Batch 5:  stats: loss 0.47842395, accuracy 0.74479979, elapsed 2.26     \n",
      "Epoch 70, CIFAR-10 Batch 1:  stats: loss 0.48348093, accuracy 0.75359976, elapsed 2.26     \n",
      "Epoch 70, CIFAR-10 Batch 2:  stats: loss 0.45380774, accuracy 0.74419987, elapsed 2.26     \n",
      "Epoch 70, CIFAR-10 Batch 3:  stats: loss 0.39841351, accuracy 0.75459981, elapsed 2.27     \n",
      "Epoch 70, CIFAR-10 Batch 4:  stats: loss 0.40943211, accuracy 0.74699980, elapsed 2.25     \n",
      "Epoch 70, CIFAR-10 Batch 5:  stats: loss 0.47535741, accuracy 0.73939985, elapsed 2.26     \n",
      "Epoch 71, CIFAR-10 Batch 1:  stats: loss 0.51664776, accuracy 0.74999988, elapsed 2.28     \n",
      "Epoch 71, CIFAR-10 Batch 2:  stats: loss 0.42268747, accuracy 0.75359976, elapsed 2.26     \n",
      "Epoch 71, CIFAR-10 Batch 3:  stats: loss 0.41257763, accuracy 0.75819981, elapsed 2.24     \n",
      "Epoch 71, CIFAR-10 Batch 4:  stats: loss 0.41844943, accuracy 0.75179982, elapsed 2.25     \n",
      "Epoch 71, CIFAR-10 Batch 5:  stats: loss 0.45597255, accuracy 0.74379992, elapsed 2.25     \n",
      "Epoch 72, CIFAR-10 Batch 1:  stats: loss 0.49752635, accuracy 0.75419986, elapsed 2.25     \n",
      "Epoch 72, CIFAR-10 Batch 2:  stats: loss 0.43545735, accuracy 0.75219989, elapsed 2.25     \n",
      "Epoch 72, CIFAR-10 Batch 3:  stats: loss 0.41948512, accuracy 0.75519985, elapsed 2.29     \n",
      "Epoch 72, CIFAR-10 Batch 4:  stats: loss 0.40149459, accuracy 0.74539983, elapsed 2.29     \n",
      "Epoch 72, CIFAR-10 Batch 5:  stats: loss 0.44667083, accuracy 0.75019991, elapsed 2.26     \n",
      "Epoch 73, CIFAR-10 Batch 1:  stats: loss 0.50378919, accuracy 0.75439990, elapsed 2.29     \n",
      "Epoch 73, CIFAR-10 Batch 2:  stats: loss 0.46405369, accuracy 0.73619986, elapsed 2.26     \n",
      "Epoch 73, CIFAR-10 Batch 3:  stats: loss 0.40599898, accuracy 0.75419980, elapsed 2.26     \n",
      "Epoch 73, CIFAR-10 Batch 4:  stats: loss 0.41483459, accuracy 0.74479985, elapsed 2.26     \n",
      "Epoch 73, CIFAR-10 Batch 5:  stats: loss 0.47157055, accuracy 0.73859984, elapsed 2.27     \n",
      "Epoch 74, CIFAR-10 Batch 1:  stats: loss 0.47011852, accuracy 0.75539988, elapsed 2.27     \n",
      "Epoch 74, CIFAR-10 Batch 2:  stats: loss 0.41564041, accuracy 0.75779986, elapsed 2.26     \n",
      "Epoch 74, CIFAR-10 Batch 3:  stats: loss 0.38855845, accuracy 0.75099981, elapsed 2.27     \n",
      "Epoch 74, CIFAR-10 Batch 4:  stats: loss 0.40663859, accuracy 0.75079989, elapsed 2.26     \n",
      "Epoch 74, CIFAR-10 Batch 5:  stats: loss 0.42067477, accuracy 0.75239980, elapsed 2.28     \n",
      "Epoch 75, CIFAR-10 Batch 1:  stats: loss 0.47396922, accuracy 0.76059985, elapsed 2.27     \n",
      "Epoch 75, CIFAR-10 Batch 2:  stats: loss 0.41649976, accuracy 0.75899982, elapsed 2.26     \n",
      "Epoch 75, CIFAR-10 Batch 3:  stats: loss 0.36937797, accuracy 0.75379986, elapsed 2.24     \n",
      "Epoch 75, CIFAR-10 Batch 4:  stats: loss 0.40087929, accuracy 0.74579984, elapsed 2.26     \n",
      "Epoch 75, CIFAR-10 Batch 5:  stats: loss 0.49387127, accuracy 0.71939981, elapsed 2.27     \n",
      "Epoch 76, CIFAR-10 Batch 1:  stats: loss 0.50994456, accuracy 0.75359982, elapsed 2.27     \n",
      "Epoch 76, CIFAR-10 Batch 2:  stats: loss 0.44214299, accuracy 0.75519985, elapsed 2.26     \n",
      "Epoch 76, CIFAR-10 Batch 3:  stats: loss 0.37850913, accuracy 0.74199986, elapsed 2.27     \n",
      "Epoch 76, CIFAR-10 Batch 4:  stats: loss 0.36346638, accuracy 0.75399995, elapsed 2.25     \n",
      "Epoch 76, CIFAR-10 Batch 5:  stats: loss 0.44123644, accuracy 0.75339985, elapsed 2.26     \n",
      "Epoch 77, CIFAR-10 Batch 1:  stats: loss 0.47484142, accuracy 0.76159978, elapsed 2.29     \n",
      "Epoch 77, CIFAR-10 Batch 2:  stats: loss 0.42387772, accuracy 0.75659984, elapsed 2.27     \n",
      "Epoch 77, CIFAR-10 Batch 3:  stats: loss 0.39863545, accuracy 0.74379981, elapsed 2.25     \n",
      "Epoch 77, CIFAR-10 Batch 4:  stats: loss 0.41955578, accuracy 0.74239981, elapsed 2.27     \n",
      "Epoch 77, CIFAR-10 Batch 5:  stats: loss 0.43157175, accuracy 0.75199986, elapsed 2.27     \n",
      "Epoch 78, CIFAR-10 Batch 1:  stats: loss 0.44208890, accuracy 0.76039982, elapsed 2.26     \n",
      "Epoch 78, CIFAR-10 Batch 2:  stats: loss 0.43597695, accuracy 0.74959981, elapsed 2.26     \n",
      "Epoch 78, CIFAR-10 Batch 3:  stats: loss 0.40076542, accuracy 0.73959982, elapsed 2.27     \n",
      "Epoch 78, CIFAR-10 Batch 4:  stats: loss 0.41247624, accuracy 0.74759984, elapsed 2.26     \n",
      "Epoch 78, CIFAR-10 Batch 5:  stats: loss 0.45440584, accuracy 0.74199986, elapsed 2.24     \n",
      "Epoch 79, CIFAR-10 Batch 1:  stats: loss 0.46377164, accuracy 0.75799990, elapsed 2.26     \n",
      "Epoch 79, CIFAR-10 Batch 2:  stats: loss 0.39111060, accuracy 0.75739980, elapsed 2.27     \n",
      "Epoch 79, CIFAR-10 Batch 3:  stats: loss 0.37596759, accuracy 0.75439990, elapsed 2.28     \n",
      "Epoch 79, CIFAR-10 Batch 4:  stats: loss 0.39463815, accuracy 0.74879980, elapsed 2.26     \n",
      "Epoch 79, CIFAR-10 Batch 5:  stats: loss 0.42594597, accuracy 0.74879980, elapsed 2.26     \n",
      "Epoch 80, CIFAR-10 Batch 1:  stats: loss 0.44293600, accuracy 0.76479977, elapsed 2.25     \n",
      "Epoch 80, CIFAR-10 Batch 2:  stats: loss 0.41210842, accuracy 0.75219989, elapsed 2.28     \n",
      "Epoch 80, CIFAR-10 Batch 3:  stats: loss 0.36841518, accuracy 0.75579989, elapsed 2.27     \n",
      "Epoch 80, CIFAR-10 Batch 4:  stats: loss 0.36023289, accuracy 0.75359982, elapsed 2.25     \n",
      "Epoch 80, CIFAR-10 Batch 5:  stats: loss 0.42351380, accuracy 0.75359982, elapsed 2.27     \n",
      "Epoch 81, CIFAR-10 Batch 1:  stats: loss 0.42089993, accuracy 0.76339984, elapsed 2.28     \n",
      "Epoch 81, CIFAR-10 Batch 2:  stats: loss 0.41384152, accuracy 0.75359982, elapsed 2.27     \n",
      "Epoch 81, CIFAR-10 Batch 3:  stats: loss 0.34683710, accuracy 0.75279987, elapsed 2.23     \n",
      "Epoch 81, CIFAR-10 Batch 4:  stats: loss 0.35313427, accuracy 0.75079978, elapsed 2.29     \n",
      "Epoch 81, CIFAR-10 Batch 5:  stats: loss 0.44134921, accuracy 0.74619985, elapsed 2.26     \n",
      "Epoch 82, CIFAR-10 Batch 1:  stats: loss 0.45788080, accuracy 0.75579983, elapsed 2.25     \n",
      "Epoch 82, CIFAR-10 Batch 2:  stats: loss 0.41390252, accuracy 0.76579988, elapsed 2.28     \n",
      "Epoch 82, CIFAR-10 Batch 3:  stats: loss 0.35912162, accuracy 0.76039988, elapsed 2.29     \n",
      "Epoch 82, CIFAR-10 Batch 4:  stats: loss 0.35936415, accuracy 0.75659978, elapsed 2.27     \n",
      "Epoch 82, CIFAR-10 Batch 5:  stats: loss 0.40458041, accuracy 0.74979985, elapsed 2.27     \n",
      "Epoch 83, CIFAR-10 Batch 1:  stats: loss 0.42865363, accuracy 0.76139987, elapsed 2.25     \n",
      "Epoch 83, CIFAR-10 Batch 2:  stats: loss 0.42844170, accuracy 0.75159979, elapsed 2.26     \n",
      "Epoch 83, CIFAR-10 Batch 3:  stats: loss 0.35690355, accuracy 0.74839985, elapsed 2.25     \n",
      "Epoch 83, CIFAR-10 Batch 4:  stats: loss 0.37080678, accuracy 0.74939984, elapsed 2.25     \n",
      "Epoch 83, CIFAR-10 Batch 5:  stats: loss 0.38590950, accuracy 0.76179987, elapsed 2.25     \n",
      "Epoch 84, CIFAR-10 Batch 1:  stats: loss 0.42610651, accuracy 0.76379985, elapsed 2.27     \n",
      "Epoch 84, CIFAR-10 Batch 2:  stats: loss 0.40495133, accuracy 0.75399983, elapsed 2.27     \n",
      "Epoch 84, CIFAR-10 Batch 3:  stats: loss 0.35544863, accuracy 0.76019979, elapsed 2.25     \n",
      "Epoch 84, CIFAR-10 Batch 4:  stats: loss 0.38883069, accuracy 0.73999983, elapsed 2.26     \n",
      "Epoch 84, CIFAR-10 Batch 5:  stats: loss 0.39613834, accuracy 0.75619984, elapsed 2.27     \n",
      "Epoch 85, CIFAR-10 Batch 1:  stats: loss 0.42339641, accuracy 0.75999975, elapsed 2.27     \n",
      "Epoch 85, CIFAR-10 Batch 2:  stats: loss 0.39308605, accuracy 0.75439990, elapsed 2.26     \n",
      "Epoch 85, CIFAR-10 Batch 3:  stats: loss 0.36503524, accuracy 0.75499976, elapsed 2.25     \n",
      "Epoch 85, CIFAR-10 Batch 4:  stats: loss 0.34270474, accuracy 0.75139987, elapsed 2.28     \n",
      "Epoch 85, CIFAR-10 Batch 5:  stats: loss 0.40573120, accuracy 0.75219977, elapsed 2.26     \n",
      "Epoch 86, CIFAR-10 Batch 1:  stats: loss 0.43444055, accuracy 0.75919974, elapsed 2.25     \n",
      "Epoch 86, CIFAR-10 Batch 2:  stats: loss 0.38338178, accuracy 0.76739985, elapsed 2.28     \n",
      "Epoch 86, CIFAR-10 Batch 3:  stats: loss 0.34322077, accuracy 0.75939983, elapsed 2.26     \n",
      "Epoch 86, CIFAR-10 Batch 4:  stats: loss 0.36134928, accuracy 0.74659991, elapsed 2.26     \n",
      "Epoch 86, CIFAR-10 Batch 5:  stats: loss 0.41065753, accuracy 0.74999982, elapsed 2.26     \n",
      "Epoch 87, CIFAR-10 Batch 1:  stats: loss 0.41865766, accuracy 0.75439984, elapsed 2.27     \n",
      "Epoch 87, CIFAR-10 Batch 2:  stats: loss 0.37856209, accuracy 0.75219977, elapsed 2.23     \n",
      "Epoch 87, CIFAR-10 Batch 3:  stats: loss 0.35722631, accuracy 0.74899983, elapsed 2.28     \n",
      "Epoch 87, CIFAR-10 Batch 4:  stats: loss 0.34202746, accuracy 0.75019991, elapsed 2.27     \n",
      "Epoch 87, CIFAR-10 Batch 5:  stats: loss 0.38254088, accuracy 0.75619978, elapsed 2.26     \n",
      "Epoch 88, CIFAR-10 Batch 1:  stats: loss 0.42645681, accuracy 0.75959986, elapsed 2.26     \n",
      "Epoch 88, CIFAR-10 Batch 2:  stats: loss 0.36914566, accuracy 0.75979984, elapsed 2.26     \n",
      "Epoch 88, CIFAR-10 Batch 3:  stats: loss 0.36726120, accuracy 0.74859983, elapsed 2.27     \n",
      "Epoch 88, CIFAR-10 Batch 4:  stats: loss 0.35774583, accuracy 0.75739986, elapsed 2.27     \n",
      "Epoch 88, CIFAR-10 Batch 5:  stats: loss 0.40560982, accuracy 0.75439990, elapsed 2.27     \n",
      "Epoch 89, CIFAR-10 Batch 1:  stats: loss 0.41323602, accuracy 0.76879984, elapsed 2.27     \n",
      "Epoch 89, CIFAR-10 Batch 2:  stats: loss 0.37790811, accuracy 0.75659978, elapsed 2.27     \n",
      "Epoch 89, CIFAR-10 Batch 3:  stats: loss 0.32770413, accuracy 0.75539982, elapsed 2.25     \n",
      "Epoch 89, CIFAR-10 Batch 4:  stats: loss 0.33452645, accuracy 0.75079983, elapsed 2.26     \n",
      "Epoch 89, CIFAR-10 Batch 5:  stats: loss 0.39521101, accuracy 0.75679982, elapsed 2.25     \n",
      "Epoch 90, CIFAR-10 Batch 1:  stats: loss 0.40213937, accuracy 0.76879978, elapsed 2.28     \n",
      "Epoch 90, CIFAR-10 Batch 2:  stats: loss 0.35830122, accuracy 0.75799984, elapsed 2.28     \n",
      "Epoch 90, CIFAR-10 Batch 3:  stats: loss 0.33239153, accuracy 0.76119983, elapsed 2.27     \n",
      "Epoch 90, CIFAR-10 Batch 4:  stats: loss 0.32119942, accuracy 0.76919979, elapsed 2.28     \n",
      "Epoch 90, CIFAR-10 Batch 5:  stats: loss 0.38883576, accuracy 0.75919986, elapsed 2.26     \n",
      "Epoch 91, CIFAR-10 Batch 1:  stats: loss 0.39938870, accuracy 0.76899987, elapsed 2.26     \n",
      "Epoch 91, CIFAR-10 Batch 2:  stats: loss 0.36198151, accuracy 0.75959980, elapsed 2.28     \n",
      "Epoch 91, CIFAR-10 Batch 3:  stats: loss 0.36738488, accuracy 0.74299985, elapsed 2.26     \n",
      "Epoch 91, CIFAR-10 Batch 4:  stats: loss 0.32605913, accuracy 0.76139981, elapsed 2.25     \n",
      "Epoch 91, CIFAR-10 Batch 5:  stats: loss 0.38819891, accuracy 0.75279987, elapsed 2.26     \n",
      "Epoch 92, CIFAR-10 Batch 1:  stats: loss 0.41379905, accuracy 0.77379984, elapsed 2.26     \n",
      "Epoch 92, CIFAR-10 Batch 2:  stats: loss 0.34096575, accuracy 0.75999981, elapsed 2.25     \n",
      "Epoch 92, CIFAR-10 Batch 3:  stats: loss 0.35365891, accuracy 0.75139987, elapsed 2.27     \n",
      "Epoch 92, CIFAR-10 Batch 4:  stats: loss 0.34946740, accuracy 0.75999987, elapsed 2.27     \n",
      "Epoch 92, CIFAR-10 Batch 5:  stats: loss 0.41787013, accuracy 0.74919987, elapsed 2.29     \n",
      "Epoch 93, CIFAR-10 Batch 1:  stats: loss 0.43402192, accuracy 0.76139981, elapsed 2.26     \n",
      "Epoch 93, CIFAR-10 Batch 2:  stats: loss 0.35737932, accuracy 0.76399988, elapsed 2.26     \n",
      "Epoch 93, CIFAR-10 Batch 3:  stats: loss 0.34820604, accuracy 0.75119984, elapsed 2.26     \n",
      "Epoch 93, CIFAR-10 Batch 4:  stats: loss 0.36814797, accuracy 0.75579989, elapsed 2.28     \n",
      "Epoch 93, CIFAR-10 Batch 5:  stats: loss 0.40906322, accuracy 0.75279981, elapsed 2.26     \n",
      "Epoch 94, CIFAR-10 Batch 1:  stats: loss 0.43577492, accuracy 0.75979984, elapsed 2.27     \n",
      "Epoch 94, CIFAR-10 Batch 2:  stats: loss 0.35957909, accuracy 0.75599980, elapsed 2.28     \n",
      "Epoch 94, CIFAR-10 Batch 3:  stats: loss 0.33856264, accuracy 0.75579983, elapsed 2.27     \n",
      "Epoch 94, CIFAR-10 Batch 4:  stats: loss 0.35620344, accuracy 0.74939990, elapsed 2.26     \n",
      "Epoch 94, CIFAR-10 Batch 5:  stats: loss 0.35413170, accuracy 0.76279986, elapsed 2.27     \n",
      "Epoch 95, CIFAR-10 Batch 1:  stats: loss 0.41302085, accuracy 0.76459986, elapsed 2.27     \n",
      "Epoch 95, CIFAR-10 Batch 2:  stats: loss 0.34738928, accuracy 0.76559985, elapsed 2.26     \n",
      "Epoch 95, CIFAR-10 Batch 3:  stats: loss 0.33555248, accuracy 0.75799984, elapsed 2.29     \n",
      "Epoch 95, CIFAR-10 Batch 4:  stats: loss 0.34764460, accuracy 0.76459992, elapsed 2.26     \n",
      "Epoch 95, CIFAR-10 Batch 5:  stats: loss 0.36249542, accuracy 0.76039988, elapsed 2.25     \n",
      "Epoch 96, CIFAR-10 Batch 1:  stats: loss 0.39324230, accuracy 0.76879984, elapsed 2.25     \n",
      "Epoch 96, CIFAR-10 Batch 2:  stats: loss 0.35410655, accuracy 0.75279981, elapsed 2.27     \n",
      "Epoch 96, CIFAR-10 Batch 3:  stats: loss 0.34659854, accuracy 0.75299984, elapsed 2.27     \n",
      "Epoch 96, CIFAR-10 Batch 4:  stats: loss 0.37081158, accuracy 0.74959987, elapsed 2.26     \n",
      "Epoch 96, CIFAR-10 Batch 5:  stats: loss 0.39906162, accuracy 0.75819981, elapsed 2.28     \n",
      "Epoch 97, CIFAR-10 Batch 1:  stats: loss 0.38310599, accuracy 0.76379985, elapsed 2.29     \n",
      "Epoch 97, CIFAR-10 Batch 2:  stats: loss 0.36853147, accuracy 0.76079983, elapsed 2.26     \n",
      "Epoch 97, CIFAR-10 Batch 3:  stats: loss 0.33569828, accuracy 0.76299983, elapsed 2.26     \n",
      "Epoch 97, CIFAR-10 Batch 4:  stats: loss 0.34774557, accuracy 0.75659978, elapsed 2.24     \n",
      "Epoch 97, CIFAR-10 Batch 5:  stats: loss 0.38441432, accuracy 0.75719976, elapsed 2.29     \n",
      "Epoch 98, CIFAR-10 Batch 1:  stats: loss 0.38501316, accuracy 0.77019978, elapsed 2.27     \n",
      "Epoch 98, CIFAR-10 Batch 2:  stats: loss 0.35887587, accuracy 0.75819981, elapsed 2.26     \n",
      "Epoch 98, CIFAR-10 Batch 3:  stats: loss 0.36667991, accuracy 0.73739982, elapsed 2.26     \n",
      "Epoch 98, CIFAR-10 Batch 4:  stats: loss 0.33223316, accuracy 0.75559980, elapsed 2.26     \n",
      "Epoch 98, CIFAR-10 Batch 5:  stats: loss 0.39695013, accuracy 0.74279988, elapsed 2.23     \n",
      "Epoch 99, CIFAR-10 Batch 1:  stats: loss 0.39027137, accuracy 0.76379985, elapsed 2.30     \n",
      "Epoch 99, CIFAR-10 Batch 2:  stats: loss 0.34484839, accuracy 0.75239986, elapsed 2.24     \n",
      "Epoch 99, CIFAR-10 Batch 3:  stats: loss 0.31847423, accuracy 0.75339985, elapsed 2.23     \n",
      "Epoch 99, CIFAR-10 Batch 4:  stats: loss 0.31609365, accuracy 0.76279980, elapsed 2.26     \n",
      "Epoch 99, CIFAR-10 Batch 5:  stats: loss 0.38176286, accuracy 0.75899982, elapsed 2.26     \n",
      "Epoch 100, CIFAR-10 Batch 1:  stats: loss 0.39386368, accuracy 0.75679982, elapsed 2.27     \n",
      "Epoch 100, CIFAR-10 Batch 2:  stats: loss 0.34617832, accuracy 0.75359982, elapsed 2.26     \n",
      "Epoch 100, CIFAR-10 Batch 3:  stats: loss 0.31275803, accuracy 0.76659983, elapsed 2.26     \n",
      "Epoch 100, CIFAR-10 Batch 4:  stats: loss 0.33142337, accuracy 0.75399989, elapsed 2.26     \n",
      "Epoch 100, CIFAR-10 Batch 5:  stats: loss 0.36531320, accuracy 0.75579983, elapsed 2.28     \n",
      "Epoch 101, CIFAR-10 Batch 1:  stats: loss 0.38735026, accuracy 0.76459986, elapsed 2.28     \n",
      "Epoch 101, CIFAR-10 Batch 2:  stats: loss 0.31874210, accuracy 0.76659983, elapsed 2.25     \n",
      "Epoch 101, CIFAR-10 Batch 3:  stats: loss 0.31343850, accuracy 0.76039988, elapsed 2.26     \n",
      "Epoch 101, CIFAR-10 Batch 4:  stats: loss 0.29528081, accuracy 0.75759983, elapsed 2.27     \n",
      "Epoch 101, CIFAR-10 Batch 5:  stats: loss 0.36286914, accuracy 0.75919974, elapsed 2.26     \n",
      "Epoch 102, CIFAR-10 Batch 1:  stats: loss 0.37976906, accuracy 0.76199991, elapsed 2.26     \n",
      "Epoch 102, CIFAR-10 Batch 2:  stats: loss 0.35167724, accuracy 0.75139976, elapsed 2.28     \n",
      "Epoch 102, CIFAR-10 Batch 3:  stats: loss 0.39314312, accuracy 0.72959983, elapsed 2.27     \n",
      "Epoch 102, CIFAR-10 Batch 4:  stats: loss 0.34193978, accuracy 0.75459981, elapsed 2.26     \n",
      "Epoch 102, CIFAR-10 Batch 5:  stats: loss 0.37505740, accuracy 0.75639975, elapsed 2.27     \n",
      "Epoch 103, CIFAR-10 Batch 1:  stats: loss 0.38555467, accuracy 0.76279986, elapsed 2.28     \n",
      "Epoch 103, CIFAR-10 Batch 2:  stats: loss 0.35198402, accuracy 0.75459975, elapsed 2.27     \n",
      "Epoch 103, CIFAR-10 Batch 3:  stats: loss 0.32117707, accuracy 0.75879979, elapsed 2.27     \n",
      "Epoch 103, CIFAR-10 Batch 4:  stats: loss 0.28841427, accuracy 0.76359981, elapsed 2.24     \n",
      "Epoch 103, CIFAR-10 Batch 5:  stats: loss 0.37232205, accuracy 0.75499988, elapsed 2.28     \n",
      "Epoch 104, CIFAR-10 Batch 1:  stats: loss 0.37566954, accuracy 0.76259977, elapsed 2.26     \n",
      "Epoch 104, CIFAR-10 Batch 2:  stats: loss 0.33945897, accuracy 0.76239985, elapsed 2.24     \n",
      "Epoch 104, CIFAR-10 Batch 3:  stats: loss 0.31113306, accuracy 0.76079988, elapsed 2.26     \n",
      "Epoch 104, CIFAR-10 Batch 4:  stats: loss 0.32857934, accuracy 0.75399983, elapsed 2.26     \n",
      "Epoch 104, CIFAR-10 Batch 5:  stats: loss 0.35899198, accuracy 0.75919986, elapsed 2.28     \n",
      "Epoch 105, CIFAR-10 Batch 1:  stats: loss 0.37210312, accuracy 0.76739985, elapsed 2.27     \n",
      "Epoch 105, CIFAR-10 Batch 2:  stats: loss 0.34476104, accuracy 0.76579988, elapsed 2.26     \n",
      "Epoch 105, CIFAR-10 Batch 3:  stats: loss 0.30733091, accuracy 0.75919980, elapsed 2.27     \n",
      "Epoch 105, CIFAR-10 Batch 4:  stats: loss 0.29591662, accuracy 0.76439977, elapsed 2.27     \n",
      "Epoch 105, CIFAR-10 Batch 5:  stats: loss 0.38656580, accuracy 0.75699979, elapsed 2.27     \n",
      "Epoch 106, CIFAR-10 Batch 1:  stats: loss 0.36257488, accuracy 0.77099985, elapsed 2.26     \n",
      "Epoch 106, CIFAR-10 Batch 2:  stats: loss 0.36559367, accuracy 0.75799984, elapsed 2.27     \n",
      "Epoch 106, CIFAR-10 Batch 3:  stats: loss 0.31733361, accuracy 0.75599986, elapsed 2.25     \n",
      "Epoch 106, CIFAR-10 Batch 4:  stats: loss 0.33393317, accuracy 0.74939984, elapsed 2.28     \n",
      "Epoch 106, CIFAR-10 Batch 5:  stats: loss 0.33719754, accuracy 0.75039983, elapsed 2.27     \n",
      "Epoch 107, CIFAR-10 Batch 1:  stats: loss 0.36846817, accuracy 0.76399976, elapsed 2.27     \n",
      "Epoch 107, CIFAR-10 Batch 2:  stats: loss 0.33558384, accuracy 0.76219982, elapsed 2.25     \n",
      "Epoch 107, CIFAR-10 Batch 3:  stats: loss 0.30709985, accuracy 0.75739986, elapsed 2.25     \n",
      "Epoch 107, CIFAR-10 Batch 4:  stats: loss 0.30798197, accuracy 0.75739986, elapsed 2.25     \n",
      "Epoch 107, CIFAR-10 Batch 5:  stats: loss 0.34816432, accuracy 0.75259984, elapsed 2.26     \n",
      "Epoch 108, CIFAR-10 Batch 1:  stats: loss 0.38191345, accuracy 0.76799977, elapsed 2.27     \n",
      "Epoch 108, CIFAR-10 Batch 2:  stats: loss 0.32561472, accuracy 0.76359981, elapsed 2.26     \n",
      "Epoch 108, CIFAR-10 Batch 3:  stats: loss 0.33984214, accuracy 0.74539983, elapsed 2.26     \n",
      "Epoch 108, CIFAR-10 Batch 4:  stats: loss 0.31526944, accuracy 0.76639980, elapsed 2.26     \n",
      "Epoch 108, CIFAR-10 Batch 5:  stats: loss 0.35151851, accuracy 0.75459981, elapsed 2.28     \n",
      "Epoch 109, CIFAR-10 Batch 1:  stats: loss 0.38312507, accuracy 0.76719975, elapsed 2.30     \n",
      "Epoch 109, CIFAR-10 Batch 2:  stats: loss 0.35028273, accuracy 0.76359981, elapsed 2.27     \n",
      "Epoch 109, CIFAR-10 Batch 3:  stats: loss 0.32048020, accuracy 0.75599986, elapsed 2.27     \n",
      "Epoch 109, CIFAR-10 Batch 4:  stats: loss 0.30308768, accuracy 0.75639981, elapsed 2.27     \n",
      "Epoch 109, CIFAR-10 Batch 5:  stats: loss 0.33029008, accuracy 0.76279974, elapsed 2.26     \n",
      "Epoch 110, CIFAR-10 Batch 1:  stats: loss 0.39153230, accuracy 0.77219987, elapsed 2.28     \n",
      "Epoch 110, CIFAR-10 Batch 2:  stats: loss 0.32888752, accuracy 0.76759982, elapsed 2.27     \n",
      "Epoch 110, CIFAR-10 Batch 3:  stats: loss 0.30277720, accuracy 0.75899988, elapsed 2.25     \n",
      "Epoch 110, CIFAR-10 Batch 4:  stats: loss 0.30356786, accuracy 0.75619984, elapsed 2.26     \n",
      "Epoch 110, CIFAR-10 Batch 5:  stats: loss 0.33348203, accuracy 0.74859977, elapsed 2.28     \n",
      "Epoch 111, CIFAR-10 Batch 1:  stats: loss 0.35595900, accuracy 0.77179986, elapsed 2.26     \n",
      "Epoch 111, CIFAR-10 Batch 2:  stats: loss 0.31439224, accuracy 0.75859976, elapsed 2.26     \n",
      "Epoch 111, CIFAR-10 Batch 3:  stats: loss 0.34074089, accuracy 0.74059987, elapsed 2.26     \n",
      "Epoch 111, CIFAR-10 Batch 4:  stats: loss 0.29353750, accuracy 0.76199985, elapsed 2.27     \n",
      "Epoch 111, CIFAR-10 Batch 5:  stats: loss 0.32904145, accuracy 0.76299977, elapsed 2.27     \n",
      "Epoch 112, CIFAR-10 Batch 1:  stats: loss 0.37310359, accuracy 0.77079976, elapsed 2.27     \n",
      "Epoch 112, CIFAR-10 Batch 2:  stats: loss 0.32399043, accuracy 0.76179981, elapsed 2.24     \n",
      "Epoch 112, CIFAR-10 Batch 3:  stats: loss 0.31917289, accuracy 0.74959981, elapsed 2.27     \n",
      "Epoch 112, CIFAR-10 Batch 4:  stats: loss 0.29995173, accuracy 0.75639987, elapsed 2.26     \n",
      "Epoch 112, CIFAR-10 Batch 5:  stats: loss 0.35158157, accuracy 0.75359982, elapsed 2.27     \n",
      "Epoch 113, CIFAR-10 Batch 1:  stats: loss 0.38537058, accuracy 0.76459980, elapsed 2.29     \n",
      "Epoch 113, CIFAR-10 Batch 2:  stats: loss 0.34048596, accuracy 0.74919987, elapsed 2.27     \n",
      "Epoch 113, CIFAR-10 Batch 3:  stats: loss 0.30595392, accuracy 0.74919981, elapsed 2.26     \n",
      "Epoch 113, CIFAR-10 Batch 4:  stats: loss 0.28963208, accuracy 0.75779986, elapsed 2.25     \n",
      "Epoch 113, CIFAR-10 Batch 5:  stats: loss 0.35373449, accuracy 0.76339984, elapsed 2.26     \n",
      "Epoch 114, CIFAR-10 Batch 1:  stats: loss 0.36849925, accuracy 0.76799989, elapsed 2.26     \n",
      "Epoch 114, CIFAR-10 Batch 2:  stats: loss 0.31207660, accuracy 0.76399976, elapsed 2.28     \n",
      "Epoch 114, CIFAR-10 Batch 3:  stats: loss 0.35782117, accuracy 0.73999983, elapsed 2.28     \n",
      "Epoch 114, CIFAR-10 Batch 4:  stats: loss 0.29698512, accuracy 0.76339984, elapsed 2.26     \n",
      "Epoch 114, CIFAR-10 Batch 5:  stats: loss 0.34284341, accuracy 0.75119984, elapsed 2.26     \n",
      "Epoch 115, CIFAR-10 Batch 1:  stats: loss 0.36870363, accuracy 0.77119982, elapsed 2.27     \n",
      "Epoch 115, CIFAR-10 Batch 2:  stats: loss 0.30103701, accuracy 0.76419985, elapsed 2.27     \n",
      "Epoch 115, CIFAR-10 Batch 3:  stats: loss 0.30035555, accuracy 0.75759989, elapsed 2.27     \n",
      "Epoch 115, CIFAR-10 Batch 4:  stats: loss 0.27954605, accuracy 0.76079983, elapsed 2.25     \n",
      "Epoch 115, CIFAR-10 Batch 5:  stats: loss 0.32616508, accuracy 0.75699985, elapsed 2.27     \n",
      "Epoch 116, CIFAR-10 Batch 1:  stats: loss 0.36146975, accuracy 0.77199990, elapsed 2.26     \n",
      "Epoch 116, CIFAR-10 Batch 2:  stats: loss 0.29895887, accuracy 0.76359981, elapsed 2.25     \n",
      "Epoch 116, CIFAR-10 Batch 3:  stats: loss 0.30699229, accuracy 0.76039982, elapsed 2.27     \n",
      "Epoch 116, CIFAR-10 Batch 4:  stats: loss 0.31927276, accuracy 0.75099981, elapsed 2.27     \n",
      "Epoch 116, CIFAR-10 Batch 5:  stats: loss 0.31850380, accuracy 0.75219977, elapsed 2.25     \n",
      "Epoch 117, CIFAR-10 Batch 1:  stats: loss 0.39044920, accuracy 0.76759982, elapsed 2.28     \n",
      "Epoch 117, CIFAR-10 Batch 2:  stats: loss 0.34224856, accuracy 0.75299978, elapsed 2.28     \n",
      "Epoch 117, CIFAR-10 Batch 3:  stats: loss 0.29971975, accuracy 0.76659983, elapsed 2.26     \n",
      "Epoch 117, CIFAR-10 Batch 4:  stats: loss 0.27760458, accuracy 0.75739986, elapsed 2.26     \n",
      "Epoch 117, CIFAR-10 Batch 5:  stats: loss 0.34033516, accuracy 0.75459981, elapsed 2.29     \n",
      "Epoch 118, CIFAR-10 Batch 1:  stats: loss 0.36601561, accuracy 0.76619977, elapsed 2.28     \n",
      "Epoch 118, CIFAR-10 Batch 2:  stats: loss 0.29226217, accuracy 0.76519984, elapsed 2.27     \n",
      "Epoch 118, CIFAR-10 Batch 3:  stats: loss 0.33223909, accuracy 0.74099988, elapsed 2.26     \n",
      "Epoch 118, CIFAR-10 Batch 4:  stats: loss 0.31543016, accuracy 0.75679982, elapsed 2.26     \n",
      "Epoch 118, CIFAR-10 Batch 5:  stats: loss 0.31117764, accuracy 0.75599980, elapsed 2.27     \n",
      "Epoch 119, CIFAR-10 Batch 1:  stats: loss 0.36683473, accuracy 0.77279985, elapsed 2.27     \n",
      "Epoch 119, CIFAR-10 Batch 2:  stats: loss 0.29200166, accuracy 0.76159984, elapsed 2.25     \n",
      "Epoch 119, CIFAR-10 Batch 3:  stats: loss 0.28060591, accuracy 0.75779986, elapsed 2.28     \n",
      "Epoch 119, CIFAR-10 Batch 4:  stats: loss 0.27833399, accuracy 0.77079982, elapsed 2.27     \n",
      "Epoch 119, CIFAR-10 Batch 5:  stats: loss 0.30924371, accuracy 0.76819980, elapsed 2.27     \n",
      "Epoch 120, CIFAR-10 Batch 1:  stats: loss 0.36056769, accuracy 0.76659983, elapsed 2.28     \n",
      "Epoch 120, CIFAR-10 Batch 2:  stats: loss 0.28970253, accuracy 0.76919985, elapsed 2.26     \n",
      "Epoch 120, CIFAR-10 Batch 3:  stats: loss 0.29764426, accuracy 0.76159984, elapsed 2.27     \n",
      "Epoch 120, CIFAR-10 Batch 4:  stats: loss 0.28995353, accuracy 0.75659984, elapsed 2.27     \n",
      "Epoch 120, CIFAR-10 Batch 5:  stats: loss 0.32853258, accuracy 0.76539981, elapsed 2.25     \n",
      "Epoch 121, CIFAR-10 Batch 1:  stats: loss 0.37578771, accuracy 0.76559979, elapsed 2.27     \n",
      "Epoch 121, CIFAR-10 Batch 2:  stats: loss 0.32092333, accuracy 0.76819980, elapsed 2.25     \n",
      "Epoch 121, CIFAR-10 Batch 3:  stats: loss 0.30439645, accuracy 0.76299977, elapsed 2.27     \n",
      "Epoch 121, CIFAR-10 Batch 4:  stats: loss 0.26440963, accuracy 0.76439983, elapsed 2.26     \n",
      "Epoch 121, CIFAR-10 Batch 5:  stats: loss 0.33151597, accuracy 0.74399984, elapsed 2.28     \n",
      "Epoch 122, CIFAR-10 Batch 1:  stats: loss 0.35220578, accuracy 0.76539981, elapsed 2.26     \n",
      "Epoch 122, CIFAR-10 Batch 2:  stats: loss 0.33997020, accuracy 0.75899982, elapsed 2.25     \n",
      "Epoch 122, CIFAR-10 Batch 3:  stats: loss 0.30325580, accuracy 0.75279987, elapsed 2.27     \n",
      "Epoch 122, CIFAR-10 Batch 4:  stats: loss 0.28094822, accuracy 0.75959980, elapsed 2.28     \n",
      "Epoch 122, CIFAR-10 Batch 5:  stats: loss 0.37136811, accuracy 0.73079985, elapsed 2.27     \n",
      "Epoch 123, CIFAR-10 Batch 1:  stats: loss 0.35242030, accuracy 0.77339983, elapsed 2.27     \n",
      "Epoch 123, CIFAR-10 Batch 2:  stats: loss 0.30018076, accuracy 0.75979984, elapsed 2.26     \n",
      "Epoch 123, CIFAR-10 Batch 3:  stats: loss 0.29525453, accuracy 0.76059985, elapsed 2.27     \n",
      "Epoch 123, CIFAR-10 Batch 4:  stats: loss 0.25686777, accuracy 0.76799983, elapsed 2.24     \n",
      "Epoch 123, CIFAR-10 Batch 5:  stats: loss 0.30749288, accuracy 0.75479984, elapsed 2.26     \n",
      "Epoch 124, CIFAR-10 Batch 1:  stats: loss 0.33022931, accuracy 0.77419972, elapsed 2.26     \n",
      "Epoch 124, CIFAR-10 Batch 2:  stats: loss 0.30023286, accuracy 0.75539982, elapsed 2.25     \n",
      "Epoch 124, CIFAR-10 Batch 3:  stats: loss 0.27284956, accuracy 0.76759982, elapsed 2.27     \n",
      "Epoch 124, CIFAR-10 Batch 4:  stats: loss 0.26809207, accuracy 0.76159990, elapsed 2.24     \n",
      "Epoch 124, CIFAR-10 Batch 5:  stats: loss 0.31206968, accuracy 0.75739986, elapsed 2.27     \n",
      "Epoch 125, CIFAR-10 Batch 1:  stats: loss 0.32942784, accuracy 0.77099991, elapsed 2.28     \n",
      "Epoch 125, CIFAR-10 Batch 2:  stats: loss 0.29310101, accuracy 0.76459986, elapsed 2.27     \n",
      "Epoch 125, CIFAR-10 Batch 3:  stats: loss 0.28569826, accuracy 0.76159978, elapsed 2.26     \n",
      "Epoch 125, CIFAR-10 Batch 4:  stats: loss 0.24798255, accuracy 0.76659983, elapsed 2.28     \n",
      "Epoch 125, CIFAR-10 Batch 5:  stats: loss 0.32261008, accuracy 0.75839984, elapsed 2.27     \n",
      "Epoch 126, CIFAR-10 Batch 1:  stats: loss 0.35892385, accuracy 0.77199984, elapsed 2.26     \n",
      "Epoch 126, CIFAR-10 Batch 2:  stats: loss 0.27853724, accuracy 0.76019984, elapsed 2.25     \n",
      "Epoch 126, CIFAR-10 Batch 3:  stats: loss 0.29087678, accuracy 0.75179982, elapsed 2.26     \n",
      "Epoch 126, CIFAR-10 Batch 4:  stats: loss 0.28087008, accuracy 0.76459980, elapsed 2.26     \n",
      "Epoch 126, CIFAR-10 Batch 5:  stats: loss 0.29941779, accuracy 0.76299983, elapsed 2.28     \n",
      "Epoch 127, CIFAR-10 Batch 1:  stats: loss 0.34866998, accuracy 0.77419978, elapsed 2.27     \n",
      "Epoch 127, CIFAR-10 Batch 2:  stats: loss 0.28598246, accuracy 0.77259982, elapsed 2.27     \n",
      "Epoch 127, CIFAR-10 Batch 3:  stats: loss 0.27186745, accuracy 0.75659978, elapsed 2.27     \n",
      "Epoch 127, CIFAR-10 Batch 4:  stats: loss 0.28271884, accuracy 0.76019979, elapsed 2.25     \n",
      "Epoch 127, CIFAR-10 Batch 5:  stats: loss 0.31080499, accuracy 0.75059986, elapsed 2.30     \n",
      "Epoch 128, CIFAR-10 Batch 1:  stats: loss 0.32577130, accuracy 0.77859986, elapsed 2.27     \n",
      "Epoch 128, CIFAR-10 Batch 2:  stats: loss 0.28479820, accuracy 0.75819981, elapsed 2.27     \n",
      "Epoch 128, CIFAR-10 Batch 3:  stats: loss 0.25842801, accuracy 0.76879984, elapsed 2.26     \n",
      "Epoch 128, CIFAR-10 Batch 4:  stats: loss 0.24241416, accuracy 0.76959980, elapsed 2.27     \n",
      "Epoch 128, CIFAR-10 Batch 5:  stats: loss 0.27296111, accuracy 0.76379979, elapsed 2.27     \n",
      "Epoch 129, CIFAR-10 Batch 1:  stats: loss 0.33304927, accuracy 0.76919985, elapsed 2.27     \n",
      "Epoch 129, CIFAR-10 Batch 2:  stats: loss 0.29168785, accuracy 0.76679981, elapsed 2.24     \n",
      "Epoch 129, CIFAR-10 Batch 3:  stats: loss 0.28945881, accuracy 0.75619984, elapsed 2.26     \n",
      "Epoch 129, CIFAR-10 Batch 4:  stats: loss 0.29630125, accuracy 0.76439983, elapsed 2.26     \n",
      "Epoch 129, CIFAR-10 Batch 5:  Epoch 131, CIFAR-10 Batch 5:  stats: loss 0.31037101, accuracy 0.76539987, elapsed 2.27     \n",
      "Epoch 132, CIFAR-10 Batch 1:  stats: loss 0.32985380, accuracy 0.77019978, elapsed 2.27     \n",
      "Epoch 132, CIFAR-10 Batch 2:  stats: loss 0.29515442, accuracy 0.76019984, elapsed 2.27     \n",
      "Epoch 132, CIFAR-10 Batch 3:  stats: loss 0.28218696, accuracy 0.74819982, elapsed 2.25     \n",
      "Epoch 132, CIFAR-10 Batch 4:  stats: loss 0.26953402, accuracy 0.76379985, elapsed 2.26     \n",
      "Epoch 132, CIFAR-10 Batch 5:  stats: loss 0.30123672, accuracy 0.76479983, elapsed 2.27     \n",
      "Epoch 133, CIFAR-10 Batch 1:  stats: loss 0.32358301, accuracy 0.77279985, elapsed 2.25     \n",
      "Epoch 133, CIFAR-10 Batch 2:  stats: loss 0.33194339, accuracy 0.75639981, elapsed 2.25     \n",
      "Epoch 133, CIFAR-10 Batch 3:  stats: loss 0.25969386, accuracy 0.76699984, elapsed 2.28     \n",
      "Epoch 133, CIFAR-10 Batch 4:  stats: loss 0.28358835, accuracy 0.76559985, elapsed 2.28     \n",
      "Epoch 133, CIFAR-10 Batch 5:  stats: loss 0.29106426, accuracy 0.76619983, elapsed 2.24     \n",
      "Epoch 134, CIFAR-10 Batch 1:  stats: loss 0.29715380, accuracy 0.77439982, elapsed 2.28     \n",
      "Epoch 134, CIFAR-10 Batch 2:  stats: loss 0.27250350, accuracy 0.76999992, elapsed 2.27     \n",
      "Epoch 134, CIFAR-10 Batch 3:  stats: loss 0.26260954, accuracy 0.76839983, elapsed 2.23     \n",
      "Epoch 134, CIFAR-10 Batch 4:  stats: loss 0.27216589, accuracy 0.76039982, elapsed 2.26     \n",
      "Epoch 134, CIFAR-10 Batch 5:  stats: loss 0.29127273, accuracy 0.76939988, elapsed 2.25     \n",
      "Epoch 135, CIFAR-10 Batch 1:  stats: loss 0.29427609, accuracy 0.77199978, elapsed 2.28     \n",
      "Epoch 135, CIFAR-10 Batch 2:  stats: loss 0.29678476, accuracy 0.76159990, elapsed 2.29     \n",
      "Epoch 135, CIFAR-10 Batch 3:  stats: loss 0.25971121, accuracy 0.76659983, elapsed 2.26     \n",
      "Epoch 135, CIFAR-10 Batch 4:  stats: loss 0.23827763, accuracy 0.77279985, elapsed 2.26     \n",
      "Epoch 135, CIFAR-10 Batch 5:  stats: loss 0.26990148, accuracy 0.76999986, elapsed 2.26     \n",
      "Epoch 136, CIFAR-10 Batch 1:  stats: loss 0.31012058, accuracy 0.77419978, elapsed 2.28     \n",
      "Epoch 136, CIFAR-10 Batch 2:  stats: loss 0.28528494, accuracy 0.75839978, elapsed 2.26     \n",
      "Epoch 136, CIFAR-10 Batch 3:  stats: loss 0.29603255, accuracy 0.75199980, elapsed 2.25     \n",
      "Epoch 136, CIFAR-10 Batch 4:  stats: loss 0.25656894, accuracy 0.77039987, elapsed 2.26     \n",
      "Epoch 136, CIFAR-10 Batch 5:  stats: loss 0.28394428, accuracy 0.77119982, elapsed 2.27     \n",
      "Epoch 137, CIFAR-10 Batch 1:  stats: loss 0.30565044, accuracy 0.76059985, elapsed 2.25     \n",
      "Epoch 137, CIFAR-10 Batch 2:  stats: loss 0.28743285, accuracy 0.76459980, elapsed 2.27     \n",
      "Epoch 137, CIFAR-10 Batch 3:  stats: loss 0.25807369, accuracy 0.76959980, elapsed 2.26     \n",
      "Epoch 137, CIFAR-10 Batch 4:  stats: loss 0.26946697, accuracy 0.75819981, elapsed 2.25     \n",
      "Epoch 137, CIFAR-10 Batch 5:  stats: loss 0.30681622, accuracy 0.75479984, elapsed 2.27     \n",
      "Epoch 138, CIFAR-10 Batch 1:  stats: loss 0.30926785, accuracy 0.77219987, elapsed 2.28     \n",
      "Epoch 138, CIFAR-10 Batch 2:  stats: loss 0.26359168, accuracy 0.77839988, elapsed 2.27     \n",
      "Epoch 138, CIFAR-10 Batch 3:  stats: loss 0.25463641, accuracy 0.76419979, elapsed 2.25     \n",
      "Epoch 138, CIFAR-10 Batch 4:  stats: loss 0.27985573, accuracy 0.75619978, elapsed 2.26     \n",
      "Epoch 138, CIFAR-10 Batch 5:  stats: loss 0.29552606, accuracy 0.76619983, elapsed 2.26     \n",
      "Epoch 139, CIFAR-10 Batch 1:  stats: loss 0.30898654, accuracy 0.76379985, elapsed 2.25     \n",
      "Epoch 139, CIFAR-10 Batch 2:  stats: loss 0.28366411, accuracy 0.76799983, elapsed 2.27     \n",
      "Epoch 139, CIFAR-10 Batch 3:  stats: loss 0.24412312, accuracy 0.77119982, elapsed 2.27     \n",
      "Epoch 139, CIFAR-10 Batch 4:  stats: loss 0.34516478, accuracy 0.74359983, elapsed 2.24     \n",
      "Epoch 139, CIFAR-10 Batch 5:  stats: loss 0.28045213, accuracy 0.77219987, elapsed 2.28     \n",
      "Epoch 140, CIFAR-10 Batch 1:  stats: loss 0.30114359, accuracy 0.77539986, elapsed 2.27     \n",
      "Epoch 140, CIFAR-10 Batch 2:  stats: loss 0.30176184, accuracy 0.76899981, elapsed 2.27     \n",
      "Epoch 140, CIFAR-10 Batch 3:  stats: loss 0.26223311, accuracy 0.77179980, elapsed 2.26     \n",
      "Epoch 140, CIFAR-10 Batch 4:  stats: loss 0.26047003, accuracy 0.76379985, elapsed 2.29     \n",
      "Epoch 140, CIFAR-10 Batch 5:  stats: loss 0.26702818, accuracy 0.77359980, elapsed 2.28     \n",
      "Epoch 141, CIFAR-10 Batch 1:  stats: loss 0.28974053, accuracy 0.77099985, elapsed 2.29     \n",
      "Epoch 141, CIFAR-10 Batch 2:  stats: loss 0.27004752, accuracy 0.77379984, elapsed 2.28     \n",
      "Epoch 141, CIFAR-10 Batch 3:  stats: loss 0.23297071, accuracy 0.77419984, elapsed 2.26     \n",
      "Epoch 141, CIFAR-10 Batch 4:  stats: loss 0.23980875, accuracy 0.77119982, elapsed 2.26     \n",
      "Epoch 141, CIFAR-10 Batch 5:  stats: loss 0.26034644, accuracy 0.77499980, elapsed 2.26     \n",
      "Epoch 142, CIFAR-10 Batch 1:  stats: loss 0.32234982, accuracy 0.76079983, elapsed 2.26     \n",
      "Epoch 142, CIFAR-10 Batch 2:  stats: loss 0.27238807, accuracy 0.77619976, elapsed 2.27     \n",
      "Epoch 142, CIFAR-10 Batch 3:  stats: loss 0.24999230, accuracy 0.77139986, elapsed 2.27     \n",
      "Epoch 142, CIFAR-10 Batch 4:  stats: loss 0.25500217, accuracy 0.76139987, elapsed 2.25     \n",
      "Epoch 142, CIFAR-10 Batch 5:  stats: loss 0.26302680, accuracy 0.77139980, elapsed 2.24     \n",
      "Epoch 143, CIFAR-10 Batch 1:  stats: loss 0.29368022, accuracy 0.77679980, elapsed 2.27     \n",
      "Epoch 143, CIFAR-10 Batch 2:  stats: loss 0.28118873, accuracy 0.77199984, elapsed 2.27     \n",
      "Epoch 143, CIFAR-10 Batch 3:  stats: loss 0.23663089, accuracy 0.77519989, elapsed 2.24     \n",
      "Epoch 143, CIFAR-10 Batch 4:  stats: loss 0.21700284, accuracy 0.76999986, elapsed 2.27     \n",
      "Epoch 143, CIFAR-10 Batch 5:  stats: loss 0.26949558, accuracy 0.76259983, elapsed 2.27     \n",
      "Epoch 144, CIFAR-10 Batch 1:  stats: loss 0.31464773, accuracy 0.77659976, elapsed 2.28     \n",
      "Epoch 144, CIFAR-10 Batch 2:  stats: loss 0.26378942, accuracy 0.76359987, elapsed 2.25     \n",
      "Epoch 144, CIFAR-10 Batch 3:  stats: loss 0.23487197, accuracy 0.77419978, elapsed 2.26     \n",
      "Epoch 144, CIFAR-10 Batch 4:  stats: loss 0.24221581, accuracy 0.76559985, elapsed 2.28     \n",
      "Epoch 144, CIFAR-10 Batch 5:  stats: loss 0.27676913, accuracy 0.76979989, elapsed 2.25     \n",
      "Epoch 145, CIFAR-10 Batch 1:  stats: loss 0.31506243, accuracy 0.77339989, elapsed 2.28     \n",
      "Epoch 145, CIFAR-10 Batch 2:  stats: loss 0.29300255, accuracy 0.76179981, elapsed 2.27     \n",
      "Epoch 145, CIFAR-10 Batch 3:  stats: loss 0.24262664, accuracy 0.76779985, elapsed 2.25     \n",
      "Epoch 145, CIFAR-10 Batch 4:  stats: loss 0.22574875, accuracy 0.77119982, elapsed 2.26     \n",
      "Epoch 145, CIFAR-10 Batch 5:  stats: loss 0.26860875, accuracy 0.77059978, elapsed 2.27     \n",
      "Epoch 146, CIFAR-10 Batch 1:  stats: loss 0.30071047, accuracy 0.77659982, elapsed 2.26     \n",
      "Epoch 146, CIFAR-10 Batch 2:  stats: loss 0.28001931, accuracy 0.76599991, elapsed 2.26     \n",
      "Epoch 146, CIFAR-10 Batch 3:  stats: loss 0.23956586, accuracy 0.76419985, elapsed 2.27     \n",
      "Epoch 146, CIFAR-10 Batch 4:  stats: loss 0.25434899, accuracy 0.76079983, elapsed 2.26     \n",
      "Epoch 146, CIFAR-10 Batch 5:  stats: loss 0.27781969, accuracy 0.76659983, elapsed 2.27     \n",
      "Epoch 147, CIFAR-10 Batch 1:  stats: loss 0.30351326, accuracy 0.76939988, elapsed 2.26     \n",
      "Epoch 147, CIFAR-10 Batch 2:  stats: loss 0.32447857, accuracy 0.75759977, elapsed 2.26     \n",
      "Epoch 147, CIFAR-10 Batch 3:  stats: loss 0.25270274, accuracy 0.77239984, elapsed 2.26     \n",
      "Epoch 147, CIFAR-10 Batch 4:  stats: loss 0.23477289, accuracy 0.77319986, elapsed 2.27     \n",
      "Epoch 147, CIFAR-10 Batch 5:  stats: loss 0.24967979, accuracy 0.76979989, elapsed 2.40     \n",
      "Epoch 148, CIFAR-10 Batch 1:  stats: loss 0.29111671, accuracy 0.77199984, elapsed 2.37     \n",
      "Epoch 148, CIFAR-10 Batch 2:  stats: loss 0.28770021, accuracy 0.76699984, elapsed 2.26     \n",
      "Epoch 148, CIFAR-10 Batch 3:  stats: loss 0.29325461, accuracy 0.75939983, elapsed 2.26     \n",
      "Epoch 148, CIFAR-10 Batch 4:  stats: loss 0.23181856, accuracy 0.77579981, elapsed 2.27     \n",
      "Epoch 148, CIFAR-10 Batch 5:  stats: loss 0.28589755, accuracy 0.75999981, elapsed 2.43     \n",
      "Epoch 149, CIFAR-10 Batch 1:  stats: loss 0.31977206, accuracy 0.76879984, elapsed 2.27     \n",
      "Epoch 149, CIFAR-10 Batch 2:  stats: loss 0.25326386, accuracy 0.76799983, elapsed 2.26     \n",
      "Epoch 149, CIFAR-10 Batch 3:  stats: loss 0.24033040, accuracy 0.76499981, elapsed 2.25     \n",
      "Epoch 149, CIFAR-10 Batch 4:  stats: loss 0.25937358, accuracy 0.75679982, elapsed 2.28     \n",
      "Epoch 149, CIFAR-10 Batch 5:  stats: loss 0.27027130, accuracy 0.76239991, elapsed 2.26     \n",
      "Epoch 150, CIFAR-10 Batch 1:  stats: loss 0.27981406, accuracy 0.77259982, elapsed 2.27     \n",
      "Epoch 150, CIFAR-10 Batch 2:  stats: loss 0.25886124, accuracy 0.76639980, elapsed 2.26     \n",
      "Epoch 150, CIFAR-10 Batch 3:  stats: loss 0.24731269, accuracy 0.77579987, elapsed 2.27     \n",
      "Epoch 150, CIFAR-10 Batch 4:  stats: loss 0.23150070, accuracy 0.77379990, elapsed 2.26     \n",
      "Epoch 150, CIFAR-10 Batch 5:  stats: loss 0.25445133, accuracy 0.77239978, elapsed 2.27     \n",
      "Epoch 151, CIFAR-10 Batch 1:  stats: loss 0.27681682, accuracy 0.77459979, elapsed 2.29     \n",
      "Epoch 151, CIFAR-10 Batch 2:  stats: loss 0.25398180, accuracy 0.77199984, elapsed 2.28     \n",
      "Epoch 151, CIFAR-10 Batch 3:  stats: loss 0.25704110, accuracy 0.76639980, elapsed 2.28     \n",
      "Epoch 151, CIFAR-10 Batch 4:  stats: loss 0.21080765, accuracy 0.77539980, elapsed 2.25     \n",
      "Epoch 151, CIFAR-10 Batch 5:  stats: loss 0.27450705, accuracy 0.76479989, elapsed 2.28     \n",
      "Epoch 152, CIFAR-10 Batch 1:  stats: loss 0.27535033, accuracy 0.78019977, elapsed 2.26     \n",
      "Epoch 152, CIFAR-10 Batch 2:  stats: loss 0.24878146, accuracy 0.76959985, elapsed 2.27     \n",
      "Epoch 152, CIFAR-10 Batch 3:  stats: loss 0.25688651, accuracy 0.76499981, elapsed 2.27     \n",
      "Epoch 152, CIFAR-10 Batch 4:  stats: loss 0.28865725, accuracy 0.74919981, elapsed 2.26     \n",
      "Epoch 152, CIFAR-10 Batch 5:  stats: loss 0.30054855, accuracy 0.76599979, elapsed 2.27     \n",
      "Epoch 153, CIFAR-10 Batch 1:  stats: loss 0.31219447, accuracy 0.77139980, elapsed 2.28     \n",
      "Epoch 153, CIFAR-10 Batch 2:  stats: loss 0.26512867, accuracy 0.77059984, elapsed 2.27     \n",
      "Epoch 153, CIFAR-10 Batch 3:  stats: loss 0.23920301, accuracy 0.77479982, elapsed 2.24     \n",
      "Epoch 153, CIFAR-10 Batch 4:  stats: loss 0.23120889, accuracy 0.77459979, elapsed 2.26     \n",
      "Epoch 153, CIFAR-10 Batch 5:  stats: loss 0.23891166, accuracy 0.78119981, elapsed 2.25     \n",
      "Epoch 154, CIFAR-10 Batch 1:  stats: loss 0.26692024, accuracy 0.77759981, elapsed 2.27     \n",
      "Epoch 154, CIFAR-10 Batch 2:  stats: loss 0.24525620, accuracy 0.76419979, elapsed 2.28     \n",
      "Epoch 154, CIFAR-10 Batch 3:  stats: loss 0.22995655, accuracy 0.77359980, elapsed 2.27     \n",
      "Epoch 154, CIFAR-10 Batch 4:  stats: loss 0.25867733, accuracy 0.76199991, elapsed 2.26     \n",
      "Epoch 154, CIFAR-10 Batch 5:  stats: loss 0.26254410, accuracy 0.76479977, elapsed 2.25     \n",
      "Epoch 155, CIFAR-10 Batch 1:  stats: loss 0.26418310, accuracy 0.78339982, elapsed 2.28     \n",
      "Epoch 155, CIFAR-10 Batch 2:  stats: loss 0.24657322, accuracy 0.77159977, elapsed 2.27     \n",
      "Epoch 155, CIFAR-10 Batch 3:  stats: loss 0.29150507, accuracy 0.74899983, elapsed 2.26     \n",
      "Epoch 155, CIFAR-10 Batch 4:  stats: loss 0.22262499, accuracy 0.77459979, elapsed 2.30     \n",
      "Epoch 155, CIFAR-10 Batch 5:  stats: loss 0.24992242, accuracy 0.77239972, elapsed 2.27     \n",
      "Epoch 156, CIFAR-10 Batch 1:  stats: loss 0.27074051, accuracy 0.77859986, elapsed 2.28     \n",
      "Epoch 156, CIFAR-10 Batch 2:  stats: loss 0.23846686, accuracy 0.77759981, elapsed 2.26     \n",
      "Epoch 156, CIFAR-10 Batch 3:  stats: loss 0.23946202, accuracy 0.77619988, elapsed 2.26     \n",
      "Epoch 156, CIFAR-10 Batch 4:  stats: loss 0.22363392, accuracy 0.77099985, elapsed 2.25     \n",
      "Epoch 156, CIFAR-10 Batch 5:  stats: loss 0.26212430, accuracy 0.76619983, elapsed 2.27     \n",
      "Epoch 157, CIFAR-10 Batch 1:  stats: loss 0.26865563, accuracy 0.77879983, elapsed 2.27     \n",
      "Epoch 157, CIFAR-10 Batch 2:  stats: loss 0.23581290, accuracy 0.77379990, elapsed 2.27     \n",
      "Epoch 157, CIFAR-10 Batch 3:  stats: loss 0.22366385, accuracy 0.77759987, elapsed 2.26     \n",
      "Epoch 157, CIFAR-10 Batch 4:  stats: loss 0.22911137, accuracy 0.77039981, elapsed 2.26     \n",
      "Epoch 157, CIFAR-10 Batch 5:  stats: loss 0.25628510, accuracy 0.77299988, elapsed 2.28     \n",
      "Epoch 158, CIFAR-10 Batch 1:  stats: loss 0.26414347, accuracy 0.77639979, elapsed 2.25     \n",
      "Epoch 158, CIFAR-10 Batch 2:  stats: loss 0.25551823, accuracy 0.76799989, elapsed 2.27     \n",
      "Epoch 158, CIFAR-10 Batch 3:  stats: loss 0.26927710, accuracy 0.75999981, elapsed 2.25     \n",
      "Epoch 158, CIFAR-10 Batch 4:  stats: loss 0.26866683, accuracy 0.75779980, elapsed 2.26     \n",
      "Epoch 158, CIFAR-10 Batch 5:  stats: loss 0.25943562, accuracy 0.76859987, elapsed 2.27     \n",
      "Epoch 159, CIFAR-10 Batch 1:  stats: loss 0.29079196, accuracy 0.77579975, elapsed 2.25     \n",
      "Epoch 159, CIFAR-10 Batch 2:  stats: loss 0.26207650, accuracy 0.77059984, elapsed 2.27     \n",
      "Epoch 159, CIFAR-10 Batch 3:  stats: loss 0.23283666, accuracy 0.77459985, elapsed 2.28     \n",
      "Epoch 159, CIFAR-10 Batch 4:  stats: loss 0.22695997, accuracy 0.76999986, elapsed 2.26     \n",
      "Epoch 159, CIFAR-10 Batch 5:  stats: loss 0.25891122, accuracy 0.77639985, elapsed 2.26     \n",
      "Epoch 160, CIFAR-10 Batch 1:  stats: loss 0.27437150, accuracy 0.77359980, elapsed 2.28     \n",
      "Epoch 160, CIFAR-10 Batch 2:  stats: loss 0.25935781, accuracy 0.76939976, elapsed 2.27     \n",
      "Epoch 160, CIFAR-10 Batch 3:  stats: loss 0.24269968, accuracy 0.77339977, elapsed 2.27     \n",
      "Epoch 160, CIFAR-10 Batch 4:  stats: loss 0.22019884, accuracy 0.77099979, elapsed 2.24     \n",
      "Epoch 160, CIFAR-10 Batch 5:  stats: loss 0.27916619, accuracy 0.77199984, elapsed 2.25     \n",
      "Epoch 161, CIFAR-10 Batch 1:  stats: loss 0.27336818, accuracy 0.77479982, elapsed 2.26     \n",
      "Epoch 161, CIFAR-10 Batch 2:  stats: loss 0.27152413, accuracy 0.77059978, elapsed 2.25     \n",
      "Epoch 161, CIFAR-10 Batch 3:  stats: loss 0.23284926, accuracy 0.76719981, elapsed 2.28     \n",
      "Epoch 161, CIFAR-10 Batch 4:  stats: loss 0.24508727, accuracy 0.76599979, elapsed 2.25     \n",
      "Epoch 161, CIFAR-10 Batch 5:  stats: loss 0.27815756, accuracy 0.76219982, elapsed 2.29     \n",
      "Epoch 162, CIFAR-10 Batch 1:  stats: loss 0.27297547, accuracy 0.77139980, elapsed 2.29     \n",
      "Epoch 162, CIFAR-10 Batch 2:  stats: loss 0.28106418, accuracy 0.77019984, elapsed 2.26     \n",
      "Epoch 162, CIFAR-10 Batch 3:  stats: loss 0.21209092, accuracy 0.77419984, elapsed 2.24     \n",
      "Epoch 162, CIFAR-10 Batch 4:  stats: loss 0.29086167, accuracy 0.75279987, elapsed 2.25     \n",
      "Epoch 162, CIFAR-10 Batch 5:  stats: loss 0.26580888, accuracy 0.76579982, elapsed 2.27     \n",
      "Epoch 163, CIFAR-10 Batch 1:  stats: loss 0.27948299, accuracy 0.77639985, elapsed 2.28     \n",
      "Epoch 163, CIFAR-10 Batch 2:  stats: loss 0.27352720, accuracy 0.76939982, elapsed 2.26     \n",
      "Epoch 163, CIFAR-10 Batch 3:  stats: loss 0.25411198, accuracy 0.76759982, elapsed 2.27     \n",
      "Epoch 163, CIFAR-10 Batch 4:  stats: loss 0.22417706, accuracy 0.77299982, elapsed 2.27     \n",
      "Epoch 163, CIFAR-10 Batch 5:  stats: loss 0.26443955, accuracy 0.76799983, elapsed 2.25     \n",
      "Epoch 164, CIFAR-10 Batch 1:  stats: loss 0.31568936, accuracy 0.76259977, elapsed 2.28     \n",
      "Epoch 164, CIFAR-10 Batch 2:  stats: loss 0.28833160, accuracy 0.77019984, elapsed 2.27     \n",
      "Epoch 164, CIFAR-10 Batch 3:  stats: loss 0.24271251, accuracy 0.77019984, elapsed 2.26     \n",
      "Epoch 164, CIFAR-10 Batch 4:  stats: loss 0.23104346, accuracy 0.77359986, elapsed 2.28     \n",
      "Epoch 164, CIFAR-10 Batch 5:  stats: loss 0.25010714, accuracy 0.77459979, elapsed 2.25     \n",
      "Epoch 165, CIFAR-10 Batch 1:  stats: loss 0.28255072, accuracy 0.77979982, elapsed 2.28     \n",
      "Epoch 165, CIFAR-10 Batch 2:  stats: loss 0.26198182, accuracy 0.76899981, elapsed 2.28     \n",
      "Epoch 165, CIFAR-10 Batch 3:  stats: loss 0.21345520, accuracy 0.77739978, elapsed 2.27     \n",
      "Epoch 165, CIFAR-10 Batch 4:  stats: loss 0.21479519, accuracy 0.77179986, elapsed 2.25     \n",
      "Epoch 165, CIFAR-10 Batch 5:  stats: loss 0.25995147, accuracy 0.75639981, elapsed 2.26     \n",
      "Epoch 166, CIFAR-10 Batch 1:  stats: loss 0.26687214, accuracy 0.76839978, elapsed 2.26     \n",
      "Epoch 166, CIFAR-10 Batch 2:  stats: loss 0.24861707, accuracy 0.76859981, elapsed 2.24     \n",
      "Epoch 166, CIFAR-10 Batch 3:  stats: loss 0.21766469, accuracy 0.77499980, elapsed 2.27     \n",
      "Epoch 166, CIFAR-10 Batch 4:  stats: loss 0.20710780, accuracy 0.77219987, elapsed 2.27     \n",
      "Epoch 166, CIFAR-10 Batch 5:  stats: loss 0.25014117, accuracy 0.76979977, elapsed 2.26     \n",
      "Epoch 167, CIFAR-10 Batch 1:  stats: loss 0.28264669, accuracy 0.77799982, elapsed 2.28     \n",
      "Epoch 167, CIFAR-10 Batch 2:  stats: loss 0.25373662, accuracy 0.76559991, elapsed 2.26     \n",
      "Epoch 167, CIFAR-10 Batch 3:  stats: loss 0.21817730, accuracy 0.77899987, elapsed 2.27     \n",
      "Epoch 167, CIFAR-10 Batch 4:  stats: loss 0.21242291, accuracy 0.77379984, elapsed 2.26     \n",
      "Epoch 167, CIFAR-10 Batch 5:  stats: loss 0.23082246, accuracy 0.77059978, elapsed 2.28     \n",
      "Epoch 168, CIFAR-10 Batch 1:  stats: loss 0.27823764, accuracy 0.78279984, elapsed 2.27     \n",
      "Epoch 168, CIFAR-10 Batch 2:  stats: loss 0.25820985, accuracy 0.76879978, elapsed 2.27     \n",
      "Epoch 168, CIFAR-10 Batch 3:  stats: loss 0.22124299, accuracy 0.77219981, elapsed 2.24     \n",
      "Epoch 168, CIFAR-10 Batch 4:  stats: loss 0.22260791, accuracy 0.76759982, elapsed 2.27     \n",
      "Epoch 168, CIFAR-10 Batch 5:  stats: loss 0.23964643, accuracy 0.76439983, elapsed 2.28     \n",
      "Epoch 169, CIFAR-10 Batch 1:  stats: loss 0.27814314, accuracy 0.77859974, elapsed 2.28     \n",
      "Epoch 169, CIFAR-10 Batch 2:  stats: loss 0.25409606, accuracy 0.76599991, elapsed 2.28     \n",
      "Epoch 169, CIFAR-10 Batch 3:  stats: loss 0.22083645, accuracy 0.77379984, elapsed 2.26     \n",
      "Epoch 169, CIFAR-10 Batch 4:  stats: loss 0.22011203, accuracy 0.76959980, elapsed 2.27     \n",
      "Epoch 169, CIFAR-10 Batch 5:  stats: loss 0.23027006, accuracy 0.77459985, elapsed 2.27     \n",
      "Epoch 170, CIFAR-10 Batch 1:  stats: loss 0.25009647, accuracy 0.77959985, elapsed 2.28     \n",
      "Epoch 170, CIFAR-10 Batch 2:  stats: loss 0.23112011, accuracy 0.77199978, elapsed 2.27     \n",
      "Epoch 170, CIFAR-10 Batch 3:  stats: loss 0.21517585, accuracy 0.78139985, elapsed 2.27     \n",
      "Epoch 170, CIFAR-10 Batch 4:  stats: loss 0.20682485, accuracy 0.77479982, elapsed 2.24     \n",
      "Epoch 170, CIFAR-10 Batch 5:  stats: loss 0.24247129, accuracy 0.77099979, elapsed 2.26     \n",
      "Epoch 171, CIFAR-10 Batch 1:  stats: loss 0.27669051, accuracy 0.77039987, elapsed 2.28     \n",
      "Epoch 171, CIFAR-10 Batch 2:  stats: loss 0.25772938, accuracy 0.76279974, elapsed 2.25     \n",
      "Epoch 171, CIFAR-10 Batch 3:  stats: loss 0.23643628, accuracy 0.76879978, elapsed 2.27     \n",
      "Epoch 171, CIFAR-10 Batch 4:  stats: loss 0.20869912, accuracy 0.76719981, elapsed 2.26     \n",
      "Epoch 171, CIFAR-10 Batch 5:  stats: loss 0.23804338, accuracy 0.78079987, elapsed 2.24     \n",
      "Epoch 172, CIFAR-10 Batch 1:  stats: loss 0.25456262, accuracy 0.77919984, elapsed 2.26     \n",
      "Epoch 172, CIFAR-10 Batch 2:  stats: loss 0.23270980, accuracy 0.76199985, elapsed 2.25     \n",
      "Epoch 172, CIFAR-10 Batch 3:  stats: loss 0.20647284, accuracy 0.77059984, elapsed 2.28     \n",
      "Epoch 172, CIFAR-10 Batch 4:  stats: loss 0.22558717, accuracy 0.76019984, elapsed 2.27     \n",
      "Epoch 172, CIFAR-10 Batch 5:  stats: loss 0.23730752, accuracy 0.76899981, elapsed 2.27     \n",
      "Epoch 173, CIFAR-10 Batch 1:  stats: loss 0.24322022, accuracy 0.78339982, elapsed 2.27     \n",
      "Epoch 173, CIFAR-10 Batch 2:  stats: loss 0.23187020, accuracy 0.77139986, elapsed 2.27     \n",
      "Epoch 173, CIFAR-10 Batch 3:  stats: loss 0.20729133, accuracy 0.77379984, elapsed 2.27     \n",
      "Epoch 173, CIFAR-10 Batch 4:  stats: loss 0.20799330, accuracy 0.77279985, elapsed 2.28     \n",
      "Epoch 173, CIFAR-10 Batch 5:  stats: loss 0.23527724, accuracy 0.77659988, elapsed 2.28     \n",
      "Epoch 174, CIFAR-10 Batch 1:  stats: loss 0.25900289, accuracy 0.77219981, elapsed 2.29     \n",
      "Epoch 174, CIFAR-10 Batch 2:  stats: loss 0.23810761, accuracy 0.77279979, elapsed 2.27     \n",
      "Epoch 174, CIFAR-10 Batch 3:  stats: loss 0.24182603, accuracy 0.76839978, elapsed 2.24     \n",
      "Epoch 174, CIFAR-10 Batch 4:  stats: loss 0.21913061, accuracy 0.77439976, elapsed 2.26     \n",
      "Epoch 174, CIFAR-10 Batch 5:  stats: loss 0.23029912, accuracy 0.77459985, elapsed 2.27     \n",
      "Epoch 175, CIFAR-10 Batch 1:  stats: loss 0.28275442, accuracy 0.77199978, elapsed 2.24     \n",
      "Epoch 175, CIFAR-10 Batch 2:  stats: loss 0.24385253, accuracy 0.75839984, elapsed 2.26     \n",
      "Epoch 175, CIFAR-10 Batch 3:  stats: loss 0.20810977, accuracy 0.77739990, elapsed 2.26     \n",
      "Epoch 175, CIFAR-10 Batch 4:  stats: loss 0.24241126, accuracy 0.75759983, elapsed 2.27     \n",
      "Epoch 175, CIFAR-10 Batch 5:  stats: loss 0.23922822, accuracy 0.77439982, elapsed 2.27     \n",
      "Epoch 176, CIFAR-10 Batch 1:  stats: loss 0.26013258, accuracy 0.77799982, elapsed 2.28     \n",
      "Epoch 176, CIFAR-10 Batch 2:  stats: loss 0.24039796, accuracy 0.76559979, elapsed 2.26     \n",
      "Epoch 176, CIFAR-10 Batch 3:  stats: loss 0.21811007, accuracy 0.77099979, elapsed 2.27     \n",
      "Epoch 176, CIFAR-10 Batch 4:  stats: loss 0.22933327, accuracy 0.76679987, elapsed 2.25     \n",
      "Epoch 176, CIFAR-10 Batch 5:  stats: loss 0.24571456, accuracy 0.77179980, elapsed 2.27     \n",
      "Epoch 177, CIFAR-10 Batch 1:  stats: loss 0.25798994, accuracy 0.77799982, elapsed 2.26     \n",
      "Epoch 177, CIFAR-10 Batch 2:  stats: loss 0.24790974, accuracy 0.77019978, elapsed 2.27     \n",
      "Epoch 177, CIFAR-10 Batch 3:  stats: loss 0.20853055, accuracy 0.77859986, elapsed 2.27     \n",
      "Epoch 177, CIFAR-10 Batch 4:  stats: loss 0.20903029, accuracy 0.77319986, elapsed 2.25     \n",
      "Epoch 177, CIFAR-10 Batch 5:  stats: loss 0.24540973, accuracy 0.76959980, elapsed 2.26     \n",
      "Epoch 178, CIFAR-10 Batch 1:  stats: loss 0.26550880, accuracy 0.76819980, elapsed 2.28     \n",
      "Epoch 178, CIFAR-10 Batch 2:  stats: loss 0.22271474, accuracy 0.76239985, elapsed 2.27     \n",
      "Epoch 178, CIFAR-10 Batch 3:  stats: loss 0.21097353, accuracy 0.78079975, elapsed 2.25     \n",
      "Epoch 178, CIFAR-10 Batch 4:  stats: loss 0.20826408, accuracy 0.76279980, elapsed 2.28     \n",
      "Epoch 178, CIFAR-10 Batch 5:  stats: loss 0.23042990, accuracy 0.77439976, elapsed 2.28     \n",
      "Epoch 179, CIFAR-10 Batch 1:  stats: loss 0.24397671, accuracy 0.77279979, elapsed 2.27     \n",
      "Epoch 179, CIFAR-10 Batch 2:  stats: loss 0.22793153, accuracy 0.76479983, elapsed 2.27     \n",
      "Epoch 179, CIFAR-10 Batch 3:  stats: loss 0.21998140, accuracy 0.77419990, elapsed 2.26     \n",
      "Epoch 179, CIFAR-10 Batch 4:  stats: loss 0.25016421, accuracy 0.75659978, elapsed 2.28     \n",
      "Epoch 179, CIFAR-10 Batch 5:  stats: loss 0.24296549, accuracy 0.77279985, elapsed 2.27     \n",
      "Epoch 180, CIFAR-10 Batch 1:  stats: loss 0.26305315, accuracy 0.77979982, elapsed 2.25     \n",
      "Epoch 180, CIFAR-10 Batch 2:  stats: loss 0.21985897, accuracy 0.77019984, elapsed 2.26     \n",
      "Epoch 180, CIFAR-10 Batch 3:  stats: loss 0.20977031, accuracy 0.77859980, elapsed 2.25     \n",
      "Epoch 180, CIFAR-10 Batch 4:  stats: loss 0.22120255, accuracy 0.77619982, elapsed 2.25     \n",
      "Epoch 180, CIFAR-10 Batch 5:  stats: loss 0.23689954, accuracy 0.76959980, elapsed 2.28     \n",
      "Epoch 181, CIFAR-10 Batch 1:  stats: loss 0.25407460, accuracy 0.78199983, elapsed 2.29     \n",
      "Epoch 181, CIFAR-10 Batch 2:  stats: loss 0.22210965, accuracy 0.77339983, elapsed 2.27     \n",
      "Epoch 181, CIFAR-10 Batch 3:  stats: loss 0.20851037, accuracy 0.77939981, elapsed 2.26     \n",
      "Epoch 181, CIFAR-10 Batch 4:  stats: loss 0.24725935, accuracy 0.77219987, elapsed 2.26     \n",
      "Epoch 181, CIFAR-10 Batch 5:  stats: loss 0.24777427, accuracy 0.77419984, elapsed 2.29     \n",
      "Epoch 182, CIFAR-10 Batch 1:  stats: loss 0.24758762, accuracy 0.77759981, elapsed 2.28     \n",
      "Epoch 182, CIFAR-10 Batch 2:  stats: loss 0.21393313, accuracy 0.77539986, elapsed 2.27     \n",
      "Epoch 182, CIFAR-10 Batch 3:  stats: loss 0.22457105, accuracy 0.77239990, elapsed 2.25     \n",
      "Epoch 182, CIFAR-10 Batch 4:  stats: loss 0.24194810, accuracy 0.76599985, elapsed 2.26     \n",
      "Epoch 182, CIFAR-10 Batch 5:  stats: loss 0.22942898, accuracy 0.77899981, elapsed 2.34     \n",
      "Epoch 183, CIFAR-10 Batch 1:  stats: loss 0.24144648, accuracy 0.77139986, elapsed 2.70     \n",
      "Epoch 183, CIFAR-10 Batch 2:  stats: loss 0.23651731, accuracy 0.76559979, elapsed 2.28     \n",
      "Epoch 183, CIFAR-10 Batch 3:  stats: loss 0.21614802, accuracy 0.77559984, elapsed 2.27     \n",
      "Epoch 183, CIFAR-10 Batch 4:  stats: loss 0.20033196, accuracy 0.76399982, elapsed 2.26     \n",
      "Epoch 183, CIFAR-10 Batch 5:  stats: loss 0.24913837, accuracy 0.77019978, elapsed 2.25     \n",
      "Epoch 184, CIFAR-10 Batch 1:  stats: loss 0.25474641, accuracy 0.77219981, elapsed 2.26     \n",
      "Epoch 184, CIFAR-10 Batch 2:  stats: loss 0.25633442, accuracy 0.75799984, elapsed 2.28     \n",
      "Epoch 184, CIFAR-10 Batch 3:  stats: loss 0.25386220, accuracy 0.76399982, elapsed 2.27     \n",
      "Epoch 184, CIFAR-10 Batch 4:  stats: loss 0.23144475, accuracy 0.76459980, elapsed 2.25     \n",
      "Epoch 184, CIFAR-10 Batch 5:  stats: loss 0.23465484, accuracy 0.77359974, elapsed 2.26     \n",
      "Epoch 185, CIFAR-10 Batch 1:  stats: loss 0.24111819, accuracy 0.77459985, elapsed 2.30     \n",
      "Epoch 185, CIFAR-10 Batch 2:  stats: loss 0.22635849, accuracy 0.77359986, elapsed 2.27     \n",
      "Epoch 185, CIFAR-10 Batch 3:  stats: loss 0.23000297, accuracy 0.77539980, elapsed 2.28     \n",
      "Epoch 185, CIFAR-10 Batch 4:  stats: loss 0.20212618, accuracy 0.76899981, elapsed 2.27     \n",
      "Epoch 185, CIFAR-10 Batch 5:  stats: loss 0.25666097, accuracy 0.77039981, elapsed 2.27     \n",
      "Epoch 186, CIFAR-10 Batch 1:  stats: loss 0.23926184, accuracy 0.77579981, elapsed 2.24     \n",
      "Epoch 186, CIFAR-10 Batch 2:  stats: loss 0.22708058, accuracy 0.76719981, elapsed 2.25     \n",
      "Epoch 186, CIFAR-10 Batch 3:  stats: loss 0.20672965, accuracy 0.77679980, elapsed 2.26     \n",
      "Epoch 186, CIFAR-10 Batch 4:  stats: loss 0.20008990, accuracy 0.76899981, elapsed 2.28     \n",
      "Epoch 186, CIFAR-10 Batch 5:  stats: loss 0.23983306, accuracy 0.76419979, elapsed 2.27     \n",
      "Epoch 187, CIFAR-10 Batch 1:  stats: loss 0.24646628, accuracy 0.77719980, elapsed 2.28     \n",
      "Epoch 187, CIFAR-10 Batch 2:  stats: loss 0.24036568, accuracy 0.76539981, elapsed 2.25     \n",
      "Epoch 187, CIFAR-10 Batch 3:  stats: loss 0.24985611, accuracy 0.76359987, elapsed 2.27     \n",
      "Epoch 187, CIFAR-10 Batch 4:  stats: loss 0.18709198, accuracy 0.78039980, elapsed 2.27     \n",
      "Epoch 187, CIFAR-10 Batch 5:  stats: loss 0.23049150, accuracy 0.77419984, elapsed 2.27     \n",
      "Epoch 188, CIFAR-10 Batch 1:  stats: loss 0.24896492, accuracy 0.76399988, elapsed 2.26     \n",
      "Epoch 188, CIFAR-10 Batch 2:  stats: loss 0.21143295, accuracy 0.77739984, elapsed 2.27     \n",
      "Epoch 188, CIFAR-10 Batch 3:  stats: loss 0.21090446, accuracy 0.78039974, elapsed 2.26     \n",
      "Epoch 188, CIFAR-10 Batch 4:  stats: loss 0.20801979, accuracy 0.76919979, elapsed 2.27     \n",
      "Epoch 188, CIFAR-10 Batch 5:  stats: loss 0.22715531, accuracy 0.77779984, elapsed 2.48     \n",
      "Epoch 189, CIFAR-10 Batch 1:  stats: loss 0.27206612, accuracy 0.76259983, elapsed 2.46     \n",
      "Epoch 189, CIFAR-10 Batch 2:  stats: loss 0.25997636, accuracy 0.76979983, elapsed 2.24     \n",
      "Epoch 189, CIFAR-10 Batch 3:  stats: loss 0.21150947, accuracy 0.77639985, elapsed 2.28     \n",
      "Epoch 189, CIFAR-10 Batch 4:  stats: loss 0.20001362, accuracy 0.77779979, elapsed 2.26     \n",
      "Epoch 189, CIFAR-10 Batch 5:  stats: loss 0.23540035, accuracy 0.77359980, elapsed 2.24     \n",
      "Epoch 190, CIFAR-10 Batch 1:  stats: loss 0.24768746, accuracy 0.77499980, elapsed 2.28     \n",
      "Epoch 190, CIFAR-10 Batch 2:  stats: loss 0.24508885, accuracy 0.75839984, elapsed 2.26     \n",
      "Epoch 190, CIFAR-10 Batch 3:  stats: loss 0.26275167, accuracy 0.75539982, elapsed 2.25     \n",
      "Epoch 190, CIFAR-10 Batch 4:  stats: loss 0.22526066, accuracy 0.76959980, elapsed 2.29     \n",
      "Epoch 190, CIFAR-10 Batch 5:  stats: loss 0.21933407, accuracy 0.77579981, elapsed 2.27     \n",
      "Epoch 191, CIFAR-10 Batch 1:  stats: loss 0.23471549, accuracy 0.77399981, elapsed 2.28     \n",
      "Epoch 191, CIFAR-10 Batch 2:  stats: loss 0.22371376, accuracy 0.77339983, elapsed 2.25     \n",
      "Epoch 191, CIFAR-10 Batch 3:  stats: loss 0.20188269, accuracy 0.77659982, elapsed 2.26     \n",
      "Epoch 191, CIFAR-10 Batch 4:  stats: loss 0.20085649, accuracy 0.77459991, elapsed 2.27     \n",
      "Epoch 191, CIFAR-10 Batch 5:  stats: loss 0.21939476, accuracy 0.77499986, elapsed 2.25     \n",
      "Epoch 192, CIFAR-10 Batch 1:  stats: loss 0.24412067, accuracy 0.77419984, elapsed 2.28     \n",
      "Epoch 192, CIFAR-10 Batch 2:  stats: loss 0.21542877, accuracy 0.76999986, elapsed 2.27     \n",
      "Epoch 192, CIFAR-10 Batch 3:  stats: loss 0.20199382, accuracy 0.77459979, elapsed 2.26     \n",
      "Epoch 192, CIFAR-10 Batch 4:  stats: loss 0.20425838, accuracy 0.77219975, elapsed 2.25     \n",
      "Epoch 192, CIFAR-10 Batch 5:  stats: loss 0.23512502, accuracy 0.77179980, elapsed 2.25     \n",
      "Epoch 193, CIFAR-10 Batch 1:  stats: loss 0.24431980, accuracy 0.77399981, elapsed 2.26     \n",
      "Epoch 193, CIFAR-10 Batch 2:  stats: loss 0.23046862, accuracy 0.77159983, elapsed 2.27     \n",
      "Epoch 193, CIFAR-10 Batch 3:  stats: loss 0.21744922, accuracy 0.77099985, elapsed 2.28     \n",
      "Epoch 193, CIFAR-10 Batch 4:  stats: loss 0.20848036, accuracy 0.77099979, elapsed 2.27     \n",
      "Epoch 193, CIFAR-10 Batch 5:  stats: loss 0.21549390, accuracy 0.77559978, elapsed 2.25     \n",
      "Epoch 194, CIFAR-10 Batch 1:  stats: loss 0.22182018, accuracy 0.77679986, elapsed 2.28     \n",
      "Epoch 194, CIFAR-10 Batch 2:  stats: loss 0.22411536, accuracy 0.78239983, elapsed 2.27     \n",
      "Epoch 194, CIFAR-10 Batch 3:  stats: loss 0.20095503, accuracy 0.77899981, elapsed 2.27     \n",
      "Epoch 194, CIFAR-10 Batch 4:  stats: loss 0.19498071, accuracy 0.77799982, elapsed 2.25     \n",
      "Epoch 194, CIFAR-10 Batch 5:  stats: loss 0.21113181, accuracy 0.77619982, elapsed 2.28     \n",
      "Epoch 195, CIFAR-10 Batch 1:  stats: loss 0.23147303, accuracy 0.77679980, elapsed 2.28     \n",
      "Epoch 195, CIFAR-10 Batch 2:  stats: loss 0.20770510, accuracy 0.76539987, elapsed 2.26     \n",
      "Epoch 195, CIFAR-10 Batch 3:  stats: loss 0.18836650, accuracy 0.76539987, elapsed 2.27     \n",
      "Epoch 195, CIFAR-10 Batch 4:  stats: loss 0.19936123, accuracy 0.76759988, elapsed 2.29     \n",
      "Epoch 195, CIFAR-10 Batch 5:  stats: loss 0.22378139, accuracy 0.77939981, elapsed 2.27     \n",
      "Epoch 196, CIFAR-10 Batch 1:  stats: loss 0.22127041, accuracy 0.77359974, elapsed 2.29     \n",
      "Epoch 196, CIFAR-10 Batch 2:  stats: loss 0.21887982, accuracy 0.77199978, elapsed 2.26     \n",
      "Epoch 196, CIFAR-10 Batch 3:  stats: loss 0.19611382, accuracy 0.77199984, elapsed 2.25     \n",
      "Epoch 196, CIFAR-10 Batch 4:  stats: loss 0.21030964, accuracy 0.76319987, elapsed 2.27     \n",
      "Epoch 196, CIFAR-10 Batch 5:  stats: loss 0.22455743, accuracy 0.77499986, elapsed 2.28     \n",
      "Epoch 197, CIFAR-10 Batch 1:  stats: loss 0.24773215, accuracy 0.77219993, elapsed 2.27     \n",
      "Epoch 197, CIFAR-10 Batch 2:  stats: loss 0.22891873, accuracy 0.77279979, elapsed 2.24     \n",
      "Epoch 197, CIFAR-10 Batch 3:  stats: loss 0.21998852, accuracy 0.77319986, elapsed 2.27     \n",
      "Epoch 197, CIFAR-10 Batch 4:  stats: loss 0.21604824, accuracy 0.76259989, elapsed 2.26     \n",
      "Epoch 197, CIFAR-10 Batch 5:  stats: loss 0.23251808, accuracy 0.78379977, elapsed 2.27     \n",
      "Epoch 198, CIFAR-10 Batch 1:  stats: loss 0.24524254, accuracy 0.77419978, elapsed 2.29     \n",
      "Epoch 198, CIFAR-10 Batch 2:  stats: loss 0.20336500, accuracy 0.77019989, elapsed 2.26     \n",
      "Epoch 198, CIFAR-10 Batch 3:  stats: loss 0.19565554, accuracy 0.77359980, elapsed 2.27     \n",
      "Epoch 198, CIFAR-10 Batch 4:  stats: loss 0.18944155, accuracy 0.77399981, elapsed 2.26     \n",
      "Epoch 198, CIFAR-10 Batch 5:  stats: loss 0.22355565, accuracy 0.77479982, elapsed 2.30     \n",
      "Epoch 199, CIFAR-10 Batch 1:  stats: loss 0.25038302, accuracy 0.77059990, elapsed 2.27     \n",
      "Epoch 199, CIFAR-10 Batch 2:  stats: loss 0.20851323, accuracy 0.77479982, elapsed 2.27     \n",
      "Epoch 199, CIFAR-10 Batch 3:  stats: loss 0.17742793, accuracy 0.77099985, elapsed 2.26     \n",
      "Epoch 199, CIFAR-10 Batch 4:  stats: loss 0.19493666, accuracy 0.76339978, elapsed 2.26     \n",
      "Epoch 199, CIFAR-10 Batch 5:  stats: loss 0.22224854, accuracy 0.77239984, elapsed 2.26     \n",
      "Epoch 200, CIFAR-10 Batch 1:  stats: loss 0.25316015, accuracy 0.77359992, elapsed 2.27     \n",
      "Epoch 200, CIFAR-10 Batch 2:  stats: loss 0.18246424, accuracy 0.77339983, elapsed 2.26     \n",
      "Epoch 200, CIFAR-10 Batch 3:  stats: loss 0.19697790, accuracy 0.77879983, elapsed 2.26     \n",
      "Epoch 200, CIFAR-10 Batch 4:  stats: loss 0.19793420, accuracy 0.77579987, elapsed 2.27     \n",
      "Epoch 200, CIFAR-10 Batch 5:  stats: loss 0.22808670, accuracy 0.77619982, elapsed 2.27     \n",
      "Epoch 201, CIFAR-10 Batch 1:  stats: loss 0.26525515, accuracy 0.77559984, elapsed 2.27     \n",
      "Epoch 201, CIFAR-10 Batch 2:  stats: loss 0.21817181, accuracy 0.76919979, elapsed 2.25     \n",
      "Epoch 201, CIFAR-10 Batch 3:  stats: loss 0.19563231, accuracy 0.77419984, elapsed 2.26     \n",
      "Epoch 201, CIFAR-10 Batch 4:  stats: loss 0.18245217, accuracy 0.78199983, elapsed 2.28     \n",
      "Epoch 201, CIFAR-10 Batch 5:  stats: loss 0.25003791, accuracy 0.76179981, elapsed 2.27     \n",
      "Epoch 202, CIFAR-10 Batch 1:  stats: loss 0.24473867, accuracy 0.77899981, elapsed 2.27     \n",
      "Epoch 202, CIFAR-10 Batch 2:  stats: loss 0.19613078, accuracy 0.77579975, elapsed 2.26     \n",
      "Epoch 202, CIFAR-10 Batch 3:  stats: loss 0.19546880, accuracy 0.77959979, elapsed 2.25     \n",
      "Epoch 202, CIFAR-10 Batch 4:  stats: loss 0.19779173, accuracy 0.77999985, elapsed 2.29     \n",
      "Epoch 202, CIFAR-10 Batch 5:  stats: loss 0.22696565, accuracy 0.77279985, elapsed 2.27     \n",
      "Epoch 203, CIFAR-10 Batch 1:  stats: loss 0.25400427, accuracy 0.78119975, elapsed 2.28     \n",
      "Epoch 203, CIFAR-10 Batch 2:  stats: loss 0.21440896, accuracy 0.77359980, elapsed 2.26     \n",
      "Epoch 203, CIFAR-10 Batch 3:  stats: loss 0.19182289, accuracy 0.77819979, elapsed 2.27     \n",
      "Epoch 203, CIFAR-10 Batch 4:  stats: loss 0.16910636, accuracy 0.77879983, elapsed 2.25     \n",
      "Epoch 203, CIFAR-10 Batch 5:  stats: loss 0.21826234, accuracy 0.78079981, elapsed 2.25     \n",
      "Epoch 204, CIFAR-10 Batch 1:  stats: loss 0.23799387, accuracy 0.77099991, elapsed 2.27     \n",
      "Epoch 204, CIFAR-10 Batch 2:  stats: loss 0.19329540, accuracy 0.77399987, elapsed 2.30     \n",
      "Epoch 204, CIFAR-10 Batch 3:  stats: loss 0.19801849, accuracy 0.76859981, elapsed 2.25     \n",
      "Epoch 204, CIFAR-10 Batch 4:  stats: loss 0.19222367, accuracy 0.76499987, elapsed 2.27     \n",
      "Epoch 204, CIFAR-10 Batch 5:  stats: loss 0.21336377, accuracy 0.77499980, elapsed 2.26     \n",
      "Epoch 205, CIFAR-10 Batch 1:  stats: loss 0.23894675, accuracy 0.78019977, elapsed 2.29     \n",
      "Epoch 205, CIFAR-10 Batch 2:  stats: loss 0.18960455, accuracy 0.77999985, elapsed 2.27     \n",
      "Epoch 205, CIFAR-10 Batch 3:  stats: loss 0.18896458, accuracy 0.77979976, elapsed 2.27     \n",
      "Epoch 205, CIFAR-10 Batch 4:  stats: loss 0.18271138, accuracy 0.77639991, elapsed 2.26     \n",
      "Epoch 205, CIFAR-10 Batch 5:  stats: loss 0.21785721, accuracy 0.77679980, elapsed 2.25     \n",
      "Epoch 206, CIFAR-10 Batch 1:  stats: loss 0.26166996, accuracy 0.76619977, elapsed 2.28     \n",
      "Epoch 206, CIFAR-10 Batch 2:  stats: loss 0.20784798, accuracy 0.77899981, elapsed 2.26     \n",
      "Epoch 206, CIFAR-10 Batch 3:  stats: loss 0.20411594, accuracy 0.77279985, elapsed 2.25     \n",
      "Epoch 206, CIFAR-10 Batch 4:  stats: loss 0.18148541, accuracy 0.77599984, elapsed 2.26     \n",
      "Epoch 206, CIFAR-10 Batch 5:  stats: loss 0.20749258, accuracy 0.77399987, elapsed 2.27     \n",
      "Epoch 207, CIFAR-10 Batch 1:  stats: loss 0.22887935, accuracy 0.77639985, elapsed 2.26     \n",
      "Epoch 207, CIFAR-10 Batch 2:  stats: loss 0.20984787, accuracy 0.76779979, elapsed 2.28     \n",
      "Epoch 207, CIFAR-10 Batch 3:  stats: loss 0.21478929, accuracy 0.77259988, elapsed 2.27     \n",
      "Epoch 207, CIFAR-10 Batch 4:  stats: loss 0.20096135, accuracy 0.76859987, elapsed 2.27     \n",
      "Epoch 207, CIFAR-10 Batch 5:  stats: loss 0.20460200, accuracy 0.77919984, elapsed 2.26     \n",
      "Epoch 208, CIFAR-10 Batch 1:  stats: loss 0.26479316, accuracy 0.76959980, elapsed 2.29     \n",
      "Epoch 208, CIFAR-10 Batch 2:  stats: loss 0.19264394, accuracy 0.76859981, elapsed 2.26     \n",
      "Epoch 208, CIFAR-10 Batch 3:  stats: loss 0.20296063, accuracy 0.76839983, elapsed 2.26     \n",
      "Epoch 208, CIFAR-10 Batch 4:  stats: loss 0.18416177, accuracy 0.77199984, elapsed 2.27     \n",
      "Epoch 208, CIFAR-10 Batch 5:  stats: loss 0.19167754, accuracy 0.78459990, elapsed 2.26     \n",
      "Epoch 209, CIFAR-10 Batch 1:  stats: loss 0.23234594, accuracy 0.77739990, elapsed 2.29     \n",
      "Epoch 209, CIFAR-10 Batch 2:  stats: loss 0.18865070, accuracy 0.77459979, elapsed 2.26     \n",
      "Epoch 209, CIFAR-10 Batch 3:  stats: loss 0.19854844, accuracy 0.77099985, elapsed 2.27     \n",
      "Epoch 209, CIFAR-10 Batch 4:  stats: loss 0.18679689, accuracy 0.77479982, elapsed 2.26     \n",
      "Epoch 209, CIFAR-10 Batch 5:  stats: loss 0.20481834, accuracy 0.76539981, elapsed 2.28     \n",
      "Epoch 210, CIFAR-10 Batch 1:  stats: loss 0.23496062, accuracy 0.77259982, elapsed 2.26     \n",
      "Epoch 210, CIFAR-10 Batch 2:  stats: loss 0.18781169, accuracy 0.77719980, elapsed 2.27     \n",
      "Epoch 210, CIFAR-10 Batch 3:  stats: loss 0.18377791, accuracy 0.78279984, elapsed 2.28     \n",
      "Epoch 210, CIFAR-10 Batch 4:  stats: loss 0.17829229, accuracy 0.77539986, elapsed 2.28     \n",
      "Epoch 210, CIFAR-10 Batch 5:  stats: loss 0.21449295, accuracy 0.77339989, elapsed 2.27     \n",
      "Epoch 211, CIFAR-10 Batch 1:  stats: loss 0.24799059, accuracy 0.76839983, elapsed 2.27     \n",
      "Epoch 211, CIFAR-10 Batch 2:  stats: loss 0.20031768, accuracy 0.77059978, elapsed 2.25     \n",
      "Epoch 211, CIFAR-10 Batch 3:  stats: loss 0.18258782, accuracy 0.77699983, elapsed 2.29     \n",
      "Epoch 211, CIFAR-10 Batch 4:  stats: loss 0.18819168, accuracy 0.77379984, elapsed 2.29     \n",
      "Epoch 211, CIFAR-10 Batch 5:  stats: loss 0.19461191, accuracy 0.77799982, elapsed 2.26     \n",
      "Epoch 212, CIFAR-10 Batch 1:  stats: loss 0.22997922, accuracy 0.77159989, elapsed 2.27     \n",
      "Epoch 212, CIFAR-10 Batch 2:  stats: loss 0.21188821, accuracy 0.76759982, elapsed 2.25     \n",
      "Epoch 212, CIFAR-10 Batch 3:  stats: loss 0.18615919, accuracy 0.78179991, elapsed 2.26     \n",
      "Epoch 212, CIFAR-10 Batch 4:  stats: loss 0.19086619, accuracy 0.77239984, elapsed 2.26     \n",
      "Epoch 212, CIFAR-10 Batch 5:  stats: loss 0.20046528, accuracy 0.77739984, elapsed 2.25     \n",
      "Epoch 213, CIFAR-10 Batch 1:  stats: loss 0.22593206, accuracy 0.77319980, elapsed 2.28     \n",
      "Epoch 213, CIFAR-10 Batch 2:  stats: loss 0.20177421, accuracy 0.76719981, elapsed 2.26     \n",
      "Epoch 213, CIFAR-10 Batch 3:  stats: loss 0.22698295, accuracy 0.75919986, elapsed 2.27     \n",
      "Epoch 213, CIFAR-10 Batch 4:  stats: loss 0.17692585, accuracy 0.77379984, elapsed 2.26     \n",
      "Epoch 213, CIFAR-10 Batch 5:  stats: loss 0.20649418, accuracy 0.77739990, elapsed 2.27     \n",
      "Epoch 214, CIFAR-10 Batch 1:  stats: loss 0.23038276, accuracy 0.77119976, elapsed 2.28     \n",
      "Epoch 214, CIFAR-10 Batch 2:  stats: loss 0.17781352, accuracy 0.76839972, elapsed 2.26     \n",
      "Epoch 214, CIFAR-10 Batch 3:  stats: loss 0.16795553, accuracy 0.77999985, elapsed 2.26     \n",
      "Epoch 214, CIFAR-10 Batch 4:  stats: loss 0.15739709, accuracy 0.78079981, elapsed 2.29     \n",
      "Epoch 214, CIFAR-10 Batch 5:  stats: loss 0.21039741, accuracy 0.77759981, elapsed 2.29     \n",
      "Epoch 215, CIFAR-10 Batch 1:  stats: loss 0.21260749, accuracy 0.77679980, elapsed 2.28     \n",
      "Epoch 215, CIFAR-10 Batch 2:  stats: loss 0.19206563, accuracy 0.77859974, elapsed 2.26     \n",
      "Epoch 215, CIFAR-10 Batch 3:  stats: loss 0.19109170, accuracy 0.77899981, elapsed 2.27     \n",
      "Epoch 215, CIFAR-10 Batch 4:  stats: loss 0.17550665, accuracy 0.77379978, elapsed 2.25     \n",
      "Epoch 215, CIFAR-10 Batch 5:  stats: loss 0.20445502, accuracy 0.78039986, elapsed 2.25     \n",
      "Epoch 216, CIFAR-10 Batch 1:  stats: loss 0.22685276, accuracy 0.77679980, elapsed 2.28     \n",
      "Epoch 216, CIFAR-10 Batch 2:  stats: loss 0.21779171, accuracy 0.77559984, elapsed 2.27     \n",
      "Epoch 216, CIFAR-10 Batch 3:  stats: loss 0.19613145, accuracy 0.77399981, elapsed 2.25     \n",
      "Epoch 216, CIFAR-10 Batch 4:  stats: loss 0.17214896, accuracy 0.78419983, elapsed 2.26     \n",
      "Epoch 216, CIFAR-10 Batch 5:  stats: loss 0.20115390, accuracy 0.77779984, elapsed 2.27     \n",
      "Epoch 217, CIFAR-10 Batch 1:  stats: loss 0.24150029, accuracy 0.76979983, elapsed 2.27     \n",
      "Epoch 217, CIFAR-10 Batch 2:  stats: loss 0.21895452, accuracy 0.77819979, elapsed 2.27     \n",
      "Epoch 217, CIFAR-10 Batch 3:  stats: loss 0.21008508, accuracy 0.77439982, elapsed 2.28     \n",
      "Epoch 217, CIFAR-10 Batch 4:  stats: loss 0.16709554, accuracy 0.77039981, elapsed 2.26     \n",
      "Epoch 217, CIFAR-10 Batch 5:  stats: loss 0.20988815, accuracy 0.77199978, elapsed 2.27     \n",
      "Epoch 218, CIFAR-10 Batch 1:  stats: loss 0.23403275, accuracy 0.77659982, elapsed 2.25     \n",
      "Epoch 218, CIFAR-10 Batch 2:  stats: loss 0.18410075, accuracy 0.78559989, elapsed 2.28     \n",
      "Epoch 218, CIFAR-10 Batch 3:  stats: loss 0.21055931, accuracy 0.77399981, elapsed 2.27     \n",
      "Epoch 218, CIFAR-10 Batch 4:  stats: loss 0.17907465, accuracy 0.78259987, elapsed 2.25     \n",
      "Epoch 218, CIFAR-10 Batch 5:  stats: loss 0.22413492, accuracy 0.76499987, elapsed 2.27     \n",
      "Epoch 219, CIFAR-10 Batch 1:  stats: loss 0.23970932, accuracy 0.76999986, elapsed 2.26     \n",
      "Epoch 219, CIFAR-10 Batch 2:  stats: loss 0.21695648, accuracy 0.77059984, elapsed 2.26     \n",
      "Epoch 219, CIFAR-10 Batch 3:  stats: loss 0.19698176, accuracy 0.76639980, elapsed 2.25     \n",
      "Epoch 219, CIFAR-10 Batch 4:  stats: loss 0.18284494, accuracy 0.76639980, elapsed 2.27     \n",
      "Epoch 219, CIFAR-10 Batch 5:  stats: loss 0.19668134, accuracy 0.77599984, elapsed 2.26     \n",
      "Epoch 220, CIFAR-10 Batch 1:  stats: loss 0.22257783, accuracy 0.77699983, elapsed 2.25     \n",
      "Epoch 220, CIFAR-10 Batch 2:  stats: loss 0.18762285, accuracy 0.78199977, elapsed 2.26     \n",
      "Epoch 220, CIFAR-10 Batch 3:  stats: loss 0.18289118, accuracy 0.78319979, elapsed 2.26     \n",
      "Epoch 220, CIFAR-10 Batch 4:  stats: loss 0.15659969, accuracy 0.78299981, elapsed 2.27     \n",
      "Epoch 220, CIFAR-10 Batch 5:  stats: loss 0.20369288, accuracy 0.77599978, elapsed 2.28     \n",
      "Epoch 221, CIFAR-10 Batch 1:  stats: loss 0.20501161, accuracy 0.78299975, elapsed 2.27     \n",
      "Epoch 221, CIFAR-10 Batch 2:  stats: loss 0.19257163, accuracy 0.77919984, elapsed 2.26     \n",
      "Epoch 221, CIFAR-10 Batch 3:  stats: loss 0.19693355, accuracy 0.77419972, elapsed 2.32     \n",
      "Epoch 221, CIFAR-10 Batch 4:  stats: loss 0.17705792, accuracy 0.77659988, elapsed 2.26     \n",
      "Epoch 221, CIFAR-10 Batch 5:  stats: loss 0.19435699, accuracy 0.77619988, elapsed 2.28     \n",
      "Epoch 222, CIFAR-10 Batch 1:  stats: loss 0.22918937, accuracy 0.77739984, elapsed 2.26     \n",
      "Epoch 222, CIFAR-10 Batch 2:  stats: loss 0.19007649, accuracy 0.77519983, elapsed 2.26     \n",
      "Epoch 222, CIFAR-10 Batch 3:  stats: loss 0.21209672, accuracy 0.77899981, elapsed 2.27     \n",
      "Epoch 222, CIFAR-10 Batch 4:  stats: loss 0.16993907, accuracy 0.77119982, elapsed 2.25     \n",
      "Epoch 222, CIFAR-10 Batch 5:  stats: loss 0.18714073, accuracy 0.77359980, elapsed 2.27     \n",
      "Epoch 223, CIFAR-10 Batch 1:  stats: loss 0.22901624, accuracy 0.77859986, elapsed 2.29     \n",
      "Epoch 223, CIFAR-10 Batch 2:  stats: loss 0.21942076, accuracy 0.77259982, elapsed 2.28     \n",
      "Epoch 223, CIFAR-10 Batch 3:  stats: loss 0.19060035, accuracy 0.78039980, elapsed 2.27     \n",
      "Epoch 223, CIFAR-10 Batch 4:  stats: loss 0.19420981, accuracy 0.77799976, elapsed 2.27     \n",
      "Epoch 223, CIFAR-10 Batch 5:  stats: loss 0.21132597, accuracy 0.76639986, elapsed 2.26     \n",
      "Epoch 224, CIFAR-10 Batch 1:  stats: loss 0.21950492, accuracy 0.78019983, elapsed 2.27     \n",
      "Epoch 224, CIFAR-10 Batch 2:  stats: loss 0.19728637, accuracy 0.78019977, elapsed 2.27     \n",
      "Epoch 224, CIFAR-10 Batch 3:  stats: loss 0.19443992, accuracy 0.77179986, elapsed 2.26     \n",
      "Epoch 224, CIFAR-10 Batch 4:  stats: loss 0.17434312, accuracy 0.77839983, elapsed 2.28     \n",
      "Epoch 224, CIFAR-10 Batch 5:  stats: loss 0.20387127, accuracy 0.77859974, elapsed 2.27     \n",
      "Epoch 225, CIFAR-10 Batch 1:  stats: loss 0.21530531, accuracy 0.77859986, elapsed 2.27     \n",
      "Epoch 225, CIFAR-10 Batch 2:  stats: loss 0.19041172, accuracy 0.77719980, elapsed 2.30     \n",
      "Epoch 225, CIFAR-10 Batch 3:  stats: loss 0.16851895, accuracy 0.77979982, elapsed 2.29     \n",
      "Epoch 225, CIFAR-10 Batch 4:  stats: loss 0.16234086, accuracy 0.77879983, elapsed 2.25     \n",
      "Epoch 225, CIFAR-10 Batch 5:  stats: loss 0.18409549, accuracy 0.77739978, elapsed 2.29     \n",
      "Epoch 226, CIFAR-10 Batch 1:  stats: loss 0.20918281, accuracy 0.77319986, elapsed 2.28     \n",
      "Epoch 226, CIFAR-10 Batch 2:  stats: loss 0.16784950, accuracy 0.77919984, elapsed 2.28     \n",
      "Epoch 226, CIFAR-10 Batch 3:  stats: loss 0.18516859, accuracy 0.77839983, elapsed 2.28     \n",
      "Epoch 226, CIFAR-10 Batch 4:  stats: loss 0.15697885, accuracy 0.77939981, elapsed 2.27     \n",
      "Epoch 226, CIFAR-10 Batch 5:  stats: loss 0.17823902, accuracy 0.77839983, elapsed 2.24     \n",
      "Epoch 227, CIFAR-10 Batch 1:  stats: loss 0.20270318, accuracy 0.77399987, elapsed 2.29     \n",
      "Epoch 227, CIFAR-10 Batch 2:  stats: loss 0.19540378, accuracy 0.76639986, elapsed 2.28     \n",
      "Epoch 227, CIFAR-10 Batch 3:  stats: loss 0.17722332, accuracy 0.77779990, elapsed 2.27     \n",
      "Epoch 227, CIFAR-10 Batch 4:  stats: loss 0.16357733, accuracy 0.77219981, elapsed 2.28     \n",
      "Epoch 227, CIFAR-10 Batch 5:  stats: loss 0.17696290, accuracy 0.77899981, elapsed 2.25     \n",
      "Epoch 228, CIFAR-10 Batch 1:  stats: loss 0.20576596, accuracy 0.77459985, elapsed 2.29     \n",
      "Epoch 228, CIFAR-10 Batch 2:  stats: loss 0.19801323, accuracy 0.76599979, elapsed 2.27     \n",
      "Epoch 228, CIFAR-10 Batch 3:  stats: loss 0.19293857, accuracy 0.77399981, elapsed 2.24     \n",
      "Epoch 228, CIFAR-10 Batch 4:  stats: loss 0.16735098, accuracy 0.78239983, elapsed 2.28     \n",
      "Epoch 228, CIFAR-10 Batch 5:  stats: loss 0.19656140, accuracy 0.77559984, elapsed 2.27     \n",
      "Epoch 229, CIFAR-10 Batch 1:  stats: loss 0.21697502, accuracy 0.78059983, elapsed 2.26     \n",
      "Epoch 229, CIFAR-10 Batch 2:  stats: loss 0.19977778, accuracy 0.77879989, elapsed 2.27     \n",
      "Epoch 229, CIFAR-10 Batch 3:  stats: loss 0.17648090, accuracy 0.78179979, elapsed 2.28     \n",
      "Epoch 229, CIFAR-10 Batch 4:  stats: loss 0.15566868, accuracy 0.78759980, elapsed 2.28     \n",
      "Epoch 229, CIFAR-10 Batch 5:  stats: loss 0.17825909, accuracy 0.77499980, elapsed 2.29     \n",
      "Epoch 230, CIFAR-10 Batch 1:  stats: loss 0.20941991, accuracy 0.77599984, elapsed 2.27     \n",
      "Epoch 230, CIFAR-10 Batch 2:  stats: loss 0.19163434, accuracy 0.77819979, elapsed 2.26     \n",
      "Epoch 230, CIFAR-10 Batch 3:  stats: loss 0.16804388, accuracy 0.77779979, elapsed 2.25     \n",
      "Epoch 230, CIFAR-10 Batch 4:  stats: loss 0.17608285, accuracy 0.77839988, elapsed 2.28     \n",
      "Epoch 230, CIFAR-10 Batch 5:  stats: loss 0.20449416, accuracy 0.77019984, elapsed 2.26     \n",
      "Epoch 231, CIFAR-10 Batch 1:  stats: loss 0.20934474, accuracy 0.77899981, elapsed 2.27     \n",
      "Epoch 231, CIFAR-10 Batch 2:  stats: loss 0.20613989, accuracy 0.77519983, elapsed 2.28     \n",
      "Epoch 231, CIFAR-10 Batch 3:  stats: loss 0.17735049, accuracy 0.78099978, elapsed 2.26     \n",
      "Epoch 231, CIFAR-10 Batch 4:  stats: loss 0.16849096, accuracy 0.78059983, elapsed 2.26     \n",
      "Epoch 231, CIFAR-10 Batch 5:  stats: loss 0.21242601, accuracy 0.76359987, elapsed 2.28     \n",
      "Epoch 232, CIFAR-10 Batch 1:  stats: loss 0.20318075, accuracy 0.77439988, elapsed 2.26     \n",
      "Epoch 232, CIFAR-10 Batch 2:  stats: loss 0.18519939, accuracy 0.77599984, elapsed 2.26     \n",
      "Epoch 232, CIFAR-10 Batch 3:  stats: loss 0.16835901, accuracy 0.78039986, elapsed 2.28     \n",
      "Epoch 232, CIFAR-10 Batch 4:  stats: loss 0.15762153, accuracy 0.77459979, elapsed 2.29     \n",
      "Epoch 232, CIFAR-10 Batch 5:  stats: loss 0.18130255, accuracy 0.77599978, elapsed 2.28     \n",
      "Epoch 233, CIFAR-10 Batch 1:  stats: loss 0.20155808, accuracy 0.77739990, elapsed 2.28     \n",
      "Epoch 233, CIFAR-10 Batch 2:  stats: loss 0.18637414, accuracy 0.78279984, elapsed 2.27     \n",
      "Epoch 233, CIFAR-10 Batch 3:  stats: loss 0.17821229, accuracy 0.77579981, elapsed 2.25     \n",
      "Epoch 233, CIFAR-10 Batch 4:  stats: loss 0.18815857, accuracy 0.76679981, elapsed 2.27     \n",
      "Epoch 233, CIFAR-10 Batch 5:  stats: loss 0.20610377, accuracy 0.77239978, elapsed 2.26     \n",
      "Epoch 234, CIFAR-10 Batch 1:  stats: loss 0.21561450, accuracy 0.77099979, elapsed 2.26     \n",
      "Epoch 234, CIFAR-10 Batch 2:  stats: loss 0.19223706, accuracy 0.76799983, elapsed 2.25     \n",
      "Epoch 234, CIFAR-10 Batch 3:  stats: loss 0.18137027, accuracy 0.76659989, elapsed 2.26     \n",
      "Epoch 234, CIFAR-10 Batch 4:  stats: loss 0.16089156, accuracy 0.76819986, elapsed 2.25     \n",
      "Epoch 234, CIFAR-10 Batch 5:  stats: loss 0.19158503, accuracy 0.77519977, elapsed 2.27     \n",
      "Epoch 235, CIFAR-10 Batch 1:  stats: loss 0.21490782, accuracy 0.77799982, elapsed 2.29     \n",
      "Epoch 235, CIFAR-10 Batch 2:  stats: loss 0.20328939, accuracy 0.77319980, elapsed 2.26     \n",
      "Epoch 235, CIFAR-10 Batch 3:  stats: loss 0.16596648, accuracy 0.77899987, elapsed 2.25     \n",
      "Epoch 235, CIFAR-10 Batch 4:  stats: loss 0.16717914, accuracy 0.77199984, elapsed 2.28     \n",
      "Epoch 235, CIFAR-10 Batch 5:  stats: loss 0.18264827, accuracy 0.77939981, elapsed 2.28     \n",
      "Epoch 236, CIFAR-10 Batch 1:  stats: loss 0.21297647, accuracy 0.77859980, elapsed 2.28     \n",
      "Epoch 236, CIFAR-10 Batch 2:  stats: loss 0.18961734, accuracy 0.77019984, elapsed 2.25     \n",
      "Epoch 236, CIFAR-10 Batch 3:  stats: loss 0.19649777, accuracy 0.78159982, elapsed 2.24     \n",
      "Epoch 236, CIFAR-10 Batch 4:  stats: loss 0.20009997, accuracy 0.77299988, elapsed 2.27     \n",
      "Epoch 236, CIFAR-10 Batch 5:  stats: loss 0.19856192, accuracy 0.77039975, elapsed 2.26     \n",
      "Epoch 237, CIFAR-10 Batch 1:  stats: loss 0.21180685, accuracy 0.78339988, elapsed 2.27     \n",
      "Epoch 237, CIFAR-10 Batch 2:  stats: loss 0.19484219, accuracy 0.77519977, elapsed 2.25     \n",
      "Epoch 237, CIFAR-10 Batch 3:  stats: loss 0.17448942, accuracy 0.77499980, elapsed 2.27     \n",
      "Epoch 237, CIFAR-10 Batch 4:  stats: loss 0.17452583, accuracy 0.77779979, elapsed 2.26     \n",
      "Epoch 237, CIFAR-10 Batch 5:  stats: loss 0.19053304, accuracy 0.77819985, elapsed 2.27     \n",
      "Epoch 238, CIFAR-10 Batch 1:  stats: loss 0.20160037, accuracy 0.78159988, elapsed 2.27     \n",
      "Epoch 238, CIFAR-10 Batch 2:  stats: loss 0.18814868, accuracy 0.77419978, elapsed 2.25     \n",
      "Epoch 238, CIFAR-10 Batch 3:  stats: loss 0.17251906, accuracy 0.77699983, elapsed 2.28     \n",
      "Epoch 238, CIFAR-10 Batch 4:  stats: loss 0.16515251, accuracy 0.77719975, elapsed 2.26     \n",
      "Epoch 238, CIFAR-10 Batch 5:  stats: loss 0.20131600, accuracy 0.77699983, elapsed 2.26     \n",
      "Epoch 239, CIFAR-10 Batch 1:  stats: loss 0.20174730, accuracy 0.77739990, elapsed 2.28     \n",
      "Epoch 239, CIFAR-10 Batch 2:  stats: loss 0.19138423, accuracy 0.77679980, elapsed 2.26     \n",
      "Epoch 239, CIFAR-10 Batch 3:  stats: loss 0.15629339, accuracy 0.78019977, elapsed 2.29     \n",
      "Epoch 239, CIFAR-10 Batch 4:  stats: loss 0.17970556, accuracy 0.77259982, elapsed 2.26     \n",
      "Epoch 239, CIFAR-10 Batch 5:  stats: loss 0.18804762, accuracy 0.76899981, elapsed 2.25     \n",
      "Epoch 240, CIFAR-10 Batch 1:  stats: loss 0.19762956, accuracy 0.77999979, elapsed 2.28     \n",
      "Epoch 240, CIFAR-10 Batch 2:  stats: loss 0.19936922, accuracy 0.77239978, elapsed 2.27     \n",
      "Epoch 240, CIFAR-10 Batch 3:  stats: loss 0.19181231, accuracy 0.78079981, elapsed 2.25     \n",
      "Epoch 240, CIFAR-10 Batch 4:  stats: loss 0.18629381, accuracy 0.77919984, elapsed 2.25     \n",
      "Epoch 240, CIFAR-10 Batch 5:  stats: loss 0.19190249, accuracy 0.76799977, elapsed 2.27     \n",
      "Epoch 241, CIFAR-10 Batch 1:  stats: loss 0.21496667, accuracy 0.77659988, elapsed 2.26     \n",
      "Epoch 241, CIFAR-10 Batch 2:  stats: loss 0.18142834, accuracy 0.77919978, elapsed 2.25     \n",
      "Epoch 241, CIFAR-10 Batch 3:  stats: loss 0.16204248, accuracy 0.77739978, elapsed 2.29     \n",
      "Epoch 241, CIFAR-10 Batch 4:  stats: loss 0.17186259, accuracy 0.77679980, elapsed 2.27     \n",
      "Epoch 241, CIFAR-10 Batch 5:  stats: loss 0.17800377, accuracy 0.77679980, elapsed 2.25     \n",
      "Epoch 242, CIFAR-10 Batch 1:  stats: loss 0.22619855, accuracy 0.77019978, elapsed 2.25     \n",
      "Epoch 242, CIFAR-10 Batch 2:  stats: loss 0.18532357, accuracy 0.78019983, elapsed 2.27     \n",
      "Epoch 242, CIFAR-10 Batch 3:  stats: loss 0.18801497, accuracy 0.77019984, elapsed 2.27     \n",
      "Epoch 242, CIFAR-10 Batch 4:  stats: loss 0.16342409, accuracy 0.77819979, elapsed 2.26     \n",
      "Epoch 242, CIFAR-10 Batch 5:  stats: loss 0.18973945, accuracy 0.77599978, elapsed 2.25     \n",
      "Epoch 243, CIFAR-10 Batch 1:  stats: loss 0.20215055, accuracy 0.77859986, elapsed 2.26     \n",
      "Epoch 243, CIFAR-10 Batch 2:  stats: loss 0.20779955, accuracy 0.77779984, elapsed 2.27     \n",
      "Epoch 243, CIFAR-10 Batch 3:  stats: loss 0.19351134, accuracy 0.77379984, elapsed 2.27     \n",
      "Epoch 243, CIFAR-10 Batch 4:  stats: loss 0.18904474, accuracy 0.77759987, elapsed 2.27     \n",
      "Epoch 243, CIFAR-10 Batch 5:  stats: loss 0.20027442, accuracy 0.76659983, elapsed 2.26     \n",
      "Epoch 244, CIFAR-10 Batch 1:  stats: loss 0.20646721, accuracy 0.77959985, elapsed 2.28     \n",
      "Epoch 244, CIFAR-10 Batch 2:  stats: loss 0.18460658, accuracy 0.77319980, elapsed 2.28     \n",
      "Epoch 244, CIFAR-10 Batch 3:  stats: loss 0.17718604, accuracy 0.77479982, elapsed 2.28     \n",
      "Epoch 244, CIFAR-10 Batch 4:  stats: loss 0.16319190, accuracy 0.77979982, elapsed 2.24     \n",
      "Epoch 244, CIFAR-10 Batch 5:  stats: loss 0.17469956, accuracy 0.77859980, elapsed 2.26     \n",
      "Epoch 245, CIFAR-10 Batch 1:  stats: loss 0.22137417, accuracy 0.77679980, elapsed 2.27     \n",
      "Epoch 245, CIFAR-10 Batch 2:  stats: loss 0.19463938, accuracy 0.77659976, elapsed 2.26     \n",
      "Epoch 245, CIFAR-10 Batch 3:  stats: loss 0.18602139, accuracy 0.76839983, elapsed 2.27     \n",
      "Epoch 245, CIFAR-10 Batch 4:  stats: loss 0.17639633, accuracy 0.77739990, elapsed 2.27     \n",
      "Epoch 245, CIFAR-10 Batch 5:  stats: loss 0.17871335, accuracy 0.77759981, elapsed 2.27     \n",
      "Epoch 246, CIFAR-10 Batch 1:  stats: loss 0.21408795, accuracy 0.77399975, elapsed 2.23     \n",
      "Epoch 246, CIFAR-10 Batch 2:  stats: loss 0.19552599, accuracy 0.77859974, elapsed 2.27     \n",
      "Epoch 246, CIFAR-10 Batch 3:  stats: loss 0.18992051, accuracy 0.77379984, elapsed 2.26     \n",
      "Epoch 246, CIFAR-10 Batch 4:  stats: loss 0.16765104, accuracy 0.78199983, elapsed 2.29     \n",
      "Epoch 246, CIFAR-10 Batch 5:  stats: loss 0.19482665, accuracy 0.76899981, elapsed 2.26     \n",
      "Epoch 247, CIFAR-10 Batch 1:  stats: loss 0.19627234, accuracy 0.77859980, elapsed 2.28     \n",
      "Epoch 247, CIFAR-10 Batch 2:  stats: loss 0.19502342, accuracy 0.77479976, elapsed 2.27     \n",
      "Epoch 247, CIFAR-10 Batch 3:  stats: loss 0.18674158, accuracy 0.77179980, elapsed 2.24     \n",
      "Epoch 247, CIFAR-10 Batch 4:  stats: loss 0.17223474, accuracy 0.77439988, elapsed 2.27     \n",
      "Epoch 247, CIFAR-10 Batch 5:  stats: loss 0.18521035, accuracy 0.77619982, elapsed 2.27     \n",
      "Epoch 248, CIFAR-10 Batch 1:  stats: loss 0.23186731, accuracy 0.76719975, elapsed 2.25     \n",
      "Epoch 248, CIFAR-10 Batch 2:  stats: loss 0.20124447, accuracy 0.77579987, elapsed 2.28     \n",
      "Epoch 248, CIFAR-10 Batch 3:  stats: loss 0.18585074, accuracy 0.77219987, elapsed 2.26     \n",
      "Epoch 248, CIFAR-10 Batch 4:  stats: loss 0.16639000, accuracy 0.78059977, elapsed 2.26     \n",
      "Epoch 248, CIFAR-10 Batch 5:  stats: loss 0.19429605, accuracy 0.77179980, elapsed 2.26     \n",
      "Epoch 249, CIFAR-10 Batch 1:  stats: loss 0.25431386, accuracy 0.77099985, elapsed 2.28     \n",
      "Epoch 249, CIFAR-10 Batch 2:  stats: loss 0.18777746, accuracy 0.78039980, elapsed 2.26     \n",
      "Epoch 249, CIFAR-10 Batch 3:  stats: loss 0.20177537, accuracy 0.76719987, elapsed 2.24     \n",
      "Epoch 249, CIFAR-10 Batch 4:  stats: loss 0.16984971, accuracy 0.78319985, elapsed 2.28     \n",
      "Epoch 249, CIFAR-10 Batch 5:  stats: loss 0.17871283, accuracy 0.77639985, elapsed 2.26     \n",
      "Epoch 250, CIFAR-10 Batch 1:  stats: loss 0.22207613, accuracy 0.77619982, elapsed 2.26     \n",
      "Epoch 250, CIFAR-10 Batch 2:  stats: loss 0.21086910, accuracy 0.76959985, elapsed 2.27     \n",
      "Epoch 250, CIFAR-10 Batch 3:  stats: loss 0.20297995, accuracy 0.77859980, elapsed 2.25     \n",
      "Epoch 250, CIFAR-10 Batch 4:  stats: loss 0.14924659, accuracy 0.78339982, elapsed 2.23     \n",
      "Epoch 250, CIFAR-10 Batch 5:  stats: loss 0.17209546, accuracy 0.77459985, elapsed 2.27     \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "global last_time\n",
    "last_time = time.time()\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7697840094566345\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe9f2b1a2b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
